# -*- coding: utf-8 -*-
"""## Import Modules"""

import os, sys
import json 
import math
import pandas as pd 
from tqdm import tqdm 
import re 
from sklearn.metrics import accuracy_score
from collections import  defaultdict, Counter, OrderedDict

from transformers import pipeline
import numpy as np 
import copy
import math
import configparser
import yaml 
import argparse
from IPython.display import display
##### spacy 
import spacy
en = spacy.load('en_core_web_sm')
STOP_WORDS = en.Defaults.stop_words
from tabulate import tabulate, simple_separated_format
import re
import spacy
from copy import deepcopy
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex
nlp = spacy.load('en_core_web_sm')

pd.options.display.max_columns = 50
pd.set_option('display.width', 1000)

from utils_wordnet import get_wordnet_shortest_path_score_between, get_wordnet_avg_path_score_between_sub_and_anchors, get_wordnet_shortest_path_length_between
from utils_concept_positioning_test import concpet_positioning_test, get_revserse_prompt

"""Anchored_Prompts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CnAjbN-8oYPkxM0q4NT1uXVf5KV-GxZ8
"""

# !pip install transformers
# !pip install spacy



"""## Patterns"""




relations = [
    {
        "relation": "IsA",
        "def_sap": ["A [X] is a type of [Y] ."],
        "def_dap": ["A [X] or [Z] is a type of [Y] ."],
        "lsp_sap": ["such [Y] as [X]"],
        "lsp_dap": ["such [Y] as [X] or [Z]"],
        "use_dap": True,
        "anchor_target": "[X]",
        "anchor_lsp_sap": ["Such as [X] and [MASK] ."],
        # "dap_x": "A [X] and [Z] are a type of [Y] .",
        # "dap_y": "",
    },
]


def get_relation_templates(relations, model, anchor_type, def_sap_patterns, def_dap_patterns, lsp_sap_patterns, lsp_dap_patterns): 
    '''
    generate the relation tempaltes based on model and anchor_type 

    '''
    # let's assumet the anchor concept is [Z] 
    relation_to_template = defaultdict() 
    for relation_initial in relations:
        # print(relation.keys())
        relation_name = relation_initial['relation'] 
        # relation_to_template[key] = defaultdict()
        relation = copy.deepcopy(relation_initial)
        # anchor_target  = relation["anchor_target"]

        relation['anchor_lsp_sap']  = lsp_sap_patterns[anchor_type]
        relation['def_sap']  = def_sap_patterns[relation_name]
        relation['def_dap']  = def_dap_patterns[relation_name]

        relation['lsp_sap']  = lsp_sap_patterns[relation_name]
        relation['lsp_dap']  = lsp_dap_patterns[relation_name]

        if relation["anchor_target"] == '[X]':
            for pattern in ['def_sap', 'lsp_sap', 'def_dap', 'lsp_dap', 'anchor_lsp_sap']:
                if 'roberta' in model:
                    relation[pattern] = [item.replace("[Y]", "<mask>") for item in relation[pattern]]
                elif 'bert' in model: 
                    relation[pattern] = [item.replace("[Y]", "[MASK]") for item in relation[pattern]]
            
        # save_dict_to_json(new_relations, 'log/relation_sap_dap.json')
        # if debug:
            # print( json.dumps(relation_to_template, indent=4)
        relation_to_template[relation_name] = relation 
    return relation_to_template


"""## Helper Functions"""


def custom_tokenizer(nlp):
    infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\"\'~]''')
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)


nlp.tokenizer = custom_tokenizer(nlp)

"""## Helper Functions"""

def save_dict_to_json(examples, output_path):
    ''' 
    save a list of dicts into otuput_path, orient='records' (each line is a dict) 
    examples: a list of dicts
    output_path: 
    '''
    # if not os.path.exists(output_path):
        # os.path.makedirs(output_path)

    with open(output_path, 'w') as fout:
        for example in examples:
            json.dump(example, fout)
            fout.write("\n")
        print(f"save {output_path} with {len(examples)} lines")

"""## Load Data"""

def tokenize_sentence(sentence):
  sentence = sentence.replace("\"", "").lower()
  tokens = [token.orth_ for token in nlp(sentence)]
  return tokens

def locate_sub_obj_position(ent, sentence, index_not_in) :
  ''' 
  function: find the index of ent in a sentence, the result will be used to filter instances whose ent cannot be find at their sentences
  args: 
    sentence: the sentnces to mask, could be the string or a list of tokens 
    ent: the ent to be found (sub_label) 
    index_not_in: the default index for failed instances (an ent not in a sentence)
  ''' 

  if isinstance(sentence, list):
    if ent not in sentence:
      return index_not_in
    return sentence.index(ent)  
  else:
    sentence = copy.deepcopy(sentence).lower()
    if isinstance(sentence, str):
      try:
        index = sentence.index(ent)
        return  index 
      except: 
        print(f"NOT FOUND sub_label: {ent} -> in sentence: {sentence}")
        return index_not_in
      
        print(ent, sentence)
        return index_not_in

def remove_noisy_test_data(df):
  ''' 
  relation="hasproperty"
  why? some data points don't belong to this relation types 
  case1., sub_label=number, such as "10 is ten."  We don't say ten is the property of 10
  case2, sub_label = 'person_name' and obj_label = 'nuts;, such as ""Andrew is [MASK].", [MASK]=nuts
  '''
  sub_labels_to_exclude = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '30', '5', '50', '60', '7', '70', '70s', '80', '9', '90']
  obj_labels_to_exclude  = ['nuts']
  df = df.query(f"sub_label not in {sub_labels_to_exclude}")
  df = df.query(f"sub_label not in {obj_labels_to_exclude}")
  return  df.reset_index(drop=True)

def read_bert_vocab(bert_vocab_path = 'data/bert-large-uncased-vocab.txt'):
    
    
    vocab = set()
    with open(bert_vocab_path, 'r') as fin: 
        lines = fin.readlines()
        for line in lines: 
            line = line.strip()
            vocab.add(line)
    return vocab        

  

def load_data(filepath, sub_col, clean_test=True, tokenize=False):
  '''
  return the cleaned data
  args:
    tokenize: if True: the maksed_sentences will be tokenzied (this is slwoers); 
            otherwise, we use the string match to filter the failed sentences
    clean_test: default is True. We filter out some noisy samples spoted by huamns 
               Note that this is relation specific 

  '''
  index_not_in = 10000
  with open(filepath, 'r', encoding='utf-8') as fin:
    data = fin.readlines()
    data = [eval(x) for x in data]
    df = pd.DataFrame(data)

    df['obj_label'] = df['obj_label'].apply(lambda x: [x] if isinstance(x, str) else x)
    df['masked_sentences'] = df['masked_sentences'].apply(lambda x: eval(x) if isinstance(x, str) else x)

  if tokenize:
    df['masked_sentence_tokens'] = df['masked_sentences'].apply(lambda x: tokenize_sentence(x[0]))
    df['sub_position'] = df[[sub_col, 'masked_sentence_tokens']].apply(lambda x: locate_sub_obj_position(x[0], x[1], index_not_in=index_not_in), axis=1)

  if clean_test: 
    df = remove_noisy_test_data(df)
    df['sub_position'] = df[[sub_col, 'masked_sentences']].apply(lambda x: locate_sub_obj_position(x[0], x[1][0], index_not_in), axis=1)
    df = df.query(f"sub_position !={index_not_in}") #.reset_index() #cue can not be matched in the sentence

  print(f"#All Instances: {len(df.index)}")
  return df.reset_index(drop=True)


def _get_article(word, capitalize_article=False):
    
    if word[0] in ['a', 'e', 'i', 'o', 'u']:
        return 'an'.capitalize() if capitalize_article else 'an'
    return 'a'.capitalize() if capitalize_article else 'a'

def check_articles(sub_label, pattern):
    '''
    select suitable article templates for a given sub_label 
    '''
    first_word = pattern.split()[0]
    if first_word == sub_label:  
        return pattern.capitalize()

    elif first_word != sub_label: 
        sub_article = _get_article(sub_label)
        return pattern  if sub_article == first_word  else 'DISCARD'


    


def get_masked_data(filepath, sub_col, sub_col_sg, sub_col_pl, relation, relation_to_template, model, singular_pattern_flag, debug=False, clean_test=True, anchor_probe='plural_probe'):
  df = load_data(filepath, sub_col, clean_test=clean_test)
  if debug:
      df = df.sample(200)

  name_to_patterns = {'def_sap': def_sap_patterns, 'def_dap': def_dap_patterns, 'lsp_sap': lsp_sap_patterns, 'lsp_dap': lsp_dap_patterns, }
                  
  #   replace [X] by mask
  if 'roberta' in model :
    df['masked_sentences'] = df['masked_sentences'].apply(lambda x: [sentence.replace("[MASK]", "<mask>") for sentence in x])

  if anchor_probe=='plural_probe':
    df[sub_col_sg] = deepcopy(df[sub_col])  #.apply(lambda x: singularize(x))  
    df[sub_col] = df[sub_col].apply(lambda x: pluralize(x))
    df[sub_col_pl] = df[sub_col]

  for pattern in ['def_sap', 'def_dap']:# , 'lsp_sap', 'anchor_lsp_sap']:
    # use check when sub_label are sigular, we nned to inderst an article before it
    if singular_pattern_flag:
      df[pattern] = df[sub_col_sg].apply(lambda x: [item.replace('[X]', f"{_get_article(x)} {x}") for item in relation_to_template[pattern] ]) #if check_articles(x, item)!='DISCARD' ])
    else:
      df[pattern] = df[sub_col_sg].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern] ])

#   for pattern in ['lsp_sap', 'lsp_dap']: #'anchor_lsp_sap']:
    #   df[pattern] = df[sub_col_pl].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern]])

  #for pattern in ['anchor_lsp_sap']:
  #  if anchor_probe == 'plural_probe':
  #      df[pattern] = df['sub_label_pl'].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern]])
  #  else:
  #      df[pattern] = df[sub_col_pl].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern]])

    # if anchor_probe == 'plural_probe':
  for pattern in ['lsp_sap', 'lsp_dap', 'anchor_lsp_sap']:
    df[pattern] = df[sub_col_pl].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern]])

  if debug:
    print(df.columns)
    print(f"{len(df.index)} instances")

  return df 

# if debug:
def test_load_data():
    for relation in ['IsA']: #['HasProperty']: # ['HasProperty', 'CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction']:
        print(f"relation: {relation}")
        filepath = f'./clsb/{relation}.jsonl'
        # df = load_data(filepath, clean_test=True, tokenize=False)
        relation_to_template  = get_relation_templates(relations, model, anchor_type='Coordinate')
        df = get_masked_data(filepath, relation, relation_to_template[relation], model)
        display(df.head())

"""## Define model and feed masked data into models"""

def get_unmasker(model, device, targets=None):
    if targets is None: 
        unmasker = pipeline('fill-mask', model=model, device=device)# 'bert-large-uncased') #initialize the masker
    else:
        unmasker = pipeline('fill-mask', model=model, device=device, targets=targets )# 'bert-large-uncased') #initialize the masker
    return unmasker


def get_target_vocab(file_path = "data/clsb/blank/vocab.txt"):
    '''
    Weir et al., (2020) used two variants of target vocabulary when predicting the possible properties 
    One of them is the SNES version, "comprising the set of human completion that fit the given prompt syntax for all concepts in the study"
    '''
    df_target_vocab = pd.read_csv(file_path, header=None) #.to_list()
    target_vocab = df_target_vocab.iloc[:,0].to_list() 
    return target_vocab



def develop_single_case():
    target_vocab = get_target_vocab(file_path = "data/clsb/blank/vocab.txt")
    unmasker = get_unmasker(model= 'bert-base-cased', targets= target_vocab )
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= len(target_vocab))

    for x in results[:10]:
        print(x)
    print("-"*80)

    unmasker = get_unmasker(model= 'bert-base-cased')
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= 10)
    for x in results[:10]:
        print(x)

def fill_mask_anchor(df, unmasker, anchor_prompt_col, top_k):
    outputs = defaultdict()
    outputs[anchor_prompt_col]  = [unmasker(x, top_k=top_k) for x in df[anchor_prompt_col]]

    return outputs

def fill_mask_obj(df, unmasker, top_k):
    outputs = defaultdict()
    # outputs['masked_sap'] = unmasker(df.masked_sap.to_list(), top_k=top_k, batch_size=100) 
    outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in df['masked_sentences'].to_list()]

    outputs['def_sap']  = [unmasker(x, top_k=top_k) for x in df['def_sap'].to_list()]
    outputs['lsp_sap']  = [unmasker(x, top_k=top_k) for x in df['lsp_sap'].to_list()]
    # outputs['masked_sentences_obj_anchor'] = unmasker(df.masked_sentences_obj_anchor.to_list(), top_k=top_k, batch_size=100) 
    # outputs['masked_sentences'] = unmasker(df.masked_sentences.to_list(), top_k=top_k, batch_size=100)
    # outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_sentences)]
    return outputs

"""## Cleaning outputs """

def aggregate_token_scores(input_word, token2probs, scorer, top_k, add_wordnet_path_score=False, sort_flag=True ):
    ''' 
    goal: we want the best scorer to consider:
        (1) frequency: a token that are elicited by multiple promptso
        (2) the probability: higher overall probability 
        (3)

    token2prob: dictionary mapping a token to a list of probs 
    anchor_anchor_scorer_list = ['freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply', 'freq', 'probSum', 'probAvg'] #TODO: rank based


    test case:
    token2probs = {'achieve': [0.2, 0.1, 0.03, 0.006], 'tried': [0.008, 0.006, 0.003, 0.001], 'perform':[0.08], 'prevent': [0.06], 'use': [0.02], 'accomplished': [0.1], 'produce':[0.06]}
    for scorer in  [ 'freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply' ]: #'freq', 'probSum', 'probAvg',
        token2prob = aggregate_token_scores(token2probs, scorer, top_k=7, sort_flag=True)
        print(scorer)
        print(f"\t{token2prob}" )
        print()

    '''
    token2prob = defaultdict()
    all_count = sum([len(item) for item in token2probs.values()])
    # print(json.dumps(token2probs, indent=4))
    for token, probs in token2probs.items(): #rank_score = w * p, w is the frequency weight, p is the probability
            count = len(probs)
            
            freq_weight = count/all_count
            
            new_score = 0 
            
            if scorer=='freq':
                new_score =  freq_weight 

            elif scorer=='probSum':
                new_score = sum(probs)

            elif scorer=='probAvg': #this ignore the frequency factor [not ideal]
                new_score = sum(probs)/ len(probs)

            elif scorer=='freqProbSum': #[close to ideal]
                new_score = freq_weight * sum(probs)
                # print(token, freq_weight,sum(probs), new_score )
            elif scorer=='probLogSum':
                probs_valid = [item for item in probs if item>0]
                if len(probs_valid )==0:
                    new_score= 0
                else:
                    new_score =  sum([math.log(item, 2) for item in probs_valid ])/len(probs_valid)
                    # new_score =  sum([math.log(item, 2) for item in probs if item>0])/len(probs)

            elif scorer=='freqProbLogSum': #[close to ideal, requires a token to be (1) frequent (2) high probs across prompts]
                probs_valid = [item for item in probs if item>0]
                if len(probs_valid )==0:
                    new_score= 0
                else:
                    new_score =   sum([math.log(item*freq_weight, 2) for item in probs_valid ])/len(probs_valid)
                    # new_score =   sum([math.log(item*freq_weight, 2) for item in probs if item>0])/len(probs)
                # print("token: {}, freq_weight: {}, logsum: {}, new_score: {}".format(token, freq_weight, sum([math.log(item, 2) for item in probs])/len(probs), new_score))
            # elif scorer=='probMultiply': #using prob multiply will penelize the tokens with high frequency
            #     new_score =  math.exp(sum([math.log(item, 2) for item in probs]))

            # 221114: add the WN path socre here to obtain reliable outputs 
            if add_wordnet_path_score:
                path_score = get_wordnet_shortest_path_score_between(input_word, token)
                #print(f"input_word: {input_word}", "token:", token )
                #print("path_score", path_score, type(path_score))
                #print("new_score", new_score, type(new_score))
                new_score += path_score  

            token2prob[token] = new_score
    
    # token2prob = sorted(token2prob.items(), key=lambda x: x[1], reverse=True )
    # if top_k is not None and isinstance(top_k, int):
        # token2prob = token2prob[:top_k] 
    # token2prob = dict(token2prob)
    return token2prob


def filter_outputs_with_probs(inputs, outputs, filter_objects_flag=True, return_probs=True, top_k=None, scorer='freqProbSum', filter_objects_with_input=True, add_wordnet_path_score=False, add_cpt_score=False, cpt_unmasker=None, mask_string=None, cpt_only=False):
    '''
    inputs: the original inputs, for example [A] is a type of [B], A is the input
    outputs: the candidates returned by PTLMs

    filter: True 
        filter: non-alpha tokens); 

    top_k: take the top_k outputs. This is important when using multiple prompts for each sub 
    add_wordnet_path_score: add wordnet path score into the output scoring function 
    add_cpt_score: add concept-positioning test score into the output scoring function 

    '''
    anchor_list = []
    anchor_scores = [] 
        
    for input_words, top_outputs in zip(inputs, outputs):  #iterate through the samples (sub)
        input_words = [re.sub("\s+", '', x) for x in input_words.split()]
        input_word  = input_words[0]
        filled_tokens  = defaultdict(int) #filter/accumulate predictions for each sample 
        filled_scores = defaultdict(list) #a list of token:[score1, score2, ...]   
        token2cpt  = defaultdict(list) #filter/accumulate predictions for each sample 

        if isinstance(top_outputs[0], list):
            flatten_output = [item for top_k_output  in top_outputs for item in top_k_output]
        else:
            flatten_output = [item for item  in top_outputs]
        # print("flatten_output", flatten_output)

        for i, output in enumerate(flatten_output):
            filled_token = output['token_str'].strip().lower()
            filled_score = output['score']
            if filter_objects_flag:
                #####Add conditions to filter unwanted ################
                # filter the repetation of a concept in the explanation. See the the following example
                # [MASK] is the capability to do a particular job . -> capacity 
                if not filled_token.isalpha(): continue
                if filled_token in STOP_WORDS: continue 
                if len(filled_token)<=1: continue 
                if filter_objects_with_input:
                    if filled_token.strip().lower() in input_words: continue
                    # [re.sub("\s+", '', x) for x in input_word.split()]: continue #filter out the target in input  
                if filled_token.startswith("#"): continue
                #####Add conditions to filter unwanted ################
                
                filled_tokens[filled_token] +=1
                filled_scores[filled_token].append(filled_score)

                ##### add CPT test for filtering ################ 
                if add_cpt_score and cpt_unmasker!=None and mask_string!=None:
                    token2cpt[filled_token].append(concpet_positioning_test(stimulus_word=input_word,
                                                                            output= output, 
                                                                            unmasker=cpt_unmasker, 
                                                                            mask_string=mask_string))

                    # token2cpt_prompt[filled_token] =  get_revserse_prompt((stimulus_word=input_word,
                                                                            # output= output))
                ##### add CPT test for filtering ################ 
            else:
                filled_tokens[filled_token] +=1
                filled_scores[filled_token] += filled_score

        if len(filled_tokens) ==0: 
            filled_tokens={'MISSING':1}
            filled_scores['MISSING'] = [0]

        # feed the input into the agrregate _token_scores() so that we can calcuate the 
        token2probs = aggregate_token_scores(input_word, token2probs=filled_scores, scorer=scorer, top_k=top_k,  add_wordnet_path_score=add_wordnet_path_score, sort_flag=True)

        if filter_objects_flag and add_cpt_score:
            # extreme case: all anchors fail the CPT and return 0; the anchor selection would become random 
            # token2cpt[filled_token] = 
            # # print("input_word", input_word)
            # print("token2cpt", token2cpt)
            token2cptsum = dict({token: sum(scores)/len(scores) for token, scores in token2cpt.items()})
            for token, probs in token2probs.items():
                token2probs[token] = probs + token2cptsum[token]
                # print(input_word, token, probs,token2probs[token] )
            
        if top_k is not None and isinstance(top_k, int):
            token2probs = sorted(token2probs.items(), key=lambda x: x[1], reverse=True )
            token2probs = token2probs[:top_k] 
            token2probs = dict(token2probs)
        anchor_list.append(list(token2probs.keys())) 
        anchor_scores.append(token2probs) 

        # print("-"*60)
    return anchor_list if not return_probs  else pd.Series((anchor_list,anchor_scores))


def filter_anchors_with_probs(df, outputs,  scorer, sub_col,  anchor_prompt_col='anchor_lsp_sap', anchor_col='subj_anchors', filter_objects_flag=True,filter_objects_with_input=True, return_probs=True, top_k=10, add_wordnet_path_score=False,  add_cpt_score=False, cpt_unmasker=None, mask_string=None, cpt_only=False):
    if not return_probs:
        df[anchor_col] = filter_outputs_with_probs(df[sub_col].to_list(), outputs[anchor_prompt_col], filter_objects_flag=filter_objects_flag, filter_objects_with_input=filter_objects_with_input, return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['obj_anchors'] = filter_outputs_with_probs(df[sub_col].to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs, top_k=top_k)
        # df['obj_mask_sentence'] = filter_outputs_with_probs(df[sub_col].to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer) #remove this to 
        # df['obj_mask_sap'] = filter_outputs_with_probs(df[sub_col].to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_sentence'].apply(lambda x: x[0])
        return df  
    else:
        df[['subj_anchors', 'subj_anchors_score']] = filter_outputs_with_probs(inputs=df[sub_col].to_list(), 
                                                                                outputs = outputs[anchor_prompt_col], 
                                                                                filter_objects_flag=filter_objects_flag,
                                                                                filter_objects_with_input=filter_objects_with_input,
                                                                                top_k=top_k, 
                                                                                return_probs=return_probs, 
                                                                                scorer=scorer, add_wordnet_path_score=add_wordnet_path_score, 
                                                                                add_cpt_score=add_cpt_score, cpt_unmasker=cpt_unmasker, mask_string=mask_string, cpt_only= cpt_only
                                                                                )
        # df[['obj_anchors', 'obj_anchors_score']] = filter_outputs_with_probs(df[sub_col].to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs)
        # df[['obj_mask_sentence', 'obj_mask_sentence_score']] = filter_outputs_with_probs(df[sub_col].to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_sentence'].apply(lambda x: x[0])
        return df

"""### DAP"""

# def unify_mask(model, text, inp_mask_token, out_mask_token):
#     '''
#     text: the input to be transfered 
#         bert: out_mask_token = [MASK]
#         roberta: out_mask_token = <mask> 
#     '''
#     output = text.replace(inp_mask_token, out_mask_token)
#     return output


def insert_multiple_anchors(original_prompt_source, original_prompts, anchors, sub_position_start, sub_label,  use_original_prompt, incorporate_operation ):
    '''
    original_prompt_source: including the original context, mannually crafted templates, 
    original_prompt: the original prompt, 
    anchors: a list of anchors
    sub_position: 

    return:
        a list of prompts with incorporated anchors
    '''
   

    #print(original_prompt)
    #print(sub_position_start, sub_position_end, type(sub_position_start), type(sub_position_end))
    #print(sub_label, sub_label_string)

    anchored_prompts = []
    if isinstance(original_prompts, list):
        for original_prompt in original_prompts:
            # original_prompt = original_prompt[0]
            sub_position_end = int(sub_position_start) + len(sub_label)
            sub_label_string = original_prompt[sub_position_start : sub_position_end ]

            if use_original_prompt:
                anchored_prompts.append(original_prompt)

            # def insert_anchors_with_or(original_prompt, anchors, sub_label_string, anchors ):
            if incorporate_operation == 'concate_or_single':
                for anchor in anchors:
                    anchored_string = f"{sub_label_string} or {anchor} "
                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
                    anchored_prompts.append(anchor_prompt)
            elif incorporate_operation == 'concate_comma_multiple':
                    anchored_string = "{}, {}".format(sub_label_string, ",".join(anchors))
                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
                    anchored_prompts.append(anchor_prompt)
            elif incorporate_operation == 'replace':
                for anchor in anchors:
                    anchored_string = f"{anchor}"
                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
                    anchored_prompts.append(anchor_prompt)
    return anchored_prompts

def insert_multiple_anchors_by_relacement(original_prompts, anchors, original_string, add_article_for_z=False):
    '''
    replace the placeholder [Z] with true values
    '''
    anchored_prompts = []
    for original_prompt in original_prompts:
        for anchor in anchors: 
            anchor = "{} {}".format(_get_article(anchor), anchor) if add_article_for_z else anchor
            anchored_prompts.append(original_prompt.replace(original_string, anchor))
    return anchored_prompts


def fill_anchor_into_dap(df, relation, relation_to_template, use_dap,  sub_col, anchor_col, incorporate_operation, original_prompt_source='template_sap', top_k_anchors = 1,  dap_col_name='masked_sentences_with_subj_anchor',  mask_string = "[MASK]", use_original_prompt=True, add_article_for_z=False ):
    if use_dap:
        if original_prompt_source == 'template_sap':
            template_sap = relation_to_template[relation]['template_sap']
            template_dap = relation_to_template[relation]['template_dap'] 
            df[dap_col_name] = df[['template_sap', anchor_col, 'sub_position',sub_col]].apply(lambda x: insert_multiple_anchors('template_sap', *x) if x[1][0]!='MISSING' else template_sap.replace("[X]", x[0]), axis=1)

        elif original_prompt_source=='masked_sentences':
                df[dap_col_name] = df[['masked_sentences',anchor_col, 'sub_position', sub_col]].apply(lambda x: insert_multiple_anchors(original_prompt_source, x[0], anchors=x[1], sub_position_start=x[2], sub_label=x[3], use_original_prompt=use_original_prompt, incorporate_operation=incorporate_operation ) if x[1][0]!='MISSING' else x[0], axis=1) 

        elif original_prompt_source in ['lsp_dap', 'def_dap']:#, 'masked_sentences']:
            df[dap_col_name] = df[[original_prompt_source, anchor_col]].apply(lambda x: insert_multiple_anchors_by_relacement(x[0], anchors=x[1],  original_string='[Z]', add_article_for_z=add_article_for_z ) if x[1][0]!='MISSING' else x[0], axis=1) 

    else: #221005: posy: come back to here when some relaitons are not using dap
        df[dap_col_name] = df[[sub_col, anchor_col]].apply(lambda x: template_sap.replace("[X]", x[:top_k_anchors]), axis=1)
    return df 



''''
args: incorporate_operation ['insert', 'replace', 'concate', ]
output: a list of anchored prompts
'''





def save_dap_templates(df, relation, output_dir):
    # Save DAP to the disk 
    # df_out = df[['pred', 'masked_sentences_with_subj_anchor', sub_col, 'obj_label', 'uuid']]
    df_out = df[['masked_sentences_with_subj_anchor', sub_col, 'obj_label', 'uuid']]
    df_out = df_out.rename(columns={'masked_sentences_with_subj_anchor': 'masked_sentences'})
    df_out['masked_sentences'] = df_out['masked_sentences'].apply(lambda x: [x])
    # os.makedirs(output_dir, mode=0o777, exist_ok=True)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_path = f'{output_dir}/{relation}.jsonl'
    # json.dump(df_out.to_dict(orient='records'),  output_path) 
    df_out_json = df_out.to_dict( orient='records')

    save_dict_to_json(df_out_json , output_path)
    # {"sub": "abdomen", "obj": "organs", "pred": "HasA", "masked_sentences": ["The abdomen contains [MASK]."], "obj_label": "organs", "uuid": "767f3c6c02e42a55f8b1c314f7167dab"}
    # return df_out

# df[[sub_col, 'masked_sentences', 'obj_mask_sentence', 'obj_label']].sample(50)
# .to_csv("IsA.sample50.filled.human_verification.incontext.csv") #, 'masked_sentences', 'obj_mask_sentence']].sample(50)

"""# Analysis

## evaluation p@k
"""


from evaluation import mean_average_precision, average_precision_at_k, precision_at_k, recall_at_k
def concept_evaluation(label, pred):
    '''
    
    label: a list with the singualr and plural labels (e.g., ['tool', 'tools'])
    pred: the top K prediction list 

    return:
        1 if label share with pred else 0  
    '''
    if not isinstance(label, list):
        label = eval(label)
        
    if not isinstance(pred, list):
        pred = eval(pred)

    shared = set(label).intersection(set(pred))
    return 1 if len(shared)>0 else 0 
    # return len(shared)/len(pred)
    

def get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):
    '''
    evalaute model predictions in concept level, ignoring the morphology affects (singular, plural)
    '''

    p_at_x = [] #defaultdict() 
    for pred_col in pred_cols: 
        suffix = pred_col.replace(pred_col_suffix, "")
        prec_cur = defaultdict()
        prec_cur['mask_type'] = suffix
        for k in k_list: 
            df[f'p{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: concept_evaluation(x[0], eval(x[1])[:k] if isinstance(x[1], str) else x[1][:k]), axis=1 )
            prec_cur[f'p@{k}'] = round(df[f'p{k}_{suffix}'].mean() , 3)*100

        p_at_x.append(prec_cur)  
        

    # aggregate the average precision across k 
    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res

def get_precision_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):

    p_at_x = [] #defaultdict() 
    for pred_col in pred_cols: 
        suffix = pred_col.replace(pred_col_suffix, "")
        prec_cur = defaultdict()
        prec_cur['mask_type'] = suffix
        for k in k_list: 
            df[f'p{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: 1 if x[0][0] in x[1][:k] else 0, axis=1 )
            prec_cur[f'p@{k}'] = round(df[f'p{k}_{suffix}'].mean() , 3)*100

            # suffix = pred_col.replace("obj_", "")
            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=np.array(x[0]), y_pred=np.array(x[1]), k=k), axis=1 )
            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=x[0], y_pred=x[1], k=k), axis=1 )

        p_at_x.append(prec_cur)  

    # aggregate the average precision across k 
    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res

def get_highest_mrr_among_labels(label, pred):
    '''
    return the highest rank among the multiple labels. This is applicable to single labels as well, if we the single label is put in a list

    pred: a list of words (candidates)
    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)
    '''
    mrr = 0 
    if pred is None: return mrr 

    rank_list = [ pred.index(item) + 1 for item in label if item in pred] 
    if len(rank_list)>0:
        mrr = 1/min(rank_list)

    return mrr 


def get_mrr(df, relation, pred_cols, label_col, pred_col_suffix):
    '''
    mrr is calculated based on the top_k rank, all elements in obj_col are used
    '''
    # def get_mrr_single(label, pred):
    #     '''
    #     pred: a list of words (candidates)
    #     label: the true label 
    #     '''
    #     mrr = 0 
    #     if pred is not None and label in pred:
    #         rank = pred.index(label) + 1
    #         mrr = 1/rank 
    #     return mrr 

    

    mrr = [] 
    for i, pred_col in enumerate(pred_cols):
        cur_mrr = defaultdict()
        suffix = pred_col.replace(pred_col_suffix, "")

        df[f'mrr_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: get_highest_mrr_among_labels(x[0], x[1]), axis=1 ) 
        
        cur_mrr['mask_type'] = suffix
        cur_mrr[f"mrr"] = round(df[f'mrr_{suffix}'].mean(), 3)*100
        mrr.append(cur_mrr)

    mrr_df =  pd.DataFrame(data = mrr) #, columns=['mask_type', 'mrr'])
    # mrr_df['mask_type']= mrr_df['mask_type'].apply(lambda x: x.replace(""))
    mrr_df['relation'] = relation
    return mrr_df 




# def get_mean_average_precision(df, relation, true_col, pred_cols, k=10):
#     '''
#     return the mean average precision for specified pred_cols
#     '''
#     df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

#     # mAP = mean_average_precision(df['obj_label_list'].to_list(), df['obj_mask_sentence'].to_list(), k=10)

#     mAP_dic = defaultdict()

#     for pred_col in pred_cols:
#         mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
#         mAP_dic[pred_col.replace("obj_", "")] = mAP

#     mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
#     mAP_df['relation'] = relation
#     return mAP_df 


# def get_precision_at_k(df, relation, pred_cols, k_list):

#     for k in k_list: 
#         for pred_col in pred_cols: 
#             suffix = pred_col.replace("obj_", "")
#             df[f'p{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: 1 if x[0] in x[1][:k] else 0, axis=1 )

#     p_at_x = defaultdict()
#     for k in k_list:
#         key = f'p@{k}'
#         p_at_x[key] = defaultdict()
#         for col in df.columns:
#             if 'top' in col: continue 
#             if f'p{k}_' in col:
#                 precision = df[col].value_counts(normalize=True).to_dict().get(1, 0) #1 is the key word of correct examples, 0 is the default value to hanld the case of all examples are wrong 
#                 precision = round(precision, 3)*100
#                 p_at_x[key][col.replace(f'p{k}_', '')] = precision 
#     df_res = pd.DataFrame(p_at_x)
#     df_res['relation'] = [relation]*len(df_res)
#     return df_res


def get_average_precision(df, relation, true_col, pred_cols, k_list):
    '''
    return the mean average precision for specified pred_cols
    '''
    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

    mAP_dic = defaultdict()
    for k in k_list:
        key = f'mAP@{k}'
        mAP_dic[key] = defaultdict()
        for pred_col in pred_cols:
            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
            mAP_dic[key][pred_col.replace("obj_", "")] = mAP

    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])
    mAP_df['relation'] = relation
    mAP_df['mask_type'] = mAP_df.index
    return mAP_df


def get_mean_average_precision(df, relation, true_col, pred_cols, k_list):
    '''
    return the mean average precision for specified pred_cols
    '''
    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

    mAP_dic = defaultdict()
    for k in k_list:
        key = f'mAP@{k}'
        mAP_dic[key] = defaultdict()
        for pred_col in pred_cols:
            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
            mAP_dic[key][pred_col.replace("obj_", "")] = mAP

    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])
    mAP_df['relation'] = relation
    mAP_df['mask_type'] = mAP_df.index
    return mAP_df


def get_mean_average_precision_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):
    # get the avearage precision per query
    map_at_x = []
    for pred_col in pred_cols: 
        suffix = pred_col.replace(pred_col_suffix, "")
        map_cur = defaultdict()
        map_cur['mask_type'] = suffix

        for k in k_list: 
            df[f'ap{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: average_precision_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1)
            map_cur[f'mAP@{k}'] = round(df[f'ap{k}_{suffix}'].mean(), 3)*100
        map_at_x.append(map_cur)

    # aggregate the average precision across k 
    df_res = pd.DataFrame(map_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res



def get_recall_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):
    # get the avearage precision per query
    recall_at_x = []
    for pred_col in pred_cols: 
        suffix = pred_col.replace(pred_col_suffix, "")
        recall_cur = defaultdict()
        recall_cur['mask_type'] = suffix

        for k in k_list: 
            df[f'recall_{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: recall_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1 )
            recall_cur[f'recall@{k}'] = round(df[f'recall_{k}_{suffix}'].mean() ,3 )*100
        recall_at_x.append(recall_cur)

    # aggregate the average precision across k 
    df_res = pd.DataFrame(recall_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res


# def get_precision_at_k(df, relation, pred_cols, k_list):

#     p_at_x = [] #defaultdict() 
#     for pred_col in pred_cols: 
#         suffix = pred_col.replace("obj_", "")
#         prec_cur = defaultdict()
#         prec_cur['mask_type'] = suffix
#         for k in k_list: 
#             suffix = pred_col.replace("obj_", "")
#             # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=np.array(x[0]), y_pred=np.array(x[1]), k=k), axis=1 )
#             df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=x[0], y_pred=x[1], k=k), axis=1 )
#             prec_cur[f'p@{k}'] = df[f'p@{k}_{suffix}'].mean() 

#         p_at_x.append(prec_cur)  

#     # aggregate the average precision across k 
#     df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])
#     df_res['relation'] = [relation]*len(df_res)
#     return df_res
    
# df_p_at_k = get_precision_at_k(df, relation='Has', pred_cols=['obj_mask_sentence', 'obj_mask_def_dap'], k_list=[10, 1816])

"""### Analysis helper functions"""

def get_rel_specific_results(df_res_all, mask_type):
# rel = 'mask_sap'
# rel = 'mask_dap'
    dfc = df_res_all.query(f"mask_type == '{mask_type}'").reset_index(drop=True)
    # round(dfc['p@1'].mean(), 2)
    
    overall = {"p@1":   round(dfc['p@1'].mean(), 1),
                "p@3": round(dfc['p@3'].mean(), 1),
                "p@5": round(dfc['p@5'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "mrr": round(dfc['mrr'].mean(), 1),
                "relation": 'Overall'
                # "mrr":  round(dfc['mrr'].mean(), 1),
                # "support": dfc['support'].sum() 
                } 
    
    dfc_overall = pd.DataFrame(overall, index=['Overall'])
    # dfc = dfc.sort_values(by=['p@1'])
    dfc = dfc.sort_values(by=['relation'])

    dfc = pd.concat([dfc, dfc_overall]).reset_index(drop=True).sort_values(['relation'])
    
    return  dfc[['relation', 'p@1','p@10', 'mrr', 'p@3', 'p@5', 'mask_type']]

def calculate_gains(df1, df2):
    '''
    df1: sap
    df2: dap
    '''
    metrics = ['p@1', 'p@3', 'p@5', 'p@10', 'mrr']
    df_gains = []
    for col in metrics:
        gains = df2[col] - df1[col]
        gains.column = col 
        df_gains.append(gains)
    
    df_gains = pd.concat(df_gains, axis=1 )
    df_gains['relation'] = df2.relation
    return df_gains[['relation', 'p@1', 'p@10', 'mrr', 'p@3', 'p@5']]

def display_gains(col1, col2, df_res_all, output_file=None):
    ''' 
    goal: diaplay the gains from col1 to col2
    col1: the baseline column 
    '''
    df1 = get_rel_specific_results(df_res_all, mask_type=col1)
    df2 = get_rel_specific_results(df_res_all, mask_type=col2)
    print(f'gains: {col2} - {col1}')
    df_gains = calculate_gains(df1, df2)
    df_gains['mask_type'] =  [ 'Gains(DAP-SAP)'] * len(df_gains.index)
    
    df_out1 = pd.concat([df1, df2, df_gains], axis=1)
    display(df_out1)
    display(df1)
    display(df2)
    if output_file is not None:
        print(f"Saving final results to {output_file}")
        df_out1.to_csv(f"{output_file}")

def aggregate_candidates(dic1, dic2, top_k):
    '''
    input are two list of candidates [{token: score}]
    goal: merge the two lists 
    1. co-occurred words 
        strategy 1 - average: (score1 + score)/2 
        strategy 2 - accumulate: score1 + score
    2. cut off the number of all candiates to top_k
    '''
    keys = set(dic1.keys()).union(dic2.keys())
    new_dic = defaultdict() 

    for key in keys:
        new_dic[key] = dic1.get(key, 0) + dic2.get(key, 0)
    new_dic = dict(Counter(new_dic).most_common(top_k))
    return list(new_dic.keys())




def get_wordnet_avg_path_between_sub_and_anchors(df, achor_col, oov_path_len = 100):
    '''
    evalaute the anchor quality by measuring
    (1) the average paths between sub_labels and their anchors 
    (2) the coverage of anchors 
    '''
    df['anchor_wordnet_path_len']  = df[[sub_col, achor_col]].apply(lambda x: [(subj_anchor,get_wordnet_shortest_path_length_between(x[0], subj_anchor, oov_path_len=oov_path_len)) for subj_anchor in x[1]] , axis=1)

    # wn_lemmas = set(wn.all_lemma_names())
    path_len_all = []
    count_oov = 0
    count = 0
    
    for path_lens in df['anchor_wordnet_path_len'].to_list():
        for anchor, path_len in path_lens: 
            count +=1
            if path_len!=oov_path_len:
                path_len_all.append(path_len) 
            else:
                count_oov = count_oov + 1

    avearage_path_len = sum(path_len_all)/len(path_len_all)
    coverage = 1 - count_oov/count

    return round(avearage_path_len, 2) , round(coverage, 3)

def load_config(config_file):
    with open(config_file) as f :
        config = yaml.load(f, Loader=yaml.FullLoader)
    return config 

def add_argument_to_config(args, config):
    for name in args.keys():
        if args[name]!=None and name in config:
            config[name] = args[name]
    return config 


# def merge_anchors_in_concept_level(uniform_funcion, words, top_k):
#     '''
#     uniform_function: either signualr or plural
    
#     '''
#     top_k_output =  []
#     for word in words:
#         uniformed_word = uniform_funcion(word) 
#         if uniformed_word not in top_k_output: 
#             top_k_output.append(uniformed_word)
#         if len(top_k_output) == top_k: 
#             return top_k_output

#     return top_k_output[:top_k]


def merge_predictions_in_concept_level(words, uniform_funcion=None, top_k=None ):
    '''
    uniform_function: either signualarize or pluralize 
    '''
    words_uniformed = [uniform_funcion(word) for word in words] if uniform_funcion !=None else words
    concepts = list(OrderedDict.fromkeys(words_uniformed))
    return concepts[:top_k] if top_k is not None else concepts



"""# The MAIN function"""

# ----------------------------------------  config ------------------------------------ 

# dynamic arguments 
parser = argparse.ArgumentParser()
parser.add_argument("--config_file", help="debug or not")
parser.add_argument("--debug", action="store_true",  help="debug or not")
parser.add_argument("--data_dir", default=None, help="debug or not")
parser.add_argument("--sub_col", default=None, help="which col is used as input x, sigular or plural")
parser.add_argument("--filter_anchors_flag", action="store_true",  help="using raw anchors or filterred anchors")
parser.add_argument("--filter_objects_flag", action="store_true",  help="filter the model predictions obj_label or not")
parser.add_argument("--filter_objects_with_anchors", action="store_true",  help="filter the model predictions obj_label or not")
parser.add_argument("--filter_objects_with_input", action="store_true",  help="filter the model predictions obj_label or not")
parser.add_argument("--add_cpt_score", action="store_true", help="adding concept positioning test to filter anchors")
parser.add_argument("--add_wordnet_path_score", action="store_true", help="adding concept positioning test to filter anchors")
parser.add_argument("--constrain_targets", action="store_true", help="constrain the target vocabulary of not")
parser.add_argument("--top_k", type=int, default=10, help="adding concept positioning test to filter anchors")
parser.add_argument("--top_k_anchors", type=int, default=5, help="how many anchors will be inserted into DAP ")
parser.add_argument('--max_anchor_num_list', nargs='+', type=int, default=[10], help= 'the max anchor num' )
parser.add_argument('--scorer_target_N_prompts', type=str, default='freqProbLogSum', help= 'the max anchor num' )
parser.add_argument('--anchor_col_prefix', type=str, default='subj_anchors', help= 'the prefix for anchor col' )
parser.add_argument('--anchor_source', type=str, default='lm', help= 'the source of getting acnhors' )
args = parser.parse_args()
args = vars(args)

debug = args['debug'] 
print(f"debug: {debug}")

config_file = args['config_file']  #sys.argv[1] 
config = load_config(config_file)
config =add_argument_to_config(args, config) 


device=0
test_relations =  config["test_relations"]  #["IsA"] #['HasA', 'HasProperty', 'IsA'] #, 'PartOf', 'MadeOf']

save_all_data= config["save_all_data"]
top_k= config['top_k']
data_dir = config["data_dir"]  

mask_string_mapping = config["mask_string_mapping"]

use_dap_global = config["use_dap_global"]
# max_anchor_num = config["max_anchor_num"]
return_probs = config["return_probs"]

dataset = data_dir.split('/')[1].upper()  #'CLSB' #LAMS

anchor_scorer_list = config["anchor_scorer_list"] #['probAvg'] #['freqProbLogSum']  # #[  'freq', 'probSum', 'probAvg', 'freqProbSum', 'probLogSum', 'freqProbLogSum']
# scorer_anchor = config[""] 'freq' #'freqProbSum'#anchor_anchor_scorer_list[0]
scorer_target_1_prompt = config["scorer_target_1_prompt"]
scorer_target_N_prompts = config["scorer_target_N_prompts"] # 'probAvg' #'probLogSum'

use_original_prompt = config["use_original_prompt"] 
constrain_targets= config["constrain_targets"] 
original_prompt_source = config["original_prompt_source"] #  'masked_sentence' #'templated_double_anchor'
incorporate_operations = config["incorporate_operations"]#, "concate_comma_multiple", "replace" ]

filter_anchors_flag = config["filter_anchors_flag"] #  True   #filter the anchors
filter_objects_flag = config["filter_objects_flag"] #  False  #filter the model predicitons/outputs 
add_wordnet_path_score = config["add_wordnet_path_score"] # True   #use worndet_path_score in anchor scoring function or not 

add_cpt_score= config["add_cpt_score"] #True  #False #
cpt_only= config["cpt_only"]  #False # False # only use cpt to select anchor or not
max_anchor_num_list  = config['max_anchor_num_list']
anchor_types = config['anchor_types']
filter_objects_with_input = config['filter_objects_with_input']
filter_objects_with_anchors = config['filter_objects_with_anchors']
sub_col = config['sub_col']
sub_col_sg= config['sub_col_sg'] 
sub_col_pl= config['sub_col_pl'] 


anchor_col= config['anchor_col_prefix'] #'subj_anchors'
anchor_col_sg= config['anchor_col_prefix'] + '_sg' #'subj_anchors'
anchor_col_pl= config['anchor_col_prefix'] + '_pl'
anchor_col_all = config['anchor_col_prefix'] + '_all'

# anchor_col= config['anchor_col'] #'subj_anchors'
# anchor_col_sg= config['anchor_col_sg'] #'subj_anchors'
# anchor_col_pl= config['anchor_col_pl'] #'subj_anchors'
# anchor_col_all = config['anchor_col_all']

top_k_anchors=config['top_k_anchors']
sub_col_sgpl = config['sub_col_sgpl']
oracle_anchor_inserted = config['oracle_anchor_inserted']
use_oracle_anchor= config['use_oracle_anchor']
anchor_source= config['anchor_source']

from inflection import singularize, pluralize 
# Customizing patterns and uniform_funcion by singular or plural prompts 
if 'plural' in data_dir: 
    singular_pattern_flag = False #False #True
else:
    singular_pattern_flag = True #False #True
if singular_pattern_flag: 
    from patterns_singular import def_sap_patterns, def_dap_patterns, lsp_sap_patterns, lsp_dap_patterns
    from inflection import  singularize as uniform_function
else: 
    from patterns_singular  import def_sap_patterns, def_dap_patterns, lsp_sap_patterns, lsp_dap_patterns
    from inflection import  pluralize as uniform_function
print("-"*40, 'config', '-'*40)
print(config)
print("-"*40, 'config', '-'*40)

bert_vocab= read_bert_vocab(bert_vocab_path = 'data/bert-large-uncased-vocab.txt')
# ----------------------------------------  config ------------------------------------ 

# ----------------------------------------  Prompt ------------------------------------ 


for incorporate_operation in incorporate_operations :
    for max_anchor_num in  max_anchor_num_list :
        for scorer_anchor in anchor_scorer_list :
            for model in mask_string_mapping.keys(): #['bert-large-cased']
                ## get the unmasker 
                if constrain_targets: 
                    target_vocab = get_target_vocab(file_path = data_dir+"vocab.txt")
                    unmasker_obj = get_unmasker(model, device=device, targets=target_vocab) #### initialize the fill-the-blank model 
                    top_k = len(target_vocab) 
                else:
                    unmasker_obj = get_unmasker(model, device=device)
                unmasker_anchor = get_unmasker(model, device=device ) #### initialize the fill-the-blank model 
                mask_string = mask_string_mapping[model]
                save_dir = f'log/{model}/{data_dir.split("/")[-3]}/{data_dir.split("/")[-2]}' 
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                for anchor_type in anchor_types: #["Coordinate_remove_Y_PUNC_FULL"]: #["Coordinate_remove_Y_PUNC"], 
                # ["Coordinate_keep_Y", "Coordinate_remove_Y", "Coordinate_remove_Y_PUNC", "Coordinate_repl_Y_CLS_SEP", "Coordinate_OLD", "Coordinate", "Coordinate_FULL"]: # ,  "synonym"]:
                    print("-"*40, f"{model}-anchor_type: {anchor_type}-scorer_anchor:{scorer_anchor}-op:{incorporate_operation}", "-"*40)
                    relation_to_template  = get_relation_templates(relations, model, anchor_type, def_sap_patterns, def_dap_patterns, lsp_sap_patterns, lsp_dap_patterns)
                    print(json.dumps(relation_to_template, indent=4))

                    dfs = []
                    df_res_all = []
                    # for relation in tqdm(relation_to_template.keys()):
                    # for relation in tqdm(['MadeOf']): #,'WAX''ReceivesAction', 'CapableOf',   'HasA']):'HasProperty' 'HasProperty'
                    for relation in test_relations: #, 'IsA', 'Has', 'HasA', 'MadeOf']: #['IsA', 'HasA']:
                    # for relation in tqdm (['HasProperty']): #(["MotivatedByGoal"] ): 
                    # for relation in tqdm(['CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction', 'HasProperty']):

                        if not relation_to_template[relation]['use_dap'] and not use_dap_global : 
                            print(f"skipping {relation}")
                            continue 
                        filepath = f'{data_dir}/{relation}.jsonl'
                        print(f"Processing {relation} ... {filepath}")
                        df = get_masked_data(filepath, sub_col, sub_col_sg, sub_col_pl,  relation, relation_to_template[relation], model, singular_pattern_flag, clean_test=True, anchor_probe=config['anchor_probe'])
                        df[sub_col_sgpl] = df[[sub_col, sub_col_sg, sub_col_pl]].apply(lambda x: f"{x[0]} {x[1]} {x[2]}", axis=1) #x[0] + " " +
                        df['obj_in_BERT'] = df['obj_label'].apply(lambda x: 1 if x[0] in bert_vocab else 0)
                        df = df.query("obj_in_BERT == 1 ").reset_index(drop=True)
                        print(f"#Valid Instances: {len(df.index)} in {dataset} (hyper in BERT)")
                       
                        if debug:
                            df = df.head(10)

                        print("\t step1: get anchors (fill and filter)")
                        # step1: get the anchor
                        outputs = fill_mask_obj(df, unmasker_obj, top_k=top_k*2)  #fill the original prompts with PTLM 

                        df[['obj_mask_sentence', 'obj_mask_sentence_score']] = filter_outputs_with_probs(df[sub_col_sgpl].to_list(), 
                                                                                                            outputs['masked_sentences'], 
                                                                                                            return_probs=return_probs, 
                                                                                                            top_k= 2* top_k, 
                                                                                                            scorer=scorer_target_N_prompts,
                                                                                                            filter_objects_flag=filter_objects_flag,
                                                                                                            filter_objects_with_input = filter_objects_with_input 
                                                                                                            )
                        df.to_csv("log/debug.csv")
                        for pattern in ['def_sap', 'lsp_sap']:
                            print("---", pattern)
                            df[[f'obj_mask_{pattern}', f'obj_mask_{pattern}_score']] = filter_outputs_with_probs(df[sub_col_sgpl].to_list(), 
                                                                                                                 outputs[pattern], 
                                                                                                                filter_objects_flag=filter_objects_flag,
                                                                                                                filter_objects_with_input = filter_objects_with_input,
                                                                                                                return_probs=return_probs, 
                                                                                                                top_k=2*top_k, 
                                                                                                                scorer=scorer_target_N_prompts)

                        # df = add_filter_outputs(df, outputs) #filter the anchor candiates
                        if use_dap_global:
                            if not oracle_anchor_inserted:
                                if not use_oracle_anchor: #generate anchors dynamically
                                    outputs.update(fill_mask_anchor(df, unmasker_anchor, anchor_prompt_col='anchor_lsp_sap', top_k = max_anchor_num*2))  #fill the anchor prompts with PTLM 
                                    df = filter_anchors_with_probs(df, outputs,
                                                                    scorer = scorer_anchor, 
                                                                    sub_col = sub_col_sgpl,
                                                                    anchor_prompt_col='anchor_lsp_sap', 
                                                                    anchor_col=anchor_col,
                                                                    filter_objects_flag = filter_anchors_flag,
                                                                    filter_objects_with_input = filter_objects_with_input,
                                                                    return_probs=True, 
                                                                    top_k=max_anchor_num, 
                                                                    add_wordnet_path_score=add_wordnet_path_score, 
                                                                    add_cpt_score= add_cpt_score, 
                                                                    cpt_unmasker= unmasker_anchor, 
                                                                    mask_string=mask_string, 
                                                                    cpt_only=cpt_only
                                                                    ) #filter the anchor candiates

                            use_dap = relation_to_template[relation]['use_dap']  if not use_dap_global else use_dap_global 

                            
                            df[anchor_col_sg] = df[anchor_col].apply(lambda x: merge_predictions_in_concept_level( x, uniform_funcion=singularize, top_k=top_k_anchors))
                            df[anchor_col_pl] = df[anchor_col].apply(lambda x: merge_predictions_in_concept_level( x, uniform_funcion=pluralize, top_k=top_k_anchors))
                            df[anchor_col_all] = df[anchor_col].apply(lambda x: merge_predictions_in_concept_level( x, uniform_funcion=singularize, top_k=None))
                         
                            if filter_objects_with_anchors: 
                                df['subj_anchors_combined'] = df[[sub_col, sub_col_sg, sub_col_pl, anchor_col, anchor_col_sg, anchor_col_pl]].apply(lambda x: f"{x[0]} {x[1]} {x[2]} " + " ".join(x[3]) + " " + " ".join(x[4]) + " " + " ".join(x[5]), axis=1) #x[0] + " " +
                            else:
                                df['subj_anchors_combined'] = df[[sub_col, sub_col_sg, sub_col_pl]].apply(lambda x: f"{x[0]} {x[1]} {x[2]}", axis=1) #x[0] + " " +

                            
                            print(f"relation: {relation} \t use_dap: {use_dap}")
                            # step2: fill DAP
                            print("\t step2: insert anchors")
                            # # fill the def_sap (masked_sentences) with anchors
                            print("sub_col", sub_col)
                            # (df, relation, relation_to_template, use_dap,  sub_col, anchor_col, incorporate_operation,original_prompt_source='template_sap', top_k_anchors = 1,  dap_col_name='masked_sentences_with_subj_anchor',  mask_string = "[MASK]", use_original_prompt=True ):
                            if not oracle_anchor_inserted:
                                df = fill_anchor_into_dap(df, relation, relation_to_template, use_dap,
                                                        sub_col = sub_col, 
                                                        anchor_col = anchor_col, 
                                                        incorporate_operation = incorporate_operation , 
                                                        original_prompt_source= 'masked_sentences', 
                                                        top_k_anchors = top_k_anchors, 
                                                        dap_col_name='masked_sentences_with_subj_anchor',
                                                        mask_string = mask_string, 
                                                        use_original_prompt=use_original_prompt, 
                                                        add_article_for_z=True
                                                        ) #create the dap promtps/masks 
    
                                df = fill_anchor_into_dap(df, relation, relation_to_template, use_dap,
                                                        sub_col = sub_col_sg, 
                                                        anchor_col = anchor_col_sg, 
                                                        incorporate_operation = incorporate_operation , 
                                                        original_prompt_source= 'def_dap', 
                                                        top_k_anchors = top_k_anchors, 
                                                        dap_col_name='def_dap_with_subj_anchor',
                                                        mask_string = mask_string, 
                                                        use_original_prompt=use_original_prompt,
                                                        add_article_for_z=True
                                                        ) #create the dap promtps/masks 
    
                                # fill the def_sap (masked_sentences) with anchors
                                df = fill_anchor_into_dap(df, relation, relation_to_template, use_dap,
                                                        sub_col = sub_col_pl, 
                                                        anchor_col = anchor_col_pl, 
                                                        incorporate_operation = incorporate_operation , 
                                                        original_prompt_source= 'lsp_dap', 
                                                        top_k_anchors = top_k_anchors, 
                                                        dap_col_name='lsp_dap_with_subj_anchor',
                                                        mask_string = mask_string, 
                                                        use_original_prompt=False, 
                                                        add_article_for_z= False
                                                        ) #create the dap promtps/masks 
    
                            if use_dap:
                                #step3: filter outputs 
                                print("\t step3: fill mask in dap anchors")
                                # outputs['obj_mask_sentence_dap'] = [unmasker_obj(x, top_k=2*top_k) for x in tqdm(df['masked_sentences_with_subj_anchor'].to_list())]
                                outputs['obj_mask_def_dap'] = [unmasker_obj(x, top_k=2*top_k) for x in tqdm(df['def_dap_with_subj_anchor'].to_list())]
                                outputs['obj_mask_lsp_dap'] = [unmasker_obj(x, top_k=2*top_k) for x in tqdm(df['lsp_dap_with_subj_anchor'].to_list())]

                               
                                #TODO: 221014 note that the follwing code is likely to affect the results a lot: whether using the anchors to filter the targets.
                                # need experiments to compare
                                # df[['obj_mask_sentence_dap', 'obj_mask_sentence_dap_score']] = filter_outputs_with_probs(df.subj_anchors_combined.to_list(), 
                                #                                                                         outputs['obj_mask_sentence_dap'],  
                                #                                                                         return_probs=return_probs, 
                                #                                                                         top_k= 2* top_k, 
                                #                                                                         scorer= scorer_target_N_prompts,
                                #                                                                         filter_objects_flag = filter_objects_flag,
                                #                                                                         filter_objects_with_input = filter_objects_with_input 
                                #                                                                         )

                                df[['obj_mask_def_dap', 'obj_mask_def_dap_score']] = filter_outputs_with_probs(df.subj_anchors_combined.to_list(), 
                                                                                                        outputs['obj_mask_def_dap'],  
                                                                                                        return_probs=return_probs, 
                                                                                                        top_k= 2*top_k, 
                                                                                                        scorer= scorer_target_N_prompts,
                                                                                                        filter_objects_flag = filter_objects_flag,
                                                                                                        filter_objects_with_input = filter_objects_with_input 
                                                                                                        )

                                df[['obj_mask_lsp_dap', 'obj_mask_lsp_dap_score']] = filter_outputs_with_probs(df.subj_anchors_combined.to_list(), 
                                                                                                        outputs['obj_mask_lsp_dap'],  
                                                                                                        return_probs=return_probs, 
                                                                                                        top_k= 2*top_k, 
                                                                                                        scorer= scorer_target_N_prompts,
                                                                                                        filter_objects_flag = filter_objects_flag,
                                                                                                        filter_objects_with_input = filter_objects_with_input
                                                                                                        )
                                # merge the predictions in concept level before evaluation
                                # words, uniform_funcion=singularize, top_k=top_k 
                                df['obj_mask_def_sap'] = df['obj_mask_def_sap'].apply(lambda x: merge_predictions_in_concept_level(x, uniform_funcion=None, top_k=top_k))
                                df['obj_mask_def_dap'] = df['obj_mask_def_dap'].apply(lambda x: merge_predictions_in_concept_level(x, uniform_funcion=None, top_k=top_k))
                                df['obj_mask_lsp_sap'] = df['obj_mask_lsp_sap'].apply(lambda x: merge_predictions_in_concept_level(x, uniform_funcion=singularize, top_k=top_k))
                                df['obj_mask_lsp_dap'] = df['obj_mask_lsp_dap'].apply(lambda x: merge_predictions_in_concept_level(x, uniform_funcion=singularize, top_k=top_k))

                                #df['obj_mask_lsp_sap'] = df['obj_mask_lsp_sap'].apply(lambda x: [singularize(word) for word in x])
                                #df['obj_mask_lsp_dap'] = df['obj_mask_lsp_dap'].apply(lambda x: [singularize(word) for word in x])

                                # step4: merge dap and sap
                                # df['obj_mask_def_sap_dap'] = df[[ 'obj_mask_sentence_score', 'obj_mask_def_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                                # df['obj_mask_def_lsp_sap'] = df[[ 'obj_mask_sentence_score', 'obj_mask_lsp_sap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                                # df['obj_mask_lsp_sap_dap'] = df[[ 'obj_mask_lsp_sap_score', 'obj_mask_lsp_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                                # df['obj_mask_sentence_dap'] = df[['obj_mask_sentence_score', 'obj_mask_def_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                            else:
                                df['subj_anchors_combined'] = df[[sub_col, 'subj_anchors']].apply(lambda x: x[0] + " " + " ".join(x[1]), axis=1)
                                df['obj_mask_def_dap'] = df.obj_mask_sap

                            display(df.head())
                            # save_dap_templates(df, relation, output_dir=f'{save_dir}/dap/{anchor_type}')

                            # Evaluations
                            # df['obj_mask_lsp_sap'] = df['obj_mask_lsp_sap'].apply(lambda x: [singularize(word) for word in x])
                            # df['obj_mask_lsp_dap'] = df['obj_mask_lsp_dap'].apply(lambda x: [singularize(word) for word in x])
                            # df['obj_mask_def_sap_uniform'] = df['obj_mask_def_sap'].apply(lambda x: [uniform_function(word) for word in x])
                            # df['obj_mask_lsp_sap_uniform'] = df['obj_mask_lsp_sap'].apply(lambda x: [uniform_function(word) for word in x])

                            # df['obj_mask_def_dap_uniform'] = df['obj_mask_def_dap'].apply(lambda x: [uniform_function(word) for word in x])
                            # df['obj_mask_lsp_dap_uniform'] = df['obj_mask_lsp_dap'].apply(lambda x: [uniform_function(word) for word in x])

                        ################################################# Evaluation: anchors ################################################# 
                        if 'sub_sister' in df.columns:
                            print("-"*40,"anchor evaluation", "-"*40)
                            pred_col_suffix=''
                            label_col = 'sub_sister'
                            pred_cols = [anchor_col_sg, anchor_col_all] #['subj_anchors']
                            df_prec_anchor = get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list=[1, 5, 10],pred_col_suffix=pred_col_suffix ) ##note that this would be super slow when top_k is large (>1000) 
                            df_mrr =  get_mrr(df, relation, pred_cols, label_col, pred_col_suffix)
                            df_prec_anchor['mrr'] = df_prec_anchor['mask_type'].apply(lambda x:  df_mrr.loc[df_mrr['mask_type']==x, f'mrr'].values[0])

                            # df_mAP = get_mean_average_precision_at_k(df, relation, pred_cols,label_col, k_list=[1, 5, max_anchor_num], pred_col_suffix=pred_col_suffix)
                            # df_recall = get_recall_at_k(df, relation, pred_cols, label_col, k_list=[1, 5, max_anchor_num], pred_col_suffix=pred_col_suffix)
                            # for k in [1, 5, max_anchor_num]:
                                # df_prec_anchor[f'mAP@{k}'] = df_prec_anchor['mask_type'].apply(lambda x:  df_mAP.loc[df_mAP['mask_type']==x, f'mAP@{k}'].values[0])
                                # df_prec_anchor[f'recall@{k}'] = df_recall['mask_type'].apply(lambda x:  df_recall.loc[df_mAP['mask_type']==x, f'recall@{k}'].values[0])

                            # add WordNet path score for evaluation 
                            anchor_wordnet_avg_path, anchor_wordnet_coverage = get_wordnet_avg_path_between_sub_and_anchors(df, anchor_col, oov_path_len = 100)
                            df_prec_anchor['anchor_wordnet_avg_path'] = anchor_wordnet_avg_path
                            df_prec_anchor['anchor_wordnet_coverage'] = anchor_wordnet_coverage

                            df_prec_anchor_display = df_prec_anchor[["mask_type", "p@1", "p@5", "p@10", 'mrr', "anchor_wordnet_avg_path", "anchor_wordnet_coverage" ]] #f"p@{max_anchor_num}", "relation",  
                                                       #"mAP@1", "mAP@5" , f"mAP@{max_anchor_num}", "recall@1", "recall@5", f"recall@{max_anchor_num}", 
                                                       # ]]
    
                            df_prec_anchor['label'] = 'sub_sister'
                            # print(tabulate(df_prec_anchor_display, headers='firstrow', tablefmt='simple'))
                            print(tabulate(df_prec_anchor_display, tablefmt='latex', headers=df_prec_anchor_display.columns).replace("\\", "").replace("&", "\t"))
                            # display(df_prec_anchor_display)


                        ################################################# Evaluation: obj_label ################################################# 
                        print("-"*40,"obj_label evaluation", "-"*40)
                        pred_col_suffix='obj_mask_'
                        df['obj_label_sg'] = df['obj_label']#.apply(lambda x: [singularize(word) for word in x])
                        label_col = 'obj_label_sg'

                        pred_cols  =[col  for col in df.columns if col.startswith(pred_col_suffix) and "_score" not in col]  #predicted target cols, e
                        # df_prec = get_precision_at_k(df, relation, pred_cols, k_list = [1,3,10,top_k])
                        df_prec = get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list=[1, 5, 10, top_k],pred_col_suffix=pred_col_suffix ) ##note that this would be super slow when top_k is large (>1000) 
                        # MRR  metric is useful when we want our system to return the best relevant item and want that item to be at a higher position
                        df_mrr =  get_mrr(df, relation, pred_cols, label_col, pred_col_suffix)
                        # display(df_mrr)
                        df_prec['mrr'] = df_prec['mask_type'].apply(lambda x:  df_mrr.loc[df_mrr['mask_type']==x, f'mrr'].values[0])

                        # df_mAP = get_mean_average_precision_at_k(df, relation, pred_cols,label_col, k_list=[1, 5, 10, top_k], pred_col_suffix=pred_col_suffix)
                        # df_recall = get_recall_at_k(df, relation, pred_cols, label_col, k_list=[1, 5, 10, top_k], pred_col_suffix=pred_col_suffix)
                        # df_mAP = get_mean_average_precision(df, relation, 'obj_label', pred_cols, k_list=[10, top_k]) 
                        # for k in [1, 5, 10, top_k]:
                        #     df_prec[f'mAP@{k}'] = df_prec['mask_type'].apply(lambda x:  df_mAP.loc[df_mAP['mask_type']==x, f'mAP@{k}'].values[0])
                        #     df_prec[f'recall@{k}'] = df_recall['mask_type'].apply(lambda x:  df_recall.loc[df_mAP['mask_type']==x, f'recall@{k}'].values[0])

                        # add WordNet path score for evaluation 
                        anchor_wordnet_avg_path, anchor_wordnet_coverage = get_wordnet_avg_path_between_sub_and_anchors(df, anchor_col, oov_path_len = 100)
                        df_prec['anchor_wordnet_avg_path'] = anchor_wordnet_avg_path
                        df_prec['anchor_wordnet_coverage'] = anchor_wordnet_coverage
                        # df_prec = df_prec[['mask_type', 'p@1', 'p@10', 'mrr', 'p@3', 'mAP', 'relation' ]]

                        df_prec_display = df_prec[["mask_type", 'p@1', 'p@5', 'p@10', "mrr", "anchor_wordnet_avg_path", "anchor_wordnet_coverage"]] # "mAP@1", "mAP@5", "mAP@10", "mAP@1", "mAP@5", "mAP@10",  'recall@1', 'recall@5', 'recall@10', "relation",

                        mask_type_id = { "def_sap": 1, "def_dap": 2, "lsp_sap": 3, "lsp_dap":4} #"sentence": 1, "sentence_dap":2, 
                        df_prec_display = df_prec_display.query(f"mask_type in {list(mask_type_id.keys())}")
                        df_prec_display['mask_type_id'] = df_prec_display['mask_type'].apply(lambda x: mask_type_id.get(x))
                        print(tabulate(df_prec_display, tablefmt='latex', headers=df_prec_display.columns).replace("\\", "").replace("&", "\t"))

                        df_res_all.append( df_prec )
                        df['relation'] = [relation]*len(df)
                        dfs.append(df)

                    # Aggregate and save results and saving 
                    
                    df_res_all = pd.concat(df_res_all).reset_index(drop=True)
                    df_res_all['label'] = 'obj_label'
                    if len(test_relations)>1:
                        for name,group in df_res_all.groupby('relation'):
                            display(group)
                    # if not debug:
                    file_results = f'{save_dir}/df_all_use_global_dap_{use_dap_global}_anchor_type_{anchor_type}_{incorporate_operation}_max_anchor_num_{top_k_anchors}_anchor_scorer_{scorer_anchor}_filter_obj_{filter_objects_flag}_filter_objects_with_input_{filter_objects_with_input}_wnp_{add_wordnet_path_score}_cpt_{add_cpt_score}_anchor_source_{anchor_source}.{dataset}.tsv'

                    # df_res_all.to_csv(file_results, sep='\t')
                    pd.concat([df_prec_anchor, df_res_all]).to_csv(file_results, sep='\t')
                    print(f"save {file_results}")
                    
                    if save_all_data:
                        file_data_results = f'{save_dir}/exp_data_results_anchor_type_{anchor_type}_{incorporate_operation}_max_anchor_num_{top_k_anchors}_anchor_scorer_{scorer_anchor}_filter_obj_{filter_objects_flag}_filter_objects_with_input_{filter_objects_with_input}_wnp_{add_wordnet_path_score}_cpt_{add_cpt_score}_anchor_source_{anchor_source}.{dataset}.csv' 
                        dfs_data = pd.concat(dfs)
                        dfs_data.to_csv(file_data_results)
                        print(f"save {file_data_results}")
                    
