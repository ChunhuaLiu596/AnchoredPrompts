{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group consistency check\n",
    "\n",
    "insert anchors into the two patterns and compare their performance with (without) anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util_wordnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c791fbf0fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minflection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpluralize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingularize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil_wordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_sister_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util_wordnet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "import copy\n",
    "import re \n",
    "import math\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from copy import deepcopy\n",
    "import pathlib\n",
    "\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import string\n",
    "from inflection import pluralize, singularize\n",
    "from util_wordnet import get_sister_terms\n",
    "from transformers import pipeline\n",
    "\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "STOP_WORDS = en.Defaults.stop_words\n",
    "\n",
    "from IPython.display import display\n",
    "from df_to_latex import DataFrame2Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"## Import Modules\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import json \n",
    "import math\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import re \n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import  defaultdict, Counter, OrderedDict\n",
    "from transformers import pipeline\n",
    "import numpy as np \n",
    "import copy\n",
    "import math\n",
    "import configparser\n",
    "import yaml \n",
    "from IPython.display import display\n",
    "##### spacy \n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "STOP_WORDS = en.Defaults.stop_words\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from utils_wordnet import get_wordnet_shortest_path_score_between, get_wordnet_avg_path_score_between_sub_and_anchors, get_wordnet_shortest_path_length_between\n",
    "from utils_concept_positioning_test import concpet_positioning_test, get_revserse_prompt\n",
    "from inflection import singularize, pluralize \n",
    "\n",
    "\"\"\"Anchored_Prompts.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1CnAjbN-8oYPkxM0q4NT1uXVf5KV-GxZ8\n",
    "\"\"\"\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install spacy\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"## Patterns\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "relations = [\n",
    "    {\n",
    "        \"relation\": \"IsA\",\n",
    "        \"def_sap\": [\"A [X] is a type of [Y] .\"],\n",
    "        \"def_dap\": [\"A [X] or [Z] is a type of [Y] .\"],\n",
    "        \"lsp_sap\": [\"such [Y] as [X]\"],\n",
    "        \"lsp_dap\": [\"such [Y] as [X] or [Z]\"],\n",
    "        \"use_dap\": True,\n",
    "        \"anchor_target\": \"[X]\",\n",
    "        \"anchor_lsp_sap\": [\"Such as [X] and [MASK] .\"],\n",
    "        # \"dap_x\": \"A [X] and [Z] are a type of [Y] .\",\n",
    "        # \"dap_y\": \"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def get_relation_templates(relations, model, anchor_type, def_sap_patterns, def_dap_patterns, lsp_sap_patterns, lsp_dap_patterns): \n",
    "    '''\n",
    "    generate the relation tempaltes based on model and anchor_type \n",
    "\n",
    "    '''\n",
    "    # let's assumet the anchor concept is [Z] \n",
    "    relation_to_template = defaultdict() \n",
    "    for relation_initial in relations:\n",
    "        # print(relation.keys())\n",
    "        relation_name = relation_initial['relation'] \n",
    "        # relation_to_template[key] = defaultdict()\n",
    "        relation = copy.deepcopy(relation_initial)\n",
    "        # anchor_target  = relation[\"anchor_target\"]\n",
    "\n",
    "        relation['anchor_lsp_sap']  = lsp_sap_patterns[anchor_type]\n",
    "        relation['def_sap']  = def_sap_patterns[relation_name]\n",
    "        relation['def_dap']  = def_dap_patterns[relation_name]\n",
    "\n",
    "        relation['lsp_sap']  = lsp_sap_patterns[relation_name]\n",
    "        relation['lsp_dap']  = lsp_dap_patterns[relation_name]\n",
    "\n",
    "        if relation[\"anchor_target\"] == '[X]':\n",
    "            for pattern in ['def_sap', 'lsp_sap', 'def_dap', 'lsp_dap', 'anchor_lsp_sap']:\n",
    "                if 'roberta' in model:\n",
    "                    relation[pattern] = [item.replace(\"[Y]\", \"<mask>\") for item in relation[pattern]]\n",
    "                elif 'bert' in model: \n",
    "                    relation[pattern] = [item.replace(\"[Y]\", \"[MASK]\") for item in relation[pattern]]\n",
    "            \n",
    "        # save_dict_to_json(new_relations, 'log/relation_sap_dap.json')\n",
    "        # if debug:\n",
    "            # print( json.dumps(relation_to_template, indent=4)\n",
    "        relation_to_template[relation_name] = relation \n",
    "    return relation_to_template\n",
    "\n",
    "\n",
    "\"\"\"## Helper Functions\"\"\"\n",
    "\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\"\"\"## Helper Functions\"\"\"\n",
    "\n",
    "def save_dict_to_json(examples, output_path):\n",
    "    ''' \n",
    "    save a list of dicts into otuput_path, orient='records' (each line is a dict) \n",
    "    examples: a list of dicts\n",
    "    output_path: \n",
    "    '''\n",
    "    # if not os.path.exists(output_path):\n",
    "        # os.path.makedirs(output_path)\n",
    "\n",
    "    with open(output_path, 'w') as fout:\n",
    "        for example in examples:\n",
    "            json.dump(example, fout)\n",
    "            fout.write(\"\\n\")\n",
    "        print(f\"save {output_path} with {len(examples)} lines\")\n",
    "\n",
    "\"\"\"## Load Data\"\"\"\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "  sentence = sentence.replace(\"\\\"\", \"\").lower()\n",
    "  tokens = [token.orth_ for token in nlp(sentence)]\n",
    "  return tokens\n",
    "\n",
    "def locate_sub_obj_position(ent, sentence, index_not_in) :\n",
    "  ''' \n",
    "  function: find the index of ent in a sentence, the result will be used to filter instances whose ent cannot be find at their sentences\n",
    "  args: \n",
    "    sentence: the sentnces to mask, could be the string or a list of tokens \n",
    "    ent: the ent to be found (sub_label) \n",
    "    index_not_in: the default index for failed instances (an ent not in a sentence)\n",
    "  ''' \n",
    "\n",
    "  if isinstance(sentence, list):\n",
    "    if ent not in sentence:\n",
    "      return index_not_in\n",
    "    return sentence.index(ent)  \n",
    "  else:\n",
    "    sentence = copy.deepcopy(sentence).lower()\n",
    "    if isinstance(sentence, str):\n",
    "      try:\n",
    "        index = sentence.index(ent)\n",
    "        return  index \n",
    "      except: \n",
    "        print(f\"NOT FOUND sub_label: {ent} -> in sentence: {sentence}\")\n",
    "        return index_not_in\n",
    "      \n",
    "        print(ent, sentence)\n",
    "        return index_not_in\n",
    "\n",
    "def remove_noisy_test_data(df):\n",
    "  ''' \n",
    "  relation=\"hasproperty\"\n",
    "  why? some data points don't belong to this relation types \n",
    "  case1., sub_label=number, such as \"10 is ten.\"  We don't say ten is the property of 10\n",
    "  case2, sub_label = 'person_name' and obj_label = 'nuts;, such as \"\"Andrew is [MASK].\", [MASK]=nuts\n",
    "  '''\n",
    "  sub_labels_to_exclude = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '30', '5', '50', '60', '7', '70', '70s', '80', '9', '90']\n",
    "  obj_labels_to_exclude  = ['nuts']\n",
    "  df = df.query(f\"sub_label not in {sub_labels_to_exclude}\")\n",
    "  df = df.query(f\"sub_label not in {obj_labels_to_exclude}\")\n",
    "  return  df.reset_index(drop=True)\n",
    "\n",
    "# def exist_\n",
    "  \n",
    "def load_data(filepath, clean_test=True, tokenize=False):\n",
    "  '''\n",
    "  return the cleaned data\n",
    "  args:\n",
    "    tokenize: if True: the maksed_sentences will be tokenzied (this is slwoers); \n",
    "            otherwise, we use the string match to filter the failed sentences\n",
    "    clean_test: default is True. We filter out some noisy samples spoted by huamns \n",
    "               Note that this is relation specific \n",
    "\n",
    "  '''\n",
    "  index_not_in = 10000\n",
    "\n",
    "  with open(filepath, 'r', encoding='utf-8') as fin:\n",
    "    data = fin.readlines()\n",
    "    data = [eval(x) for x in data]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "#     df['obj_label'] = df['obj_label'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "#   if tokenize:\n",
    "#     df['masked_sentence_tokens'] = df['masked_sentences'].apply(lambda x: tokenize_sentence(x[0]))\n",
    "#     df['sub_position'] = df[['sub_label', 'masked_sentence_tokens']].apply(lambda x: locate_sub_obj_position(x[0], x[1], index_not_in=index_not_in), axis=1)\n",
    "\n",
    "#   if clean_test: \n",
    "#     df = remove_noisy_test_data(df)\n",
    "#     df['sub_position'] = df[['sub_label', 'masked_sentences']].apply(lambda x: locate_sub_obj_position(x[0], x[1][0], index_not_in), axis=1)\n",
    "#     df = df.query(f\"sub_position !={index_not_in}\") #.reset_index() #cue can not be matched in the sentence\n",
    "\n",
    "  print(f\"#Test_instances: {len(df.index)}\")\n",
    "  return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _get_article(word):\n",
    "    \n",
    "    if word[0] in ['a', 'e', 'i', 'o', 'u']:\n",
    "        return 'an'.capitalize()\n",
    "    return 'a'.capitalize()\n",
    "\n",
    "def check_articles(sub_label, pattern):\n",
    "    '''\n",
    "    select suitable article templates for a given sub_label \n",
    "    '''\n",
    "    first_word = pattern.split()[0]\n",
    "    if first_word == sub_label:  \n",
    "        return pattern.capitalize()\n",
    "\n",
    "    elif first_word != sub_label: \n",
    "        sub_article = _get_article(sub_label)\n",
    "        return pattern  if sub_article == first_word  else 'DISCARD'\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_masked_data(filepath, relation, relation_to_template, model, singular_pattern_flag, debug=False, clean_test=True):\n",
    "  df = load_data(filepath, clean_test=clean_test)\n",
    "  if debug:\n",
    "      df = df.sample(200)\n",
    "\n",
    "  name_to_patterns = {'def_sap': def_sap_patterns, 'def_dap': def_dap_patterns, 'lsp_sap': lsp_sap_patterns, 'lsp_dap': lsp_dap_patterns, }\n",
    "                  \n",
    "  #   replace [X] by mask\n",
    "  if 'roberta' in model :\n",
    "    df['masked_sentences'] = df['masked_sentences'].apply(lambda x: [sentence.replace(\"[MASK]\", \"<mask>\") for sentence in x])\n",
    "\n",
    "  for pattern in ['def_sap', 'def_dap']:# , 'lsp_sap', 'anchor_lsp_sap']:\n",
    "    # use check when sub_label are sigular, we nned to inderst an article before it\n",
    "    if singular_pattern_flag:\n",
    "      df[pattern] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern] if check_articles(x, item)!='DISCARD' ])\n",
    "    else:\n",
    "      df[pattern] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern] ])\n",
    "\n",
    "  for pattern in ['lsp_sap', 'lsp_dap', 'anchor_lsp_sap']:\n",
    "      df[pattern] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern]])\n",
    "\n",
    "#   for pattern in ['def_dap', 'lsp_dap']:\n",
    "    #   df[pattern] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in relation_to_template[pattern] if check_articles(x, item)!='DISCARD'])\n",
    "\n",
    "  if debug:\n",
    "    print(df.columns)\n",
    "    print(f\"{len(df.index)} instances\")\n",
    "\n",
    "  return df \n",
    "\n",
    "# if debug:\n",
    "def test_load_data():\n",
    "    for relation in ['IsA']: #['HasProperty']: # ['HasProperty', 'CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction']:\n",
    "        print(f\"relation: {relation}\")\n",
    "        filepath = f'./clsb/{relation}.jsonl'\n",
    "        # df = load_data(filepath, clean_test=True, tokenize=False)\n",
    "        relation_to_template  = get_relation_templates(relations, model, anchor_type='Coordinate')\n",
    "        df = get_masked_data(filepath, relation, relation_to_template[relation], model)\n",
    "        display(df.head())\n",
    "\n",
    "\"\"\"## Define model and feed masked data into models\"\"\"\n",
    "\n",
    "def get_unmasker(model, device, targets=None):\n",
    "    if targets is None: \n",
    "        unmasker = pipeline('fill-mask', model=model, device=device)# 'bert-large-uncased') #initialize the masker\n",
    "    else:\n",
    "        unmasker = pipeline('fill-mask', model=model, device=device, targets=targets )# 'bert-large-uncased') #initialize the masker\n",
    "    return unmasker\n",
    "\n",
    "\n",
    "def get_target_vocab(file_path = \"data/clsb/blank/vocab.txt\"):\n",
    "    '''\n",
    "    Weir et al., (2020) used two variants of target vocabulary when predicting the possible properties \n",
    "    One of them is the SNES version, \"comprising the set of human completion that fit the given prompt syntax for all concepts in the study\"\n",
    "    '''\n",
    "    df_target_vocab = pd.read_csv(file_path, header=None) #.to_list()\n",
    "    target_vocab = df_target_vocab.iloc[:,0].to_list() \n",
    "    return target_vocab\n",
    "\n",
    "\n",
    "\n",
    "def develop_single_case():\n",
    "    target_vocab = get_target_vocab(file_path = \"data/clsb/blank/vocab.txt\")\n",
    "    unmasker = get_unmasker(model= 'bert-base-cased', targets= target_vocab )\n",
    "    text = 'Everyone knows that a bear has [MASK] .'\n",
    "    results = unmasker(text, top_k= len(target_vocab))\n",
    "\n",
    "    for x in results[:10]:\n",
    "        print(x)\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    unmasker = get_unmasker(model= 'bert-base-cased')\n",
    "    text = 'Everyone knows that a bear has [MASK] .'\n",
    "    results = unmasker(text, top_k= 10)\n",
    "    for x in results[:10]:\n",
    "        print(x)\n",
    "\n",
    "def fill_mask_anchor(df, unmasker, anchor_col):\n",
    "    outputs = defaultdict()\n",
    "    outputs[anchor_col]  = [unmasker(x, top_k=top_k) for x in df[anchor_col]]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def fill_mask_obj(df, unmasker):\n",
    "    outputs = defaultdict()\n",
    "    # outputs['masked_sap'] = unmasker(df.masked_sap.to_list(), top_k=top_k, batch_size=100) \n",
    "    outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in df['masked_sentences'].to_list()]\n",
    "\n",
    "    outputs['def_sap']  = [unmasker(x, top_k=top_k) for x in df['def_sap'].to_list()]\n",
    "    outputs['lsp_sap']  = [unmasker(x, top_k=top_k) for x in df['lsp_sap'].to_list()]\n",
    "    # outputs['masked_sentences_obj_anchor'] = unmasker(df.masked_sentences_obj_anchor.to_list(), top_k=top_k, batch_size=100) \n",
    "    # outputs['masked_sentences'] = unmasker(df.masked_sentences.to_list(), top_k=top_k, batch_size=100)\n",
    "    # outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_sentences)]\n",
    "    return outputs\n",
    "\n",
    "\"\"\"## Cleaning outputs \"\"\"\n",
    "\n",
    "def aggregate_token_scores(input_word, token2probs, scorer, top_k, add_wordnet_path_score=False, sort_flag=True ):\n",
    "    ''' \n",
    "    goal: we want the best scorer to consider:\n",
    "        (1) frequency: a token that are elicited by multiple promptso\n",
    "        (2) the probability: higher overall probability \n",
    "        (3)\n",
    "\n",
    "    token2prob: dictionary mapping a token to a list of probs \n",
    "    anchor_anchor_scorer_list = ['freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply', 'freq', 'probSum', 'probAvg'] #TODO: rank based\n",
    "\n",
    "\n",
    "    test case:\n",
    "    token2probs = {'achieve': [0.2, 0.1, 0.03, 0.006], 'tried': [0.008, 0.006, 0.003, 0.001], 'perform':[0.08], 'prevent': [0.06], 'use': [0.02], 'accomplished': [0.1], 'produce':[0.06]}\n",
    "    for scorer in  [ 'freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply' ]: #'freq', 'probSum', 'probAvg',\n",
    "        token2prob = aggregate_token_scores(token2probs, scorer, top_k=7, sort_flag=True)\n",
    "        print(scorer)\n",
    "        print(f\"\\t{token2prob}\" )\n",
    "        print()\n",
    "\n",
    "    '''\n",
    "    token2prob = defaultdict()\n",
    "    all_count = sum([len(item) for item in token2probs.values()])\n",
    "    # print(json.dumps(token2probs, indent=4))\n",
    "    for token, probs in token2probs.items(): #rank_score = w * p, w is the frequency weight, p is the probability\n",
    "            count = len(probs)\n",
    "            \n",
    "            freq_weight = count/all_count\n",
    "            \n",
    "            new_score = 0 \n",
    "            \n",
    "            if scorer=='freq':\n",
    "                new_score =  freq_weight \n",
    "\n",
    "            elif scorer=='probSum':\n",
    "                new_score = sum(probs)\n",
    "\n",
    "            elif scorer=='probAvg': #this ignore the frequency factor [not ideal]\n",
    "                new_score = sum(probs)/ len(probs)\n",
    "\n",
    "            elif scorer=='freqProbSum': #[close to ideal]\n",
    "                new_score = freq_weight * sum(probs)\n",
    "                # print(token, freq_weight,sum(probs), new_score )\n",
    "            elif scorer=='probLogSum':\n",
    "                probs_valid = [item for item in probs if item>0]\n",
    "                if len(probs_valid )==0:\n",
    "                    new_score= 0\n",
    "                else:\n",
    "                    new_score =  sum([math.log(item, 2) for item in probs_valid ])/len(probs_valid)\n",
    "                    # new_score =  sum([math.log(item, 2) for item in probs if item>0])/len(probs)\n",
    "\n",
    "            elif scorer=='freqProbLogSum': #[close to ideal, requires a token to be (1) frequent (2) high probs across prompts]\n",
    "                probs_valid = [item for item in probs if item>0]\n",
    "                if len(probs_valid )==0:\n",
    "                    new_score= 0\n",
    "                else:\n",
    "                    new_score =   sum([math.log(item*freq_weight, 2) for item in probs_valid ])/len(probs_valid)\n",
    "                    # new_score =   sum([math.log(item*freq_weight, 2) for item in probs if item>0])/len(probs)\n",
    "                # print(\"token: {}, freq_weight: {}, logsum: {}, new_score: {}\".format(token, freq_weight, sum([math.log(item, 2) for item in probs])/len(probs), new_score))\n",
    "            # elif scorer=='probMultiply': #using prob multiply will penelize the tokens with high frequency\n",
    "            #     new_score =  math.exp(sum([math.log(item, 2) for item in probs]))\n",
    "\n",
    "            # 221114: add the WN path socre here to obtain reliable outputs \n",
    "            if add_wordnet_path_score:\n",
    "                path_score = get_wordnet_shortest_path_score_between(input_word, token)\n",
    "                #print(f\"input_word: {input_word}\", \"token:\", token )\n",
    "                #print(\"path_score\", path_score, type(path_score))\n",
    "                #print(\"new_score\", new_score, type(new_score))\n",
    "                new_score += path_score  \n",
    "\n",
    "            token2prob[token] = new_score\n",
    "    \n",
    "    # token2prob = sorted(token2prob.items(), key=lambda x: x[1], reverse=True )\n",
    "    # if top_k is not None and isinstance(top_k, int):\n",
    "        # token2prob = token2prob[:top_k] \n",
    "    # token2prob = dict(token2prob)\n",
    "    return token2prob\n",
    "\n",
    "\n",
    "def filter_outputs_with_probs(inputs, outputs, filter=True, return_probs=True, top_k=None, scorer='freqProbSum', filter_inputs=True, add_wordnet_path_score=False, add_cpt_score=False, cpt_unmasker=None, mask_string=None, cpt_only=False):\n",
    "    '''\n",
    "    inputs: the original inputs, for example [A] is a type of [B], A is the input\n",
    "    outputs: the candidates returned by PTLMs\n",
    "\n",
    "    filter: True \n",
    "        filter: non-alpha tokens); \n",
    "\n",
    "    top_k: take the top_k outputs. This is important when using multiple prompts for each sub \n",
    "    add_wordnet_path_score: add wordnet path score into the output scoring function \n",
    "    add_cpt_score: add concept-positioning test score into the output scoring function \n",
    "\n",
    "    '''\n",
    "    anchor_list = []\n",
    "    anchor_scores = [] \n",
    "        \n",
    "    for input_word, top_outputs in zip(inputs, outputs):  #iterate through the samples (sub)\n",
    "        filled_tokens  = defaultdict(int) #filter/accumulate predictions for each sample \n",
    "        filled_scores = defaultdict(list) #a list of token:[score1, score2, ...]   \n",
    "        token2cpt  = defaultdict(list) #filter/accumulate predictions for each sample \n",
    "\n",
    "        if isinstance(top_outputs[0], list):\n",
    "            flatten_output = [item for top_k_output  in top_outputs for item in top_k_output]\n",
    "        else:\n",
    "            flatten_output = [item for item  in top_outputs]\n",
    "        # print(\"flatten_output\", flatten_output)\n",
    "\n",
    "        for i, output in enumerate(flatten_output):\n",
    "            filled_token = output['token_str'].strip().lower()\n",
    "            filled_score = output['score']\n",
    "            if filter:\n",
    "                #####Add conditions to filter unwanted ################\n",
    "                # filter the repetation of a concept in the explanation. See the the following example\n",
    "                # [MASK] is the capability to do a particular job . -> capacity \n",
    "                if not filled_token.isalpha(): continue\n",
    "                if filled_token in STOP_WORDS: continue \n",
    "                if len(filled_token)<=1: continue \n",
    "                # if filter_inputs:\n",
    "                if filled_token.strip().lower() in [re.sub(\"\\s+\", '', x) for x in input_word.split()]: continue #filter out the target in input  \n",
    "                if filled_token.startswith(\"#\"): continue\n",
    "                #####Add conditions to filter unwanted ################\n",
    "                \n",
    "                filled_tokens[filled_token] +=1\n",
    "                filled_scores[filled_token].append(filled_score)\n",
    "\n",
    "                ##### add CPT test for filtering ################ \n",
    "                if add_cpt_score and cpt_unmasker!=None and mask_string!=None:\n",
    "                    token2cpt[filled_token].append(concpet_positioning_test(stimulus_word=input_word,\n",
    "                                                                            output= output, \n",
    "                                                                            unmasker=cpt_unmasker, \n",
    "                                                                            mask_string=mask_string))\n",
    "\n",
    "                    # token2cpt_prompt[filled_token] =  get_revserse_prompt((stimulus_word=input_word,\n",
    "                                                                            # output= output))\n",
    "                ##### add CPT test for filtering ################ \n",
    "            else:\n",
    "                filled_tokens[filled_token] +=1\n",
    "                filled_scores[filled_token] += filled_score\n",
    "\n",
    "        if len(filled_tokens) ==0: \n",
    "            filled_tokens={'MISSING':1}\n",
    "            filled_scores['MISSING'] = [0]\n",
    "\n",
    "        # feed the input into the agrregate _token_scores() so that we can calcuate the \n",
    "        token2probs = aggregate_token_scores(input_word, token2probs=filled_scores, scorer=scorer, top_k=top_k,  add_wordnet_path_score=add_wordnet_path_score, sort_flag=True)\n",
    "\n",
    "        if filter and add_cpt_score:\n",
    "            # extreme case: all anchors fail the CPT and return 0; the anchor selection would become random \n",
    "            # token2cpt[filled_token] = \n",
    "            token2cptsum = dict({token: sum(scores)/len(scores) for token, scores in token2cpt.items()})\n",
    "            for token, probs in token2probs.items():\n",
    "                token2probs[token] = probs + token2cptsum[token]\n",
    "                # print(input_word, token, probs,token2probs[token] )\n",
    "            \n",
    "        if top_k is not None and isinstance(top_k, int):\n",
    "            token2probs = sorted(token2probs.items(), key=lambda x: x[1], reverse=True )\n",
    "            token2probs = token2probs[:top_k] \n",
    "            token2probs = dict(token2probs)\n",
    "        anchor_list.append(list(token2probs.keys())) \n",
    "        anchor_scores.append(token2probs) \n",
    "\n",
    "        # print(\"-\"*60)\n",
    "    return anchor_list if not return_probs  else pd.Series((anchor_list,anchor_scores))\n",
    "\n",
    "\n",
    "def filter_anchors_with_probs(df, outputs,  scorer, anchor_col='anchor_lsp_sap', filter=True, return_probs=True, top_k=10, add_wordnet_path_score=False,  add_cpt_score=False, cpt_unmasker=None, mask_string=None, cpt_only=False):\n",
    "    if not return_probs:\n",
    "        df['subj_anchors'] = filter_outputs_with_probs(df.sub_label.to_list(), outputs[anchor_col], filter=filter, return_probs=return_probs, top_k=top_k, scorer=scorer)\n",
    "        # df['obj_anchors'] = filter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs, top_k=top_k)\n",
    "        # df['obj_mask_sentence'] = filter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer) #remove this to \n",
    "        # df['obj_mask_sap'] = filter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer)\n",
    "        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])\n",
    "        # df['top1_incontext_obj'] = df['obj_mask_sentence'].apply(lambda x: x[0])\n",
    "        return df  \n",
    "    else:\n",
    "        df[['subj_anchors', 'subj_anchors_score']] = filter_outputs_with_probs(df.sub_label.to_list(), \n",
    "                                                                                outputs[anchor_col], \n",
    "                                                                                filter=filter,\n",
    "                                                                                top_k=top_k, \n",
    "                                                                                return_probs=return_probs, \n",
    "                                                                                scorer=scorer, add_wordnet_path_score=add_wordnet_path_score, \n",
    "                                                                                add_cpt_score=add_cpt_score, cpt_unmasker=cpt_unmasker, mask_string=mask_string, cpt_only= cpt_only\n",
    "                                                                                )\n",
    "        # df[['obj_anchors', 'obj_anchors_score']] = filter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs)\n",
    "        # df[['obj_mask_sentence', 'obj_mask_sentence_score']] = filter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer)\n",
    "        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])\n",
    "        # df['top1_incontext_obj'] = df['obj_mask_sentence'].apply(lambda x: x[0])\n",
    "        return df\n",
    "\n",
    "\"\"\"### DAP\"\"\"\n",
    "\n",
    "# def unify_mask(model, text, inp_mask_token, out_mask_token):\n",
    "#     '''\n",
    "#     text: the input to be transfered \n",
    "#         bert: out_mask_token = [MASK]\n",
    "#         roberta: out_mask_token = <mask> \n",
    "#     '''\n",
    "#     output = text.replace(inp_mask_token, out_mask_token)\n",
    "#     return output\n",
    "\n",
    "\n",
    "def insert_multiple_anchors(original_prompt_source, original_prompts, anchors, sub_position_start, sub_label,  use_original_prompt, incorporate_operation ):\n",
    "    '''\n",
    "    original_prompt_source: including the original context, mannually crafted templates, \n",
    "    original_prompt: the original prompt, \n",
    "    anchors: a list of anchors\n",
    "    sub_position: \n",
    "\n",
    "    return:\n",
    "        a list of prompts with incorporated anchors\n",
    "    '''\n",
    "   \n",
    "\n",
    "    #print(original_prompt)\n",
    "    #print(sub_position_start, sub_position_end, type(sub_position_start), type(sub_position_end))\n",
    "    #print(sub_label, sub_label_string)\n",
    "\n",
    "    anchored_prompts = []\n",
    "    if isinstance(original_prompts, list):\n",
    "        for original_prompt in original_prompts:\n",
    "            # original_prompt = original_prompt[0]\n",
    "            sub_position_end = int(sub_position_start) + len(sub_label)\n",
    "            sub_label_string = original_prompt[sub_position_start : sub_position_end ]\n",
    "\n",
    "            if use_original_prompt:\n",
    "                anchored_prompts.append(original_prompt)\n",
    "\n",
    "            # def insert_anchors_with_or(original_prompt, anchors, sub_label_string, anchors ):\n",
    "            if incorporate_operation == 'concate_or_single':\n",
    "                for anchor in anchors:\n",
    "                    anchored_string = f\"{sub_label_string} or {anchor}\"\n",
    "                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]\n",
    "                    anchored_prompts.append(anchor_prompt)\n",
    "            elif incorporate_operation == 'concate_comma_multiple':\n",
    "                    anchored_string = \"{}, {}\".format(sub_label_string, \",\".join(anchors))\n",
    "                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]\n",
    "                    anchored_prompts.append(anchor_prompt)\n",
    "            elif incorporate_operation == 'replace':\n",
    "                for anchor in anchors:\n",
    "                    anchored_string = f\"{anchor}\"\n",
    "                    anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]\n",
    "                    anchored_prompts.append(anchor_prompt)\n",
    "    return anchored_prompts\n",
    "\n",
    "def insert_multiple_anchors_by_relacement(original_prompts, anchors, original_string):\n",
    "    '''\n",
    "    replace the placeholder [Z] with true values\n",
    "    '''\n",
    "    anchored_prompts = []\n",
    "    for original_prompt in original_prompts:\n",
    "        for anchor in anchors: \n",
    "            anchored_prompts.append(original_prompt.replace(original_string, anchor))\n",
    "    return anchored_prompts\n",
    "\n",
    "\n",
    "def fill_anchor_into_dap(df, relation, relation_to_template, use_dap, incorporate_operation, original_prompt_source='template_sap', top_k_anchors = 1,  dap_col_name='masked_sentences_with_subj_anchor',  mask_string = \"[MASK]\", use_original_prompt=True, ):\n",
    "    if use_dap:\n",
    "        if original_prompt_source == 'template_sap':\n",
    "            template_sap = relation_to_template[relation]['template_sap']\n",
    "            template_dap = relation_to_template[relation]['template_dap'] \n",
    "            df[dap_col_name] = df[['template_sap', 'subj_anchors', 'sub_position','sub_label']].apply(lambda x: insert_multiple_anchors('template_sap', *x) if x[1][0]!='MISSING' else template_sap.replace(\"[X]\", x[0]), axis=1)\n",
    "\n",
    "        elif original_prompt_source=='masked_sentences':\n",
    "                df[dap_col_name] = df[['masked_sentences','subj_anchors', 'sub_position', 'sub_label']].apply(lambda x: insert_multiple_anchors(original_prompt_source, x[0], anchors=x[1], sub_position_start=x[2], sub_label=x[3], use_original_prompt=use_original_prompt, incorporate_operation=incorporate_operation ) if x[1][0]!='MISSING' else x[0], axis=1) \n",
    "\n",
    "        elif original_prompt_source in ['lsp_dap', 'def_dap']:#, 'masked_sentences']:\n",
    "            df[dap_col_name] = df[[original_prompt_source,'subj_anchors']].apply(lambda x: insert_multiple_anchors_by_relacement(x[0], anchors=x[1],  original_string='[Z]' ) if x[1][0]!='MISSING' else x[0], axis=1) \n",
    "\n",
    "    else: #221005: posy: come back to here when some relaitons are not using dap\n",
    "        df[dap_col_name] = df[['sub_label', 'subj_anchors']].apply(lambda x: template_sap.replace(\"[X]\", x[:top_k_anchors]), axis=1)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "args: incorporate_operation ['insert', 'replace', 'concate', ]\n",
    "output: a list of anchored prompts\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_dap_templates(df, relation, output_dir):\n",
    "    # Save DAP to the disk \n",
    "    # df_out = df[['pred', 'masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]\n",
    "    df_out = df[['masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]\n",
    "    df_out = df_out.rename(columns={'masked_sentences_with_subj_anchor': 'masked_sentences'})\n",
    "    df_out['masked_sentences'] = df_out['masked_sentences'].apply(lambda x: [x])\n",
    "    # os.makedirs(output_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_path = f'{output_dir}/{relation}.jsonl'\n",
    "    # json.dump(df_out.to_dict(orient='records'),  output_path) \n",
    "    df_out_json = df_out.to_dict( orient='records')\n",
    "\n",
    "    save_dict_to_json(df_out_json , output_path)\n",
    "    # {\"sub\": \"abdomen\", \"obj\": \"organs\", \"pred\": \"HasA\", \"masked_sentences\": [\"The abdomen contains [MASK].\"], \"obj_label\": \"organs\", \"uuid\": \"767f3c6c02e42a55f8b1c314f7167dab\"}\n",
    "    # return df_out\n",
    "\n",
    "# df[['sub_label', 'masked_sentences', 'obj_mask_sentence', 'obj_label']].sample(50)\n",
    "# .to_csv(\"IsA.sample50.filled.human_verification.incontext.csv\") #, 'masked_sentences', 'obj_mask_sentence']].sample(50)\n",
    "\n",
    "\"\"\"# Analysis\n",
    "\n",
    "## evaluation p@k\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from evaluation import mean_average_precision, average_precision_at_k, precision_at_k, recall_at_k\n",
    "\n",
    "def get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "    '''\n",
    "    evalaute model predictions in concept level, ignoring the morphology affects (singular, plural)\n",
    "    '''\n",
    "\n",
    "    p_at_x = [] #defaultdict() \n",
    "    for pred_col in pred_cols: \n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "        prec_cur = defaultdict()\n",
    "        prec_cur['mask_type'] = suffix\n",
    "        for k in k_list: \n",
    "            df[f'p{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: concept_evaluation(x[0], eval(x[1])[:k] if isinstance(x[1], str) else x[1][:k]), axis=1 )\n",
    "            prec_cur[f'p@{k}'] = round(df[f'p{k}_{suffix}'].mean() , 3)*100\n",
    "\n",
    "        p_at_x.append(prec_cur)  \n",
    "\n",
    "    # aggregate the average precision across k \n",
    "    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])\n",
    "    df_res['relation'] = [relation]*len(df_res)\n",
    "    return df_res\n",
    "\n",
    "def get_precision_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "\n",
    "    p_at_x = [] #defaultdict() \n",
    "    for pred_col in pred_cols: \n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "        prec_cur = defaultdict()\n",
    "        prec_cur['mask_type'] = suffix\n",
    "        for k in k_list: \n",
    "            df[f'p{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: 1 if x[0][0] in x[1][:k] else 0, axis=1 )\n",
    "            prec_cur[f'p@{k}'] = round(df[f'p{k}_{suffix}'].mean() , 3)*100\n",
    "\n",
    "            # suffix = pred_col.replace(\"obj_\", \"\")\n",
    "            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=np.array(x[0]), y_pred=np.array(x[1]), k=k), axis=1 )\n",
    "            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=x[0], y_pred=x[1], k=k), axis=1 )\n",
    "\n",
    "        p_at_x.append(prec_cur)  \n",
    "\n",
    "    # aggregate the average precision across k \n",
    "    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])\n",
    "    df_res['relation'] = [relation]*len(df_res)\n",
    "    return df_res\n",
    "\n",
    "def get_highest_mrr_among_labels(label, pred):\n",
    "    '''\n",
    "    return the highest rank among the multiple labels. This is applicable to single labels as well, if we the single label is put in a list\n",
    "\n",
    "    pred: a list of words (candidates)\n",
    "    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)\n",
    "    '''\n",
    "    mrr = 0 \n",
    "    if pred is None: return mrr \n",
    "\n",
    "    rank_list = [ pred.index(item) + 1 for item in label if item in pred] \n",
    "    if len(rank_list)>0:\n",
    "        mrr = 1/min(rank_list)\n",
    "\n",
    "    return mrr \n",
    "\n",
    "\n",
    "def get_mrr(df, relation, pred_cols, label_col, pred_col_suffix):\n",
    "    '''\n",
    "    mrr is calculated based on the top_k rank, all elements in obj_col are used\n",
    "    '''\n",
    "    # def get_mrr_single(label, pred):\n",
    "    #     '''\n",
    "    #     pred: a list of words (candidates)\n",
    "    #     label: the true label \n",
    "    #     '''\n",
    "    #     mrr = 0 \n",
    "    #     if pred is not None and label in pred:\n",
    "    #         rank = pred.index(label) + 1\n",
    "    #         mrr = 1/rank \n",
    "    #     return mrr \n",
    "\n",
    "    \n",
    "\n",
    "    mrr = [] \n",
    "    for i, pred_col in enumerate(pred_cols):\n",
    "        cur_mrr = defaultdict()\n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "\n",
    "        df[f'mrr_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: get_highest_mrr_among_labels(x[0], x[1]), axis=1 ) \n",
    "        \n",
    "        cur_mrr['mask_type'] = suffix\n",
    "        cur_mrr[f\"mrr\"] = round(df[f'mrr_{suffix}'].mean(), 3)*100\n",
    "        mrr.append(cur_mrr)\n",
    "\n",
    "    mrr_df =  pd.DataFrame(data = mrr) #, columns=['mask_type', 'mrr'])\n",
    "    # mrr_df['mask_type']= mrr_df['mask_type'].apply(lambda x: x.replace(\"\"))\n",
    "    mrr_df['relation'] = relation\n",
    "    return mrr_df \n",
    "\n",
    "\n",
    "def get_average_precision(df, relation, true_col, pred_cols, k_list):\n",
    "    '''\n",
    "    return the mean average precision for specified pred_cols\n",
    "    '''\n",
    "    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)\n",
    "\n",
    "    mAP_dic = defaultdict()\n",
    "    for k in k_list:\n",
    "        key = f'mAP@{k}'\n",
    "        mAP_dic[key] = defaultdict()\n",
    "        for pred_col in pred_cols:\n",
    "            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)\n",
    "            mAP_dic[key][pred_col.replace(\"obj_\", \"\")] = mAP\n",
    "\n",
    "    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])\n",
    "    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])\n",
    "    mAP_df['relation'] = relation\n",
    "    mAP_df['mask_type'] = mAP_df.index\n",
    "    return mAP_df\n",
    "\n",
    "\n",
    "def get_mean_average_precision(df, relation, true_col, pred_cols, k_list):\n",
    "    '''\n",
    "    return the mean average precision for specified pred_cols\n",
    "    '''\n",
    "    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)\n",
    "\n",
    "    mAP_dic = defaultdict()\n",
    "    for k in k_list:\n",
    "        key = f'mAP@{k}'\n",
    "        mAP_dic[key] = defaultdict()\n",
    "        for pred_col in pred_cols:\n",
    "            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)\n",
    "            mAP_dic[key][pred_col.replace(\"obj_\", \"\")] = mAP\n",
    "\n",
    "    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])\n",
    "    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])\n",
    "    mAP_df['relation'] = relation\n",
    "    mAP_df['mask_type'] = mAP_df.index\n",
    "    return mAP_df\n",
    "\n",
    "\n",
    "def get_mean_average_precision_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "    # get the avearage precision per query\n",
    "    map_at_x = []\n",
    "    for pred_col in pred_cols: \n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "        map_cur = defaultdict()\n",
    "        map_cur['mask_type'] = suffix\n",
    "\n",
    "        for k in k_list: \n",
    "            df[f'ap{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: average_precision_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1 )\n",
    "            map_cur[f'mAP@{k}'] = round(df[f'ap{k}_{suffix}'].mean(), 3)*100\n",
    "        map_at_x.append(map_cur)\n",
    "\n",
    "    # aggregate the average precision across k \n",
    "    df_res = pd.DataFrame(map_at_x) #, columns=['mask_type', 'mAP'])\n",
    "    df_res['relation'] = [relation]*len(df_res)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "\n",
    "def get_recall_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "    # get the avearage precision per query\n",
    "    recall_at_x = []\n",
    "    for pred_col in pred_cols: \n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "        recall_cur = defaultdict()\n",
    "        recall_cur['mask_type'] = suffix\n",
    "\n",
    "        for k in k_list: \n",
    "            df[f'recall_{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: recall_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1 )\n",
    "            recall_cur[f'recall@{k}'] = round(df[f'recall_{k}_{suffix}'].mean() ,3 )*100\n",
    "        recall_at_x.append(recall_cur)\n",
    "\n",
    "    # aggregate the average precision across k \n",
    "    df_res = pd.DataFrame(recall_at_x) #, columns=['mask_type', 'mAP'])\n",
    "    df_res['relation'] = [relation]*len(df_res)\n",
    "    return df_res\n",
    "\n",
    "\"\"\"### Analysis helper functions\"\"\"\n",
    "\n",
    "def get_rel_specific_results(df_res_all, mask_type):\n",
    "# rel = 'mask_sap'\n",
    "# rel = 'mask_dap'\n",
    "    dfc = df_res_all.query(f\"mask_type == '{mask_type}'\").reset_index(drop=True)\n",
    "    # round(dfc['p@1'].mean(), 2)\n",
    "    \n",
    "    overall = {\"p@1\":   round(dfc['p@1'].mean(), 1),\n",
    "                \"p@3\": round(dfc['p@3'].mean(), 1),\n",
    "                \"p@5\": round(dfc['p@5'].mean(), 1),\n",
    "                \"p@10\": round(dfc['p@10'].mean(), 1),\n",
    "                \"p@10\": round(dfc['p@10'].mean(), 1),\n",
    "                \"mrr\": round(dfc['mrr'].mean(), 1),\n",
    "                \"relation\": 'Overall'\n",
    "                # \"mrr\":  round(dfc['mrr'].mean(), 1),\n",
    "                # \"support\": dfc['support'].sum() \n",
    "                } \n",
    "    \n",
    "    dfc_overall = pd.DataFrame(overall, index=['Overall'])\n",
    "    # dfc = dfc.sort_values(by=['p@1'])\n",
    "    dfc = dfc.sort_values(by=['relation'])\n",
    "\n",
    "    dfc = pd.concat([dfc, dfc_overall]).reset_index(drop=True).sort_values(['relation'])\n",
    "    \n",
    "    return  dfc[['relation', 'p@1','p@10', 'mrr', 'p@3', 'p@5', 'mask_type']]\n",
    "\n",
    "def calculate_gains(df1, df2):\n",
    "    '''\n",
    "    df1: sap\n",
    "    df2: dap\n",
    "    '''\n",
    "    metrics = ['p@1', 'p@3', 'p@5', 'p@10', 'mrr']\n",
    "    df_gains = []\n",
    "    for col in metrics:\n",
    "        gains = df2[col] - df1[col]\n",
    "        gains.column = col \n",
    "        df_gains.append(gains)\n",
    "    \n",
    "    df_gains = pd.concat(df_gains, axis=1 )\n",
    "    df_gains['relation'] = df2.relation\n",
    "    return df_gains[['relation', 'p@1', 'p@10', 'mrr', 'p@3', 'p@5']]\n",
    "\n",
    "def display_gains(col1, col2, df_res_all, output_file=None):\n",
    "    ''' \n",
    "    goal: diaplay the gains from col1 to col2\n",
    "    col1: the baseline column \n",
    "    '''\n",
    "    df1 = get_rel_specific_results(df_res_all, mask_type=col1)\n",
    "    df2 = get_rel_specific_results(df_res_all, mask_type=col2)\n",
    "    print(f'gains: {col2} - {col1}')\n",
    "    df_gains = calculate_gains(df1, df2)\n",
    "    df_gains['mask_type'] =  [ 'Gains(DAP-SAP)'] * len(df_gains.index)\n",
    "    \n",
    "    df_out1 = pd.concat([df1, df2, df_gains], axis=1)\n",
    "    display(df_out1)\n",
    "    display(df1)\n",
    "    display(df2)\n",
    "    if output_file is not None:\n",
    "        print(f\"Saving final results to {output_file}\")\n",
    "        df_out1.to_csv(f\"{output_file}\")\n",
    "\n",
    "def aggregate_candidates(dic1, dic2, top_k):\n",
    "    '''\n",
    "    input are two list of candidates [{token: score}]\n",
    "    goal: merge the two lists \n",
    "    1. co-occurred words \n",
    "        strategy 1 - average: (score1 + score)/2 \n",
    "        strategy 2 - accumulate: score1 + score\n",
    "    2. cut off the number of all candiates to top_k\n",
    "    '''\n",
    "    keys = set(dic1.keys()).union(dic2.keys())\n",
    "    new_dic = defaultdict() \n",
    "\n",
    "    for key in keys:\n",
    "        new_dic[key] = dic1.get(key, 0) + dic2.get(key, 0)\n",
    "    new_dic = dict(Counter(new_dic).most_common(top_k))\n",
    "    return list(new_dic.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_avg_path_between_sub_and_anchors(df, oov_path_len = 100):\n",
    "    '''\n",
    "    evalaute the anchor quality by measuring\n",
    "    (1) the average paths between sub_labels and their anchors \n",
    "    (2) the coverage of anchors \n",
    "    '''\n",
    "    df['anchor_wordnet_path_len']  = df[['sub_label', 'subj_anchors']].apply(lambda x: [(subj_anchor,get_wordnet_shortest_path_length_between(x[0], subj_anchor, oov_path_len=oov_path_len)) for subj_anchor in x[1]] , axis=1)\n",
    "\n",
    "    # wn_lemmas = set(wn.all_lemma_names())\n",
    "    path_len_all = []\n",
    "    count_oov = 0\n",
    "    count = 0\n",
    "    \n",
    "    for path_lens in df['anchor_wordnet_path_len'].to_list():\n",
    "        for anchor, path_len in path_lens: \n",
    "            count +=1\n",
    "            if path_len!=oov_path_len:\n",
    "                path_len_all.append(path_len) \n",
    "            else:\n",
    "                count_oov = count_oov + 1\n",
    "\n",
    "    avearage_path_len = sum(path_len_all)/len(path_len_all)\n",
    "    coverage = 1 - count_oov/count\n",
    "\n",
    "    return round(avearage_path_len, 2) , round(coverage, 3)\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file) as f :\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config \n",
    "\n",
    "def concept_evaluation(label, pred):\n",
    "    '''\n",
    "    \n",
    "    label: a list with the singualr and plural labels (e.g., ['tool', 'tools'])\n",
    "    pred: the top K prediction list \n",
    "\n",
    "    return:\n",
    "        1 if label share with pred else 0  \n",
    "    '''\n",
    "    if not isinstance(label, list):\n",
    "        # label = eval(label)\n",
    "        label = [label]\n",
    "        \n",
    "    if not isinstance(pred, list):\n",
    "        pred = eval(pred)\n",
    "\n",
    "    shared = set(label).intersection(set(pred))\n",
    "    return 1 if len(shared)>0 else 0 \n",
    "    \n",
    "\n",
    "def group_evaluation(label, pred):\n",
    "    '''\n",
    "    \n",
    "    label: a list with the singualr and plural labels (e.g., ['tool', 'tools'])\n",
    "    pred: the top K prediction list \n",
    "\n",
    "    return:\n",
    "        1 if label share with pred else 0  \n",
    "    '''\n",
    "    if not isinstance(label, list):\n",
    "        # label = eval(label)\n",
    "        label = [label]\n",
    "        \n",
    "    if not isinstance(pred, list):\n",
    "        pred = eval(pred)\n",
    "\n",
    "    shared = set(label).intersection(set(pred))\n",
    "    return 1 if len(shared)>0 else 0 \n",
    "    \n",
    "\n",
    "def merge_predictions_in_concept_level(uniform_funcion, words, top_k=None ):\n",
    "    '''\n",
    "    uniform_function: either signualarize or pluralize \n",
    "    '''\n",
    "    words_uniformed = [uniform_funcion(word) for word in words]\n",
    "    concepts = list(OrderedDict.fromkeys(words_uniformed))\n",
    "    return concepts[:top_k] if top_k is not None else concepts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------  MAIN ------------------------------------ \n",
    "\n",
    "#config \n",
    "scorer_target_1_prompt =  'probSum'\n",
    "scorer_target_N_prompts =  'probAvg' #'probLogSum'\n",
    "\n",
    "\n",
    "# define model \n",
    "# vocab = ['flower', 'tool', 'building', 'insect', 'vegetable', 'tree', 'vehicle', 'bird', 'fish', 'trees', 'flowers', 'tools', 'vegetables', 'insects', 'birds', 'vehicles', 'fish', 'buildings' ]\n",
    "# vocab_sg = ['flower', 'tool', 'building', 'insect', 'vegetable', 'tree', 'vehicle', 'bird', 'fish']\n",
    "# vocab_pl = ['trees', 'flowers', 'tools', 'vegetables', 'insects', 'birds', 'vehicles', 'fish', 'buildings' ]\n",
    "\n",
    "device = 0\n",
    "model= 'bert-large-uncased'\n",
    "unmasker_sg = get_unmasker(model, device=device) #, targets=vocab)  \n",
    "unmasker_pl = get_unmasker(model, device=device) #, targets=vocab) \n",
    "top_k = 10\n",
    "batch_size = 100\n",
    "return_probs=True\n",
    "data_dir = 'data/hypernymsuite/SHWARTZ/consistency_group/' #sys.argv[1]\n",
    "debug = True # eval(sys.argv[2])\n",
    "\n",
    "print(f\"Debug: {debug}\")\n",
    "# process data \n",
    "# files = [ 'IsA.lsp_sap.jsonl',  'IsA.lsp_dap.jsonl' ] #'IsA.def_sap.jsonl',  'IsA.def_dap.jsonl', \n",
    "files = [ 'IsA.lsp_dap.jsonl' ] #'IsA.def_sap.jsonl',  'IsA.def_dap.jsonl', \n",
    "# data_dir = 'data/lm_diagnostic_extended/consistency_group/'\n",
    "save_dir = f'log/{model}/{data_dir.split(\"/\")[-3]}/{data_dir.split(\"/\")[-2]}' \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pattern_num = 6\n",
    "\n",
    "for file in files: \n",
    "    filepath = data_dir + file\n",
    "    df = load_data(filepath, clean_test=True, tokenize=False)\n",
    "    \n",
    "    total_row = len(df.index) #total_row = 12994\n",
    "    chunk_size = 100\n",
    "    index_list = [i for i in range(0, total_row, chunk_size)]\n",
    "    if index_list[-1] < total_row-1:\n",
    "        index_list.append(total_row-1)\n",
    "\n",
    "    for i, index in enumerate(index_list):\n",
    "        if i < len(index_list)-1:\n",
    "            dfi = df.iloc[index : index_list[i+1], :]\n",
    "            print(index, index_list[i+1], len(dfi.index))\n",
    "            mask_type = file.split('.')[1]\n",
    "            if debug: df =  df.head(20)\n",
    "\n",
    "            #Fill the blank \n",
    "            outputs = defaultdict()\n",
    "            for i in range(1, pattern_num+1):\n",
    "                # outputs[f'mask_sentences_sg_{i}'] = unmasker_sg(df[f'mask_sentences_singular_{i}'].to_list(), top_k=top_k, batch_size=batch_size) \n",
    "                # outputs[f'mask_sentences_pl_{i}'] = unmasker_pl(df[f'mask_sentences_plural_{i}'].to_list(), top_k=top_k, batch_size=batch_size) \n",
    "\n",
    "                outputs[f'mask_sentences_sg_{i}'] = [unmasker_sg(x, top_k=2*top_k) for x in df[f'mask_sentences_singular_{i}'].to_list() ]\n",
    "                outputs[f'mask_sentences_pl_{i}'] = [unmasker_pl(x, top_k=2*top_k) for x in df[f'mask_sentences_plural_{i}'].to_list()] #, batch_size=batch_size) \n",
    "\n",
    "                df[[f'obj_mask_sentence_sg_{i}', f'obj_mask_sentence_sg_{i}_score']] = filter_outputs_with_probs(df.sub_label_singular.to_list(), \n",
    "                                                                                                    outputs[f'mask_sentences_sg_{i}'], \n",
    "                                                                                                    return_probs=return_probs, \n",
    "                                                                                                    top_k=2*top_k, \n",
    "                                                                                                    scorer=scorer_target_N_prompts,\n",
    "                                                                                                    filter_inputs = True \n",
    "                                                                                                    )\n",
    "\n",
    "                df[[f'obj_mask_sentence_pl_{i}', f'obj_mask_sentence_{i}_pl_score']] = filter_outputs_with_probs(df.sub_label_plural.to_list(), \n",
    "                                                                                                    outputs[f'mask_sentences_pl_{i}'], \n",
    "                                                                                                    return_probs=return_probs, \n",
    "                                                                                                    top_k=2*top_k, \n",
    "                                                                                                    scorer=scorer_target_N_prompts,\n",
    "                                                                                                    filter_inputs = True \n",
    "                                                                                                    )\n",
    "\n",
    "                df[f'obj_mask_sentence_sg_{i}'] = df[f'obj_mask_sentence_sg_{i}'].apply(lambda x: merge_predictions_in_concept_level( singularize, x, top_k))\n",
    "                df[f'obj_mask_sentence_pl_{i}'] = df[f'obj_mask_sentence_pl_{i}'].apply(lambda x: merge_predictions_in_concept_level( pluralize, x , top_k))\n",
    "\n",
    "            df['obj_label_singular'] = df['obj_label_singular'].apply(lambda x: singularize(x) if isinstance(x, str) else singularize(x[0]))\n",
    "            df['obj_label_plural'] = df['obj_label_plural'].apply(lambda x: pluralize(x) if isinstance(x, str) else pluralize(x[0]))\n",
    "\n",
    "            # evalaute the predictions \n",
    "\n",
    "            ################################################# Evaluation: obj_label ################################################# \n",
    "            print(\"-\"*40,\"obj_label evaluation\", \"-\"*40)\n",
    "            pred_col_sg=[x for x in df.columns if 'obj_mask_sentence_sg_' in x and 'score' not in x]\n",
    "            pred_col_pl=[x for x in df.columns if 'obj_mask_sentence_pl_' in x and 'score' not in x]\n",
    "            print(\"pred_col_sg\", pred_col_sg)\n",
    "            print(\"pred_col_pl\", pred_col_pl)\n",
    "\n",
    "            df_res = []\n",
    "            for k in [1,10]:\n",
    "                # for i, (pred_col_sg, pred_col_pl) in enumerate(zip(pred_col_sg, pred_col_pl), start=1):\n",
    "                for i in range(1, pattern_num):\n",
    "                    df[f'p1_sg_{i}'] = df[['obj_label_singular', f'obj_mask_sentence_sg_{i}']].apply(lambda x: concept_evaluation(x[0], x[1][:k]), axis=1 )\n",
    "                    df[f'p1_pl_{i}'] = df[['obj_label_plural', f'obj_mask_sentence_pl_{i}']].apply(lambda x: concept_evaluation(x[0], x[1][:k]), axis=1 )\n",
    "\n",
    "                pred_col_sg_p1 = [x for x in df.columns if 'p1_sg_' in x ]\n",
    "                pred_col_pl_p1 = [x for x in df.columns if 'p1_pl_' in x ]\n",
    "\n",
    "\n",
    "                df['p1_sg'] = df[pred_col_sg_p1].apply(lambda x: int(all(ele == 1 for ele in x)), axis=1)\n",
    "                df['p1_pl'] = df[pred_col_pl_p1].apply(lambda x: int(all(ele == 1 for ele in x)), axis=1)\n",
    "\n",
    "                df['p1_sgpl'] = df[['p1_sg', 'p1_pl']].apply(lambda x: 1 if x[0]==1 and x[1]==1 else 0, axis=1)\n",
    "\n",
    "                acc_sg = round(df['p1_sg'].sum()/len(df.index), 3) * 100\n",
    "                acc_pl = round(df['p1_pl'].sum()/len(df.index), 3) * 100\n",
    "                acc_sgpl = round(df['p1_sgpl'].sum()/len(df.index), 3) * 100\n",
    "\n",
    "                df_res.append({\"K\":k, 'Singular': acc_sg, 'Plural': acc_pl, 'Paired Singular-Plural': acc_sgpl})\n",
    "\n",
    "            df_res = pd.DataFrame(data=df_res)\n",
    "            df_res['mask_type'] = mask_type\n",
    "            df_res['data_dir'] = data_dir \n",
    "\n",
    "            display(df_res)\n",
    "            outpath = f'{save_dir}/{file.replace(\"jsonl\", \"csv\")}'\n",
    "            # df.to_csv(outpath)\n",
    "\n",
    "            res_outpath = f'{save_dir}/{file.replace(\"jsonl\", \"tsv\")}'\n",
    "        #     df_res.to_csv(res_outpath)\n",
    "\n",
    "            print(f\"Save {outpath}\")\n",
    "            print(f\"Save {res_outpath}\")\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000\n",
      "3000 6000\n",
      "6000 9000\n",
      "9000 12000\n",
      "12000 12993\n"
     ]
    }
   ],
   "source": [
    "# df.head()\n",
    "# splitting dataframe by row index\n",
    "total_row = len(df.index) #total_row = 12994\n",
    "chunk_size = 3000\n",
    "index_list = [i for i in range(0, total_row, chunk_size)]\n",
    "if index_list[-1] < total_row-1:\n",
    "    index_list.append(total_row-1)\n",
    "\n",
    "for i, index in enumerate(index_list):\n",
    "    if i < len(index_list)-1:\n",
    "#         print(index, index_list[i+1])\n",
    "        dfi = df.iloc[index : index_list[i+1], :]\n",
    "    \n",
    "# df_1 = df.iloc[:1000,:]\n",
    "# df_2 = df.iloc[1000:,:]\n",
    "# df_1.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
