# -*- coding: utf-8 -*-
"""fill_in_anchor_word_property.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yACrIO2oQPndOepxrFbUKlfvKSbc-xu
"""



# !pip install transformer

# ! pip install spacy

# !jupyter nbconvert --to script fill_in_anchor_word_property.ipynb
# !jupyter nbconvert fill_in_anchor_word_property.ipynb --to script

"""## Import Modules"""

import os, sys
import json 
import pandas as pd 
from tqdm import tqdm 
import re 
from sklearn.metrics import accuracy_score
from collections import  defaultdict, Counter 
from transformers import pipeline
import numpy as np 
import copy
import math
# # import IPython
# from IPython.display import display
import torch

##### spacy 
import spacy
en = spacy.load('en_core_web_sm')
STOP_WORDS = en.Defaults.stop_words
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex

def custom_tokenizer(nlp):
    infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\"\'~]''')
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)

nlp = spacy.load('en_core_web_sm')
nlp.tokenizer = custom_tokenizer(nlp)

pd.options.display.max_columns = 50
pd.set_option('display.width', 1000)

"""## Helper Functions"""

def save_dict_to_json(examples, output_path):
    ''' 
    save a list of dicts into otuput_path, orient='records' (each line is a dict) 
    examples: a list of dicts
    output_path: 
    '''
    # if not os.path.exists(output_path):
        # os.path.makedirs(output_path)

    with open(output_path, 'w') as fout:
        for example in examples:
            json.dump(example, fout)
            fout.write("\n")
        print(f"save {output_path} with {len(examples)} lines")

"""# DAP relational templates"""

# from utils_relation import relations
anchor_type_to_prompts = {
     "synonym": [
            f"[X] and [Z] have similar properties .",
            f"[X] and [Z] share similar properties .",
            f"[X] and [Z] are similar .",
            f"[X] and [Z] have similar meanings .",
            f"[X] have a similar meaning as [Z] .",
            f"[X] and [Z] are synonyms .",
            f"[X] and [Z] are synonymous .",
            f'[X] and [Z] mean the same thing .',
            f'[X] and [Z] are the same thing .',
            f'[X] means the same thing as [Z] .'
            ], 
    "coordinate": [
        f"such as [X] and [Z] .",
        f"such as [X] or [Z] .",
        f", such as [X] and [Z] .", 
        f"as [X] and [Z] .", 
        f"as [X] or [Z] .", 
        f", such as [X] and [Z] .", 
        f", such as [X] or [Z] ." 
        f"[X], [Z] or other ",
        f"[X], [Z] and other ",
        f", including [X] and [Z] ", 
        f", including [X] or [Z] ",
        f", especially [X] and [Z] ", 
        f", especially [X] or [Z] ",
        ], 
    "hyponym": [f"[X] is a type of [Z] .",
        f"such [Z] as [X], and ",
        f"such [Z] as [X], or ",
        f"such as [X] and other [Z] .",
        f"such as [X] or other [Z] .",
        f", [X], and other [Z] .",
        f", [X], or other [Z] .",
        f"[Z], including [X] and ",
        f"[Z], including [X], or "]
}


relations = [
    {
        "relation": "Is",
        "template_sap": "A [X] is [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "A [X] and [Z] are a type of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "Has",
        "template_sap": "Usually, we would expect [X] to have [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        # "template_anchor": "Such [X] as [MASK] .",
        "template_anchor": "Such [X] as [MASK] .",
        "dap_x": "Usually, we would expect [X] or [Z] to have [Y] .",
        "dap_y": "Usually, we would expect [X] to have [Y] or [Z].",
    },
    {
        "relation": "Task8",
        "template_sap": "[X] is associated with [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "WAX",
        "template_sap": "[X] is associated with [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "HasProperty",
        "template_sap": "[X] are [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "CapableOf",
        "template_sap": "[X] can [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "[X] and [Z] can [Y] .",
        "dap_y": "[X] can [Y] or [Z] .",
    },
    {
        "relation": "ReceivesAction",
        "template_sap": "The [X] can be [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "The [X] or [Z] can be [Y] .",
        "dap_y": "The [X] can be [Y] or [Z] .",
    },
    {
        "relation": "UsedFor",
        "template_sap": "The [X] is used for [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "The [X] and [Z] are used for [Y] .",
        "dap_y": "",
    },
    {
        "relation": "Causes",
        "template_sap": "It is typical for [X] to cause [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": ", including [X] and [MASK] .",
        "dap_x": "It is typical for [X] and [Z] to cause [Y] .",
        "dap_y": "It is typical for [X] to cause [Y] or [Z] .",
    },
    {
        "relation": "Desires",
        "template_sap": "[X] desire to [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] desires to [Y] .",
        "dap_y": "[X] desires to [Y] or [X] .",
    },
    {
        "relation": "HasA",
        "template_sap": "Usually, we would expect [X] to have [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        # "template_anchor": "Such [X] as [MASK] .",
        "template_anchor": "Such [X] as [MASK] .",
        "dap_x": "Usually, we would expect [X] or [Z] to have [Y] .",
        "dap_y": "Usually, we would expect [X] to have [Y] or [Z].",
    },
    {
        "relation": "HasPrerequisite",
        "template_sap": "In order to [X], one must first [Y] .",
        "use_dap": False,
        "target_anchor": "[Y]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "In order to [X] or [Z], one must first [Y] .",
        "dap_y": "In order to [X], one must first [Y] or [Z] ."
    },
    {
        "relation": "HasSubevent",
        "template_sap": "You [Y] when you [X] .",
        "use_dap": False,
        "target_anchor": "[Y]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "You [Y] when you [X] and [Z] .",
        "dap_y": "You [Y] when you [X] or [Z] .",
    },
    {
        "relation": "MotivatedByGoal",
        "template_sap": "Someone [X]s in order to [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "[X] or [MASK] .",
        "dap_x": "Someone [X]s or [Z]s in order to [Y] .",
        "dap_y": "Someone [X]s in order to [Y] or [Z]s.",
    },
    {
        "relation": "AtLocation",
        "template_sap": "[X] are found in [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] are found in [Y] .",
        "dap_y": "",
    },
    {
        "relation": "CausesDesire",
        "template_sap": "The [X] makes people want to [Y]r .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "The [X] and [Z] make people want to [Y]r .",
        "dap_y": "",
    },
    {
        "relation": "IsA",
        "template_sap": "A [X] is a type of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "A [X] and [Z] are a type of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "MadeOf",
        "template_sap": "[X] is made of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] is made of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "PartOf",
        "template_sap": "[X] is a part of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] and [Z] are  part of [Y] .",
        "dap_y": "",
    }

]



def get_relation_templates(relations, model,  anchor_type, anchor_type_to_prompts): 
    '''
    generate the relation tempaltes based on model and anchor_type 

    '''
    # let's assumet the anchor concept is [Z] 
    relation_to_template = defaultdict() 
    for relation in relations:
        # print(relation.keys())
        key = relation['relation'] 
        relation_to_template[key] = defaultdict()
        target_anchor  = relation["target_anchor"]
        relation['template_anchor']  = anchor_type_to_prompts[anchor_type]
        
        if target_anchor == '[X]':
            template_sap = relation['template_sap'].replace("[Y]", "[MASK]")
            # template_dap = template_sap.replace("[X]", "[Z] or [X]") #anchors for [X] 
            template_dap = relation['dap_x'].replace("[Y]", "[MASK]")

        elif target_anchor == '[Y]':
            template_sap = relation['template_sap']
            template_dap = template_sap.replace("[Y]", "[Z] or [MASK]") #anchors for [Y]
            template_sap = template_sap.replace("[Y]", "[MASK]")
            # template_dap = relation['dap_y']

        if 'roberta' in model:
            template_sap = template_sap.replace("[MASK]", "<mask>")
            template_dap = template_dap.replace("[MASK]", "<mask>")
            relation['template_anchor'] = [item.replace("[Z]", "<mask>") for item in relation['template_anchor']]
        elif 'bert' in model:
            relation['template_anchor'] = [item.replace("[Z]", "[MASK]") for item in relation['template_anchor']]
        # relation.update(dap_temp)
        relation_to_template[key]['template_anchor'] = relation['template_anchor']
        relation_to_template[key]['template_sap'] = template_sap 
        relation_to_template[key]['template_dap'] = template_dap 
        relation_to_template[key]["use_dap"] = relation["use_dap"] 

        # save_dict_to_json(new_relations, 'log/relation_sap_dap.json')
        # if debug:
            # print( json.dumps(relation_to_template, indent=4))
    return relation_to_template

"""## Read data

### config
"""

## debug=True
#debug=False 
#top_k=10
#data_dir = 'data/sap_filter/'
## model = 'roberta-large'
#mask_string_mapping = {
                ## "roberta-large": "<mask>", 
               #"bert-base-cased": "[MASK]",
            ##    "bert-large-cased": "[MASK]"
               #}

"""## Load Data"""

def tokenize_sentence(sentence):
  sentence = sentence.replace("\"", "").lower()
  tokens = [token.orth_ for token in nlp(sentence)]
  return tokens

def locate_sub_obj_position(ent, sentence, index_not_in) :
  ''' 
  function: find the index of ent in a sentence, the result will be used to filter instances whose ent cannot be find at their sentences
  args: 
    sentence: the sentnces to mask, could be the string or a list of tokens 
    ent: the ent to be found (sub_label) 
    index_not_in: the default index for failed instances (an ent not in a sentence)
  ''' 

  if isinstance(sentence, list):
    if ent not in sentence:
      return index_not_in
    return sentence.index(ent)  
  else:
    sentence = copy.deepcopy(sentence).lower()
    if isinstance(sentence, str):
      try:
        index = sentence.index(ent)
        return  index 
      except: 
        print(f"NOT FOUND {ent} -> {sentence}")
        return index_not_in
      
        print(ent, sentence)
        return index_not_in

def remove_noisy_test_data(df):
  ''' 
  relation="hasproperty"
  why? some data points don't belong to this relation types 
  case1., sub_label=number, such as "10 is ten."  We don't say ten is the property of 10
  case2, sub_label = 'person_name' and obj_label = 'nuts;, such as ""Andrew is [MASK].", [MASK]=nuts
  '''
  sub_labels_to_exclude = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '30', '5', '50', '60', '7', '70', '70s', '80', '9', '90']
  obj_labels_to_exclude  = ['nuts']
  df = df.query(f"sub_label not in {sub_labels_to_exclude}")
  df = df.query(f"sub_label not in {obj_labels_to_exclude}")
  return  df.reset_index(drop=True)

# def exist_
  
def load_data(filepath, clean_test=True, tokenize=False):
  '''
  return the cleaned data
  args:
    tokenize: if True: the maksed_sentences will be tokenzied (this is slwoers); 
            otherwise, we use the string match to filter the failed sentences
    clean_test: default is True. We filter out some noisy samples spoted by huamns 
               Note that this is relation specific 

  '''
  index_not_in = 10000

  with open(filepath, 'r', encoding='utf-8') as fin:
    data = fin.readlines()
    data = [eval(x) for x in data]
    df = pd.DataFrame(data)

  if tokenize:
    df['masked_sentence_tokens'] = df['masked_sentences'].apply(lambda x: tokenize_sentence(x[0]))
    df['sub_position'] = df[['sub_label', 'masked_sentence_tokens']].apply(lambda x: locate_sub_obj_position(x[0], x[1], index_not_in=index_not_in), axis=1)

  if clean_test: 
    df = remove_noisy_test_data(df)
    df['sub_position'] = df[['sub_label', 'masked_sentences']].apply(lambda x: locate_sub_obj_position(x[0], x[1][0], index_not_in), axis=1)
    df = df.query(f"sub_position !={index_not_in}") #.reset_index() #cue can not be matched in the sentence

  print(f"#Test_instances: {len(df.index)}")
  return df.reset_index(drop=True)



def get_masked_data(filepath, relation, relation_to_template, model, debug=False):
  df = load_data(filepath)
  if debug:
      df = df.sample(200)

  df['masked_sap'] = df['sub_label'].apply(lambda x: relation_to_template[relation]['template_sap'].replace("[X]", x))

  # df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: f"{x} are [MASK] .")

  template_anchor = relation_to_template[relation]['template_anchor']
  if 'roberta' in model :
    df['masked_sentences'] = df['masked_sentences'].apply(lambda x: [x[0].replace("[MASK]", "<mask>")])

  df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in template_anchor])

  # df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: f"Such as {x} and [MASK] .")
  # df['masked_sentences_obj_anchor'] = df['obj_label'].apply(lambda x: f"Such as {x} and [MASK] .")

  if debug:
    print(df.columns)
    print(f"{len(df.index)} instances")

  return df 

#debug 
# # if debug:
# for relation in ['HasProperty']: # ['HasProperty', 'CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction']:
#   print(f"relation: {relation}")
#   filepath = f'data/sap_filter/{relation}.jsonl'
#   # df = load_data(filepath, clean_test=True, tokenize=False)
#   df = get_masked_data(filepath, relation, relation_to_template, model)
#   display(df.head())

"""## Define model and feed masked data into models"""

def get_unmasker(model, targets=None):
    if targets is None: 
        unmasker = pipeline('fill-mask', model=model)# 'bert-large-uncased') #initialize the masker
    else:
        unmasker = pipeline('fill-mask', model=model, targets=targets )# 'bert-large-uncased') #initialize the masker

    # if torch.cuda.is_available():
    #     unmasker = unmasker.cuda()
    return unmasker


def get_target_vocab(file_path = "data/clsb/blank/vocab.txt"):
    '''
    Weir et al., (2020) used two variants of target vocabulary when predicting the possible properties 
    One of them is the SNES version, "comprising the set of human completion that fit the given prompt syntax for all concepts in the study"
    '''
    df_target_vocab = pd.read_csv(file_path, header=None) #.to_list()
    target_vocab = df_target_vocab.iloc[:,0].to_list() 
    return target_vocab



def develop_single_case():
    target_vocab = get_target_vocab(file_path = "data/clsb/blank/vocab.txt")
    unmasker = get_unmasker(model= 'bert-base-cased', targets= target_vocab )
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= len(target_vocab))

    for x in results[:10]:
        print(x)
    print("-"*80)

    unmasker = get_unmasker(model= 'bert-base-cased')
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= 10)
    for x in results[:10]:
        print(x)

def fill_mask(df, unmasker):
    outputs = defaultdict()
    # outputs['masked_anchor_prompts'] = unmasker(df.masked_anchor_prompts.to_list(), top_k=top_k, batch_size=100)
    outputs['masked_anchor_prompts'] = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_anchor_prompts)]
    # outputs['masked_sentences_obj_anchor'] = unmasker(df.masked_sentences_obj_anchor.to_list(), top_k=top_k, batch_size=100) 
    outputs['masked_sap'] = unmasker(df.masked_sap.to_list(), top_k=top_k, batch_size=100) 
    # outputs['masked_sentences'] = unmasker(df.masked_sentences.to_list(), top_k=top_k, batch_size=100)
    # outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_sentences)]
    outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in df.masked_sentences]
    return outputs

"""## Cleaning outputs """

def aggregate_token_scores(token2probs, scorer, top_k, sort_flag=True ):
    ''' 
    goal: we want the best scorer to consider:
        (1) frequency: a token that are elicited by multiple promptso
        (2) the probability: higher overall probability 
        (3)

    token2prob: dictionary mapping a token to a list of probs 
    anchor_scorer_list = ['freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply', 'freq', 'probSum', 'probAvg'] #TODO: rank based


    test case:
    token2probs = {'achieve': [0.2, 0.1, 0.03, 0.006], 'tried': [0.008, 0.006, 0.003, 0.001], 'perform':[0.08], 'prevent': [0.06], 'use': [0.02], 'accomplished': [0.1], 'produce':[0.06]}
    for scorer in  [ 'freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply' ]: #'freq', 'probSum', 'probAvg',
        token2prob = aggregate_token_scores(token2probs, scorer, top_k=7, sort_flag=True)
        print(scorer)
        print(f"\t{token2prob}" )
        print()

    '''
    
    token2prob = defaultdict()
    all_count = sum([len(item) for item in token2probs.values()])
    # print(json.dumps(token2probs, indent=4))
    for token, probs in token2probs.items(): #rank_score = w * p, w is the frequency weight, p is the probability
            count = len(probs)
            freq_weight = count/all_count
            new_score = 0 

            if scorer=='freq':
                new_score =  freq_weight 

            if scorer=='probSum':
                new_score = sum(probs)

            if scorer=='probAvg': #this ignore the frequency factor [not ideal]
                new_score = sum(probs)/ len(probs)

            if scorer=='freqProbSum': #[close to ideal]
                new_score = freq_weight * sum(probs)
                # print(token, freq_weight,sum(probs), new_score )

            if scorer=='probMultiply': #using prob multiply will penelize the tokens with high frequency
                new_score =  math.exp(sum([math.log(item, 10) for item in probs]))
            
            if scorer=='probMultiplyAvg':
                new_score =  sum([math.log(item, 10) for item in probs])/len(probs)

            if scorer=='freqProbMultiply':
                new_score =  sum([math.log(item, 10) for item in probs])

            token2prob[token] = new_score

    token2prob = sorted(token2prob.items(), key=lambda x: x[1], reverse=True )
    if top_k is not None and isinstance(top_k, int):
        token2prob = token2prob[:top_k] 

    token2prob = dict(token2prob)
    return token2prob


def fillter_outputs_with_probs(inputs, outputs, filter=True, return_probs=True, top_k=None, scorer='freqProbSum'):
    '''
    inputs: the original inputs, for example [A] is a type of [B], A is the input
    outputs: the candidates returned by PTLMs

    filter: True 
        filter: non-alpha tokens); 

    top_k: take the top_k outputs. This is important when using multiple prompts for each sub 

    '''
    anchor_list = []
    anchor_scores = [] 
        
    for input, top_outputs in zip(inputs, outputs):  #iterate through the samples (sub)
        filled_tokens  = defaultdict(int) #filter/accumulate predictions for each sample 
        filled_scores = defaultdict(list) 

        if isinstance(top_outputs[0], list):
            flatten_output = [item for top_k_output  in top_outputs for item in top_k_output]
        else:
            flatten_output = [item for item  in top_outputs]
        for i, output in enumerate(flatten_output):
            filled_token = output['token_str'].strip().lower()
            filled_score = output['score']
            if filter:
                #####Add conditions to filter unwanted ################
                # filter the repetation of a concept in the explanation. See the the following example
                # [MASK] is the capability to do a particular job . -> capacity 
                if not filled_token.isalpha(): continue
                if filled_token in STOP_WORDS: continue 
                if len(filled_token)<=1: continue 
                if filled_token.strip() in [re.sub("\s+", '', x) for x in input.split()]: continue #filter out the target in input  
                if filled_token.startswith("#"): continue
                #####Add conditions to filter unwanted ################
                
                filled_tokens[filled_token] +=1
                filled_scores[filled_token].append(filled_score)
            else:
                # filled_tokens.append(filled_token)  
                filled_tokens[filled_token] +=1
                filled_scores[filled_token] += filled_score
                # filled_scores.append(filled_score)
        # row.filled_subj_anchor = filled_tokens 
        if len(filled_tokens) ==0: 
            filled_tokens={'MISSING':1}
            filled_scores['MISSING'] = [0]

        # all_count = sum(filled_tokens.values())

        # # select frequent and high-probs preidictions 
        # for (token, count) in filled_tokens.items(): #rank_score = w * p, w is the frequency weight, p is the probability
        #     freq_weight = count/all_count 

        #     anchor_scorer_mapping = {"freq", "probMultiply", "probSum", 'probFreqSum'}

        #     new_score =  freq_weight 
        #     new_score = filled_scores[token] 
        #     new_score = freq_weight   * filled_scores[token] 


        #     filled_scores[token] = new_score

        # filled_scores = sorted(filled_scores.items(), key=lambda x: x[1], reverse=True )
        # if top_k is not None and isinstance(top_k, int):
        #    filled_scores = filled_scores[:top_k] 

        # filled_scores = dict(filled_scores)
        filled_scores_aggregated = aggregate_token_scores(token2probs=filled_scores, scorer=scorer, top_k=top_k, sort_flag=True)
        anchor_list.append(list(filled_scores_aggregated.keys())) 
        anchor_scores.append(filled_scores_aggregated) 
        # print("-"*60)
    return anchor_list if not return_probs  else pd.Series((anchor_list,anchor_scores))


def add_filter_outputs_with_probs(df, outputs,  scorer, return_probs=True, top_k=10,):
    if not return_probs:
        df['subj_anchors'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_anchor_prompts'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['obj_anchors'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs, top_k=top_k)
        # df['obj_mask_incontext'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer) #remove this to 
        df['obj_mask_sap'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_incontext'].apply(lambda x: x[0])
        return df  
    else:
        df[['subj_anchors', 'subj_anchors_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_anchor_prompts'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df[['obj_anchors', 'obj_anchors_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs)
        # df[['obj_mask_incontext', 'obj_mask_incontext_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_incontext'].apply(lambda x: x[0])
        return df

"""### DAP"""

# def unify_mask(model, text, inp_mask_token, out_mask_token):
#     '''
#     text: the input to be transfered 
#         bert: out_mask_token = [MASK]
#         roberta: out_mask_token = <mask> 
#     '''
#     output = text.replace(inp_mask_token, out_mask_token)
#     return output


def insert_multiple_anchors(original_prompt_source, original_prompt, anchors, sub_position_start, sub_label,  use_original_prompt, mask_string = "[MASK]" ):
    '''
    original_prompt_source: including the original context, mannually crafted templates, 
    original_prompt: the original prompt, 
    anchors: a list of anchors
    sub_position: 

    return:
        a list of prompts with incorporated anchors
    '''
    sub_position_end = int(sub_position_start) + len(sub_label)
    sub_label_string = original_prompt[sub_position_start : sub_position_end ]

    #print(original_prompt)
    #print(sub_position_start, sub_position_end, type(sub_position_start), type(sub_position_end))
    #print(sub_label, sub_label_string)

    anchored_prompts = []
    if isinstance(original_prompt, list):
        original_prompt = original_prompt[0]

    if use_original_prompt:
        anchored_prompts.append(original_prompt)

    for anchor in anchors:
        anchored_string = f"{sub_label_string} or {anchor}"
        # anchor_prompt = original_prompt.replace(sub_label_string , anchored_string)
        anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
        anchored_prompts.append(anchor_prompt)
    return anchored_prompts

def fill_anchor_into_dap(df, relation, relation_to_template, use_dap, original_prompt_source='template_sap', top_k_anchors = 1, dap_col_name='masked_sentences_with_subj_anchor',  mask_string = "[MASK]", use_original_prompt=True):
    if use_dap:
        if original_prompt_source == 'template_sap':
            template_sap = relation_to_template[relation]['template_sap']
            template_dap = relation_to_template[relation]['template_dap'] 
            df[dap_col_name] = df[['template_sap', 'subj_anchors', 'sub_position','sub_label']].apply(lambda x: insert_multiple_anchors('template_sap', *x) if x[1][0]!='MISSING' else template_sap.replace("[X]", x[0]), axis=1)

        elif original_prompt_source=='original_context':
                df[dap_col_name] = df[['masked_sentences','subj_anchors', 'sub_position', 'sub_label']].apply(lambda x: insert_multiple_anchors(original_prompt_source, x[0][0], anchors=x[1], sub_position_start=x[2], use_original_prompt=use_original_prompt, sub_label=x[3],  ) if x[1][0]!='MISSING' else x[0], axis=1) 
    else: #221005: posy: come back to here when some relaitons are not using dap
        df[dap_col_name] = df[['sub_label', 'subj_anchors']].apply(lambda x: template_sap.replace("[X]", x[:top_k_anchors]), axis=1)
    return df 



def save_dap_templates(df, relation, output_dir):
    # Save DAP to the disk 
    # df_out = df[['pred', 'masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]
    df_out = df[['masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]
    df_out = df_out.rename(columns={'masked_sentences_with_subj_anchor': 'masked_sentences'})
    df_out['masked_sentences'] = df_out['masked_sentences'].apply(lambda x: [x])
    # os.makedirs(output_dir, mode=0o777, exist_ok=True)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_path = f'{output_dir}/{relation}.jsonl'
    # json.dump(df_out.to_dict(orient='records'),  output_path) 
    df_out_json = df_out.to_dict( orient='records')

    save_dict_to_json(df_out_json , output_path)
    # {"sub": "abdomen", "obj": "organs", "pred": "HasA", "masked_sentences": ["The abdomen contains [MASK]."], "obj_label": "organs", "uuid": "767f3c6c02e42a55f8b1c314f7167dab"}
    # return df_out

# df[['sub_label', 'masked_sentences', 'obj_mask_incontext', 'obj_label']].sample(50)
# .to_csv("IsA.sample50.filled.human_verification.incontext.csv") #, 'masked_sentences', 'obj_mask_incontext']].sample(50)
# df[[ 'masked_anchor_prompts', 'masked_sentences', 'sub_label', 'subj_anchors', 'obj_label']].sample(50)
# df[['sub_label', 'obj_label', 'masked_sentences', 'masked_anchor_prompts', 'subj_anchors']].head()

"""# Analysis

## evaluation p@k
"""

def get_mrr(df):

    def get_mrr_single(label, pred):
        '''
        pred: a list of words (candidates)
        label: the true label 
        '''
        mrr = 0 
        if pred is not None and label in pred:
            rank = pred.index(label) + 1
            mrr = 1/rank 
        return mrr 

    df['mrr_general'] = df[['obj_label', 'obj_mask_sap']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
    df['mrr_anchor'] = df[['obj_label', 'obj_mask_dap']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
    df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
    df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
    df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )

    MRR = defaultdict()
    MRR['general'] =   round(df['mrr_general'].mean()*100, 1)
    MRR['anchor'] =    round(df['mrr_anchor'].mean()*100, 1)
    MRR['incontext'] = round(df['mrr_incontext'].mean()*100, 1)
    return MRR



def get_mrr(df, relation):
    '''
    mrr is calculated based on the top_k rank, all elements in obj_col are used
    '''

    def get_mrr_single(label, pred):
        '''
        pred: a list of words (candidates)
        label: the true label 
        '''
        mrr = 0 
        if pred is not None and label in pred:
            rank = pred.index(label) + 1
            mrr = 1/rank 
        return mrr 

    obj_cols = ['obj_mask_sap', 'obj_mask_dap', 'obj_mask_incontext', 'obj_mask_incontext_dap', 'obj_mask_sap_dap'] 
    mrr_cols = [item.replace('obj_', 'mrr_') for item in obj_cols]
    label_col = 'obj_label'

    mrr = defaultdict()
    for i, (obj_col, mrr_col) in enumerate(zip(obj_cols, mrr_cols)):
        if not obj_col in df.columns: continue
        df[mrr_col] = df[[label_col, obj_col]].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 ) 
        mrr[mrr_col.replace("mrr_", "")] = round(df[mrr_col].mean()*100, 1) 

    mrr_df =  pd.DataFrame(data = mrr.items(), columns=['mask_type', 'mrr'])
    # mrr_df['mask_type']= mrr_df['mask_type'].apply(lambda x: x.replace(""))
    mrr_df['relation'] = relation
    return mrr_df 




def get_precision_at_k(df, relation, pred_cols, k_list):

    for k in k_list: 
        for pred_col in pred_cols: 
            suffix = pred_col.replace("obj_", "")
            df[f'p{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: 1 if x[0] in x[1][:k] else 0, axis=1 )

    p_at_x = defaultdict()
    for k in k_list:
        key = f'p@{k}'
        p_at_x[key] = defaultdict()
        for col in df.columns:
            if 'top' in col: continue 
            if f'p{k}_' in col:
                precision = df[col].value_counts(normalize=True).to_dict().get(1, 0) #1 is the key word of correct examples, 0 is the default value to hanld the case of all examples are wrong 
                precision = round(precision, 3)*100
                p_at_x[key][col.replace(f'p{k}_', '')] = precision 
    df_res = pd.DataFrame(p_at_x)
    df_res['relation'] = [relation]*len(df_res)
    return df_res

from evaluation import mean_average_precision
# def get_mean_average_precision(df, relation, true_col, pred_cols, k_list):
#     '''
#     return the mean average precision for specified pred_cols
#     '''
#     df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

#     mAP_dic = defaultdict()
#     for k in k_list:
#         key = f'mAP@{k}'
#         mAP_dic[key] = defaultdict()
#         for pred_col in pred_cols:
#             mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
#             mAP_dic[key][pred_col.replace("obj_", "")] = mAP

#     # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
#     mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])
#     mAP_df['relation'] = relation
#     mAP_df['mask_type'] = mAP_df.index
#     return mAP_df



def get_mean_average_precision(df, relation, true_col, pred_cols, k=10):
    '''
    return the mean average precision for specified pred_cols
    '''
    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

    # mAP = mean_average_precision(df['obj_label_list'].to_list(), df['obj_mask_incontext'].to_list(), k=10)

    mAP_dic = defaultdict()

    for pred_col in pred_cols:
        mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
        mAP_dic[pred_col.replace("obj_", "")] = mAP

    mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
    mAP_df['relation'] = relation
    return mAP_df


"""### Analysis helper functions"""

def get_rel_specific_results(df_res_all, mask_type):
# rel = 'mask_sap'
# rel = 'mask_dap'
    dfc = df_res_all.query(f"mask_type == '{mask_type}'").reset_index(drop=True)
    # round(dfc['p@1'].mean(), 2)
    
    overall = {"p@1":   round(dfc['p@1'].mean(), 1),
                "p@3": round(dfc['p@3'].mean(), 1),
                "p@5": round(dfc['p@5'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "mrr": round(dfc['mrr'].mean(), 1),
                "relation": 'Overall'
                # "mrr":  round(dfc['mrr'].mean(), 1),
                # "support": dfc['support'].sum() 
                } 
    
    dfc_overall = pd.DataFrame(overall, index=['Overall'])
    # dfc = dfc.sort_values(by=['p@1'])
    dfc = dfc.sort_values(by=['relation'])

    dfc = pd.concat([dfc, dfc_overall]).reset_index(drop=True).sort_values(['relation'])
    
    return  dfc[['relation', 'p@1','p@10', 'mrr', 'p@3', 'p@5', 'mask_type']]

def calculate_gains(df1, df2):
    '''
    df1: sap
    df2: dap
    '''
    metrics = ['p@1', 'p@3', 'p@5', 'p@10', 'mrr']
    df_gains = []
    for col in metrics:
        gains = df2[col] - df1[col]
        gains.column = col 
        df_gains.append(gains)
    
    df_gains = pd.concat(df_gains, axis=1 )
    df_gains['relation'] = df2.relation
    return df_gains[['relation', 'p@1', 'p@10', 'mrr', 'p@3', 'p@5']]

def display_gains(col1, col2, df_res_all, output_file=None):
    ''' 
    goal: diaplay the gains from col1 to col2
    col1: the baseline column 
    '''
    df1 = get_rel_specific_results(df_res_all, mask_type=col1)
    df2 = get_rel_specific_results(df_res_all, mask_type=col2)
    print(f'gains: {col2} - {col1}')
    df_gains = calculate_gains(df1, df2)
    df_gains['mask_type'] =  [ 'Gains(DAP-SAP)'] * len(df_gains.index)
    
    df_out1 = pd.concat([df1, df2, df_gains], axis=1)
    display(df_out1)
    display(df1)
    display(df2)
    if output_file is not None:
        print(f"Saving final results to {output_file}")
        df_out1.to_csv(f"{output_file}")

def aggregate_candidates(dic1, dic2, top_k):
    '''
    input are two list of candidates [{token: score}]
    goal: merge the two lists 
    1. co-occurred words 
        strategy 1 - average: (score1 + score)/2 
        strategy 2 - accumulate: score1 + score
    2. cut off the number of all candiates to top_k
    '''
    keys = set(dic1.keys()).union(dic2.keys())
    new_dic = defaultdict() 

    for key in keys:
        new_dic[key] = dic1.get(key, 0) + dic2.get(key, 0)
    new_dic = dict(Counter(new_dic).most_common(top_k))
    return list(new_dic.keys())

"""# The main function"""

# ---------- config ----------------
top_k=10
# data_dir = 'data/sap_filter/'
#data_dir = 'data/clsb/everyone_knows/'
data_dir = 'data/clsb/blank/'
mask_string_mapping = {
                # "roberta-large": "<mask>", 
               "bert-base-cased": "[MASK]",
            #    "bert-large-cased": "[MASK]"
               }

# use_dap_global = False #True
use_dap_global = True
max_anchor_num = 3 
debug = False #True 
# debug = True 
return_probs = True
dataset_name = 'LAMA'
anchor_scorer_list = ['freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply', 'freq', 'probSum', 'probAvg']
scorer_anchor = 'freq' #'freqProbSum'#anchor_scorer_list[0]
scorer_target = 'probSum'
use_original_prompt = True
constrain_targets=True
# ---------- config ----------------



for model in mask_string_mapping.keys(): #['bert-large-cased']

    # get the unmasker 
    if constrain_targets: 
        target_vocab = get_target_vocab(file_path = "data/clsb/blank/vocab.txt")
        unmasker = get_unmasker(model, targets=target_vocab) #### initialize the fill-the-blank model 
        top_k = len(target_vocab) 
    else:
        unmasker = get_unmasker(model) #### initialize the fill-the-blank model 
    mask_string = mask_string_mapping[model]
    save_dir = f'log/{model}/{dataset_name}' 
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    for anchor_type in ["coordinate"]: # ,  "synonym"]:
    # anchor_type="synonym" 
        print("-"*40, f"{model}-anchor_type: {anchor_type}", "-"*40)
        relation_to_template  = get_relation_templates(relations, model, anchor_type, anchor_type_to_prompts)
        if debug:
            print(json.dumps(relation_to_template, indent=4))

        dfs = []
        df_res_all = []
        # for relation in tqdm(relation_to_template.keys()):
        # for relation in tqdm(['MadeOf']): #,'WAX''ReceivesAction', 'CapableOf',   'HasA']):'HasProperty' 'HasProperty'
        for relation in ['Has']: #, 'IsA', 'Has', 'HasA', 'MadeOf']: #['IsA', 'HasA']:
        # for relation in tqdm (['HasProperty']): #(["MotivatedByGoal"] ): 
        # for relation in tqdm(['CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction', 'HasProperty']):

            if not relation_to_template[relation]['use_dap'] and not use_dap_global : 
                print(f"skipping {relation}")
                continue 
            filepath = f'{data_dir}/{relation}.jsonl'
            print(f"Processing {relation} ... {filepath}")
            df = get_masked_data(filepath, relation, relation_to_template, model=model) #anchor mask, general mask,
            if debug:
                df = df.head(5)


            print("\t step1: get anchors (fill and filter)")
            # step1: get the anchor
            outputs = fill_mask(df, unmasker)  #fill the mask with PTLM 
            df[['obj_mask_incontext', 'obj_mask_incontext_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer_target )
            df[['obj_mask_sap', 'obj_mask_sap_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer_target )

            # df = add_filter_outputs(df, outputs) #filter the anchor candiates
            if use_dap_global:
                df = add_filter_outputs_with_probs(df, outputs, scorer = scorer_anchor,  return_probs=True, top_k=max_anchor_num) #filter the anchor candiates

                use_dap = relation_to_template[relation]['use_dap']  if not use_dap_global else use_dap_global 
                
                print(f"relation: {relation} \t use_dap: {use_dap}")
                # step2: fill DAP
                print("\t step2: insert anchors")
                df = fill_anchor_into_dap(df, relation, relation_to_template, use_dap, original_prompt_source='original_context', top_k_anchors = max_anchor_num, dap_col_name='masked_sentences_with_subj_anchor',mask_string = mask_string, use_original_prompt=use_original_prompt) #create the dap promtps/masks 

                if use_dap:
                    #step3: filter outputs 
                    print("\t step3: fill mask in dap anchors")
                    outputs['obj_mask_dap'] = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_sentences_with_subj_anchor.to_list())]
                    df['subj_anchors_combined'] = df[['sub_label', 'subj_anchors']].apply(lambda x: x[0] + " " + " ".join(x[1]), axis=1)

                    #TODO: 221014 note that the follwing code is likely to affect the results a lot: whether using the anchors to filter the targets.
                    # need experiments to compare
                    df[['obj_mask_dap', 'obj_mask_dap_score']] = fillter_outputs_with_probs(df.subj_anchors_combined.to_list(), outputs['obj_mask_dap'],  return_probs=return_probs, top_k=top_k, scorer= scorer_target)

                    # step4: merge dap and sap
                    df['obj_mask_sap_dap'] = df[[ 'obj_mask_sap_score', 'obj_mask_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k), axis=1)
                    df['obj_mask_incontext_dap'] = df[['obj_mask_incontext_score', 'obj_mask_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k), axis=1)
                else:
                    df['subj_anchors_combined'] = df[['sub_label', 'subj_anchors']].apply(lambda x: x[0] + " " + " ".join(x[1]), axis=1)
                    df['obj_mask_dap'] = df.obj_mask_sap

                print(df.head())
                save_dap_templates(df, relation, output_dir=f'{save_dir}/dap/{anchor_type}')

            # Evaluations
            pred_cols  =[col  for col in df.columns if col.startswith("obj_mask_") and "_score" not in col]  #predicted target cols, e
            df_prec = get_precision_at_k(df, relation, pred_cols, k_list = [1,3,10,top_k])
            df_prec['mask_type'] = df_prec.index

            df_mrr =  get_mrr(df, relation)
            df_mAP = get_mean_average_precision(df, relation, 'obj_label', pred_cols, k=top_k) 

            df_prec['mAP'] = df_prec['mask_type'].apply(lambda x:  df_mAP.loc[df_mAP['mask_type']==x, 'mAP'].values[0])
            df_prec['mrr'] = df_prec['mask_type'].apply(lambda x:  df_mrr.loc[df_mrr['mask_type']==x, 'mrr'].values[0])
            # output_cols = ['mask_type', 'p@1', 'p@10', 'mrr'] 
            # df_mAP = get_mean_average_precision(df, relation, 'obj_label', pred_cols, k_list=[10, top_k]) 
            # for k in [10, top_k]:
                # mAP_col = f'mAP@{k}'
                # output_cols.append(f'mAP@{k}')
                # df_prec[f'mAP@{k}'] = df_prec['mask_type'].apply(lambda x:  df_mAP.loc[df_mAP['mask_type']==x, f'mAP@{k}'].values[0])
            # output_cols.append('relation)
            # df_prec = df_prec[output_cols]
            df_prec = df_prec[['mask_type', 'p@1', 'p@10', 'mrr', 'p@3', 'mAP', 'relation' ]]
            
            df_res_all.append( df_prec )
            df['relation'] = [relation]*len(df)
            dfs.append(df)

        # Aggregate and save results and saving 
        dfs_data = pd.concat(dfs)
        df_res_all = pd.concat(df_res_all).reset_index(drop=True)
        for name,group in df_res_all.groupby('mask_type'):
            print(group)
        if not debug:
            file_data_results = f'{save_dir}/exp_data_results_anchor_type_{anchor_type}_max_anchor_num_{max_anchor_num}.csv' 
            file_results = f'{save_dir}/df_all_use_global_dap_{use_dap_global}_anchor_type_{anchor_type}_max_anchor_num_{max_anchor_num}.csv'
            dfs_data.to_csv(file_data_results)
            df_res_all.to_csv(file_results)
            print(f"save {file_data_results}")
            print(f"save {file_results}")

        ## df_res_all = pd.read_csv(file_results , index_col=0)
        ## df_res_all.head()
        #col1 = 'mask_incontext'  #baseline columns
        #col2 = 'mask_dap'
        
        #output_file = f"{save_dir}/{anchor_type}_{col1}_vs_{col2}.csv"
        #display_gains(col1, col2, df_res_all, output_file= output_file if not debug else None )

# df[[ 'sub_label', 'subj_anchors',  'masked_sap', 'obj_label']].head(50) #'masked_sentences_with_subj_anchor',
# df['']
# df.head()
# df[['sub_label', 'obj_label', 'obj_mask_incontext', 'obj_mask_dap']]
