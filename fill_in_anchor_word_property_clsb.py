# -*- coding: utf-8 -*-
"""fill_in_anchor_word_property.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yACrIO2oQPndOepxrFbUKlfvKSbc-xu
"""



# !pip install transformers

# ! pip install spacy
# -*- coding: utf-8 -*-
"""fill_in_anchor_word_property.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yACrIO2oQPndOepxrFbUKlfvKSbc-xu
"""



# !pip install transformers

# ! pip install spacy

# !jupyter nbconvert --to script fill_in_anchor_word_property.ipynb
# !jupyter nbconvert fill_in_anchor_word_property.ipynb --to script

"""## Import Modules"""

import os, sys
import json 
import math
import pandas as pd 
from tqdm import tqdm 
import re 
from sklearn.metrics import accuracy_score
from collections import  defaultdict, Counter 
from transformers import pipeline
import numpy as np 
import copy
import math
from IPython.display import display
##### spacy 
import spacy
en = spacy.load('en_core_web_sm')
STOP_WORDS = en.Defaults.stop_words

import re
import spacy
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex

def custom_tokenizer(nlp):
    infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\"\'~]''')
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)

nlp = spacy.load('en_core_web_sm')
nlp.tokenizer = custom_tokenizer(nlp)

pd.options.display.max_columns = 50
pd.set_option('display.width', 1000)

"""## Helper Functions"""

def save_dict_to_json(examples, output_path):
    ''' 
    save a list of dicts into otuput_path, orient='records' (each line is a dict) 
    examples: a list of dicts
    output_path: 
    '''
    # if not os.path.exists(output_path):
        # os.path.makedirs(output_path)

    with open(output_path, 'w') as fout:
        for example in examples:
            json.dump(example, fout)
            fout.write("\n")
        print(f"save {output_path} with {len(examples)} lines")

"""# DAP relational templates"""

# from utils_relation import relations
anchor_type_to_prompts = {
     "synonym": [
            f"[X] and [Z] have similar properties .",
            f"[X] and [Z] share similar properties .",
            f"[X] and [Z] are similar .",
            f"[X] and [Z] have similar meanings .",
            f"[X] have a similar meaning as [Z] .",
            f"[X] and [Z] are synonyms .",
            f"[X] and [Z] are synonymous .",
            f'[X] and [Z] mean the same thing .'double_anchor_type_to_prompts,
            f'[X] and [Z] are the same thing .',
            f'[X] means the same thing as [Z] .'
            ], 
    "coordinate": [
        f"such as [X] and [Z] ",
        f"such as [X] or [Z] ",
        f", such as [X] and [Z] ", 
        f", such as [X] or [Z] ", 
        f"as [X] and [Z] .", 
        f"as [X] or [Z] .", 
        f", such as [X] and [Z] .", 
        f", such as [X] or [Z] .", 
        f"[X], [Z] or other ",
        f"[X], [Z] and other ",
        f", including [X] and [Z] ", 
        f", including [X] or [Z] ",
        f", especially [X] and [Z] ", 
        f", especially [X] or [Z] ",
        ], 
    "hyponym": [f"[X] is a type of [Z] .",
        f"such [Z] as [X], and ",
        f"such [Z] as [X], or ",
        f"such as [X] and other [Z] .",
        f"such as [X] or other [Z] .",
        f", [X], and other [Z] .",
        f", [X], or other [Z] .",
        f"[Z], including [X] and ",
        f"[Z], including [X], or "]
}
single_anchor_type_to_prompts = {
      "hypernym": [
        f"[MASK], such as [X] ",
        f"[MASK], such as [X] and ",
        f"[MASK], including [X] ",
        f"[MASK], including [X] and ",
        f"[MASK], especially [X] ",
        f"[MASK], especially [X] and ",
        f"[MASK], like [X] and ",
        f"[MASK] (e.g., [X]) ",
        f", [X] or other [MASK] ",
        f", [X] and other [MASK] ",

        f"such as [X] or [Z] .",
        f", such as [X] and [Z] .", 
        f"as [X] and [Z] .", 
        f"as [X] or [Z] .", 
        f", such as [X] and [Z] .", 
        f", such as [X] or [Z] .", 
        f"[X], [Z] or other ",
        f"[X], [Z] and other ",
        f", including [X] and [Z] ", 
        f", including [X] or [Z] ",
        f", especially [X] and [Z] ", 
        f", especially [X] or [Z] ",
        ], 
}

double_anchor_type_to_prompts={
    "hypernym": [
        f"[MASK] such as [X] and [Z] ,",
        f"[MASK] such as [X] or [Z] ,",
        f"such [MASK] as [X] and [Z] ,",
        f"such [MASK] as [X] or [Z] ,",
        f"[X], [Z] or other [MASK] " ,
        f"[X], [Z] and other [MASK] " ,
        f"[MASK], including [X] and [Z] " ,
        f"[MASK], including [X] or [Z] " ,
        f"[MASK], especially [X] and [Z] " ,
        f"[MASK], especially [X] or [Z] " 
        ]
}

relations = [
    {
        "relation": "Is",
        "template_sap": "A [X] is [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "A [X] and [Z] are a type of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "Has",
        "template_sap": "Usually, we would expect [X] to have [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        # "template_anchor": "Such [X] as [MASK] .",
        "template_anchor": "Such [X] as [MASK] .",
        "dap_x": "Usually, we would expect [X] or [Z] to have [Y] .",
        "dap_y": "Usually, we would expect [X] to have [Y] or [Z].",
    },
    {
        "relation": "Task8",
        "template_sap": "[X] is associated with [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "WAX",
        "template_sap": "[X] is associated with [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "HasProperty",
        "template_sap": "[X] are [Y] ." ,
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor":[],
        "dap_x": "[X] and [Z] are [Y] .",
        "dap_y": "[X] and [Z] are [Y] .",
    },
    {
        "relation": "CapableOf",
        "template_sap": "[X] can [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "[X] and [Z] can [Y] .",
        "dap_y": "[X] can [Y] or [Z] .",
    },
    {
        "relation": "ReceivesAction",
        "template_sap": "The [X] can be [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "The [X] or [Z] can be [Y] .",
        "dap_y": "The [X] can be [Y] or [Z] .",
    },
    {
        "relation": "UsedFor",
        "template_sap": "The [X] is used for [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": [],
        "dap_x": "The [X] and [Z] are used for [Y] .",
        "dap_y": "",
    },
    {
        "relation": "Causes",
        "template_sap": "It is typical for [X] to cause [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": ", including [X] and [MASK] .",
        "dap_x": "It is typical for [X] and [Z] to cause [Y] .",
        "dap_y": "It is typical for [X] to cause [Y] or [Z] .",
    },
    {
        "relation": "Desires",
        "template_sap": "[X] desire to [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] desires to [Y] .",
        "dap_y": "[X] desires to [Y] or [X] .",
    },
    {
        "relation": "HasA",
        "template_sap": "Usually, we would expect [X] to have [Y] .",
        "use_dap": False,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        # "template_anchor": "Such [X] as [MASK] .",
        "template_anchor": "Such [X] as [MASK] .",
        "dap_x": "Usually, we would expect [X] or [Z] to have [Y] .",
        "dap_y": "Usually, we would expect [X] to have [Y] or [Z].",
    },
    {
        "relation": "HasPrerequisite",
        "template_sap": "In order to [X], one must first [Y] .",
        "use_dap": False,
        "target_anchor": "[Y]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "In order to [X] or [Z], one must first [Y] .",
        "dap_y": "In order to [X], one must first [Y] or [Z] ."
    },
    {
        "relation": "HasSubevent",
        "template_sap": "You [Y] when you [X] .",
        "use_dap": False,
        "target_anchor": "[Y]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "You [Y] when you [X] and [Z] .",
        "dap_y": "You [Y] when you [X] or [Z] .",
    },
    {
        "relation": "MotivatedByGoal",
        "template_sap": "Someone [X]s in order to [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "[X] or [MASK] .",
        "dap_x": "Someone [X]s or [Z]s in order to [Y] .",
        "dap_y": "Someone [X]s in order to [Y] or [Z]s.",
    },
    {
        "relation": "AtLocation",
        "template_sap": "[X] are found in [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] are found in [Y] .",
        "dap_y": "",
    },
    {
        "relation": "CausesDesire",
        "template_sap": "The [X] makes people want to [Y]r .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "The [X] and [Z] make people want to [Y]r .",
        "dap_y": "",
    },
    {
        "relation": "IsA",
        "template_sap": "A [X] is a type of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "A [X] and [Z] are a type of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "MadeOf",
        "template_sap": "[X] is made of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] or [Z] is made of [Y] .",
        "dap_y": "",
    },
    {
        "relation": "PartOf",
        "template_sap": "[X] is a part of [Y] .",
        "use_dap": True,
        "target_anchor": "[X]",
        "template_anchor": "Such as [X] and [MASK] .",
        "dap_x": "[X] and [Z] are  part of [Y] .",
        "dap_y": "",
    }

]



def get_relation_templates(relations, model, anchor_type, anchor_type_to_prompts, double_anchor_type, double_anchor_type_to_prompts): 
    '''
    generate the relation tempaltes based on model and anchor_type 

    '''
    # let's assumet the anchor concept is [Z] 
    relation_to_template = defaultdict() 
    for relation in relations:
        # print(relation.keys())
        key = relation['relation'] 
        relation_to_template[key] = defaultdict()
        target_anchor  = relation["target_anchor"]
        relation['template_anchor']  = anchor_type_to_prompts[anchor_type]
        relation['template_double_anchor']  = double_anchor_type_to_prompts[double_anchor_type]
        
        if target_anchor == '[X]':
            template_sap = relation['template_sap'].replace("[Y]", "[MASK]")
            # template_dap = template_sap.replace("[X]", "[Z] or [X]") #anchors for [X] 
            template_dap = relation['dap_x'].replace("[Y]", "[MASK]")

        elif target_anchor == '[Y]':
            template_sap = relation['template_sap']
            template_dap = template_sap.replace("[Y]", "[Z] or [MASK]") #anchors for [Y]
            template_sap = template_sap.replace("[Y]", "[MASK]")
            # template_dap = relation['dap_y']

        if 'roberta' in model:
            template_sap = template_sap.replace("[MASK]", "<mask>")
            template_dap = template_dap.replace("[MASK]", "<mask>")
            relation['template_anchor'] = [item.replace("[Z]", "<mask>") for item in relation['template_anchor']]
            relation['template_double_anchor'] = [item.replace("[MASK]", "<mask>") for item in relation['template_double_anchor']]
        elif 'bert' in model:
            relation['template_anchor'] = [item.replace("[Z]", "[MASK]") for item in relation['template_anchor']]
        # relation.update(dap_temp)
        relation_to_template[key]['template_anchor'] = relation['template_anchor']
        relation_to_template[key]['template_double_anchor'] = relation['template_double_anchor']
        relation_to_template[key]['template_sap'] = template_sap 
        relation_to_template[key]['template_dap'] = template_dap 
        relation_to_template[key]["use_dap"] = relation["use_dap"] 

        # save_dict_to_json(new_relations, 'log/relation_sap_dap.json')
        # if debug:
            # print( json.dumps(relation_to_template, indent=4))
    return relation_to_template

"""## Load Data"""

def tokenize_sentence(sentence):
  sentence = sentence.replace("\"", "").lower()
  tokens = [token.orth_ for token in nlp(sentence)]
  return tokens

def locate_sub_obj_position(ent, sentence, index_not_in) :
  ''' 
  function: find the index of ent in a sentence, the result will be used to filter instances whose ent cannot be find at their sentences
  args: 
    sentence: the sentnces to mask, could be the string or a list of tokens 
    ent: the ent to be found (sub_label) 
    index_not_in: the default index for failed instances (an ent not in a sentence)
  ''' 

  if isinstance(sentence, list):
    if ent not in sentence:
      return index_not_in
    return sentence.index(ent)  
  else:
    sentence = copy.deepcopy(sentence).lower()
    if isinstance(sentence, str):
      try:
        index = sentence.index(ent)
        return  index 
      except: 
        print(f"NOT FOUND {ent} -> {sentence}")
        return index_not_in
      
        print(ent, sentence)
        return index_not_in

def remove_noisy_test_data(df):
  ''' 
  relation="hasproperty"
  why? some data points don't belong to this relation types 
  case1., sub_label=number, such as "10 is ten."  We don't say ten is the property of 10
  case2, sub_label = 'person_name' and obj_label = 'nuts;, such as ""Andrew is [MASK].", [MASK]=nuts
  '''
  sub_labels_to_exclude = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '30', '5', '50', '60', '7', '70', '70s', '80', '9', '90']
  obj_labels_to_exclude  = ['nuts']
  df = df.query(f"sub_label not in {sub_labels_to_exclude}")
  df = df.query(f"sub_label not in {obj_labels_to_exclude}")
  return  df.reset_index(drop=True)

# def exist_
  
def load_data(filepath, clean_test=True, tokenize=False):
  '''
  return the cleaned data
  args:
    tokenize: if True: the maksed_sentences will be tokenzied (this is slwoers); 
            otherwise, we use the string match to filter the failed sentences
    clean_test: default is True. We filter out some noisy samples spoted by huamns 
               Note that this is relation specific 

  '''
  index_not_in = 10000

  with open(filepath, 'r', encoding='utf-8') as fin:
    data = fin.readlines()
    data = [eval(x) for x in data]
    df = pd.DataFrame(data)

    df['obj_label'] = df['obj_label'].apply(lambda x: [x] if isinstance(x, str) else x)

  if tokenize:
    df['masked_sentence_tokens'] = df['masked_sentences'].apply(lambda x: tokenize_sentence(x[0]))
    df['sub_position'] = df[['sub_label', 'masked_sentence_tokens']].apply(lambda x: locate_sub_obj_position(x[0], x[1], index_not_in=index_not_in), axis=1)

  if clean_test: 
    df = remove_noisy_test_data(df)
    df['sub_position'] = df[['sub_label', 'masked_sentences']].apply(lambda x: locate_sub_obj_position(x[0], x[1][0], index_not_in), axis=1)
    df = df.query(f"sub_position !={index_not_in}") #.reset_index() #cue can not be matched in the sentence

  print(f"#Test_instances: {len(df.index)}")
  return df.reset_index(drop=True)



def get_masked_data(filepath, relation, relation_to_template, model, debug=False):
  df = load_data(filepath)
  if debug:
      df = df.sample(200)

  df['masked_sap'] = df['sub_label'].apply(lambda x: relation_to_template[relation]['template_sap'].replace("[X]", x))

  # df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: f"{x} are [MASK] .")

  template_anchor = relation_to_template[relation]['template_anchor']
  if 'roberta' in model :
    df['masked_sentences'] = df['masked_sentences'].apply(lambda x: [x[0].replace("[MASK]", "<mask>")])

  df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in template_anchor])

  template_double_anchor = relation_to_template[relation]['template_double_anchor']
  df['masked_double_anchor_prompts'] = df['sub_label'].apply(lambda x: [item.replace('[X]', x) for item in template_double_anchor])

  # df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: f"Such as {x} and [MASK] .")
  # df['masked_sentences_obj_anchor'] = df['obj_label'].apply(lambda x: f"Such as {x} and [MASK] .")

  if debug:
    print(df.columns)
    print(f"{len(df.index)} instances")

  return df 

#debug 
# # if debug:
# for relation in ['HasProperty']: # ['HasProperty', 'CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction']:
#   print(f"relation: {relation}")
#   filepath = f'data/sap_filter/{relation}.jsonl'
#   # df = load_data(filepath, clean_test=True, tokenize=False)
#   df = get_masked_data(filepath, relation, relation_to_template, model)
#   display(df.head())

"""## Define model and feed masked data into models"""

def get_unmasker(model, targets=None):
    if targets is None: 
        unmasker = pipeline('fill-mask', model=model)# 'bert-large-uncased') #initialize the masker
    else:
        unmasker = pipeline('fill-mask', model=model, targets=targets )# 'bert-large-uncased') #initialize the masker
    return unmasker

def get_target_vocab(file_path = "data/clsb/blank/vocab.txt"):
    '''
    Weir et al., (2020) used two variants of target vocabulary when predicting the possible properties 
    One of them is the SNES version, "comprising the set of human completion that fit the given prompt syntax for all concepts in the study"
    '''
    df_target_vocab = pd.read_csv(file_path, header=None) #.to_list()
    target_vocab = df_target_vocab.iloc[:,0].to_list() 
    return target_vocab



def develop_single_case():
    target_vocab = get_target_vocab(file_path = "data/clsb/blank/vocab.txt")
    unmasker = get_unmasker(model= 'bert-base-cased', targets= target_vocab )
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= len(target_vocab))

    for x in results[:10]:
        print(x)
    print("-"*80)

    unmasker = get_unmasker(model= 'bert-base-cased')
    text = 'Everyone knows that a bear has [MASK] .'
    results = unmasker(text, top_k= 10)
    for x in results[:10]:
        print(x)

def fill_mask_anchor(df, unmasker):
    outputs = defaultdict()
    outputs['masked_anchor_prompts'] = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_anchor_prompts)]
    
    return outputs

def fill_mask_obj(df, unmasker):
    outputs = defaultdict()
    outputs['masked_sap'] = unmasker(df.masked_sap.to_list(), top_k=top_k, batch_size=100) 
    outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in df.masked_sentences]
    # outputs['masked_anchor_prompts'] = unmasker(df.masked_anchor_prompts.to_list(), top_k=top_k, batch_size=100)
    # outputs['masked_sentences_obj_anchor'] = unmasker(df.masked_sentences_obj_anchor.to_list(), top_k=top_k, batch_size=100) 
    # outputs['masked_sentences'] = unmasker(df.masked_sentences.to_list(), top_k=top_k, batch_size=100)
    # outputs['masked_sentences']  = [unmasker(x, top_k=top_k) for x in tqdm(df.masked_sentences)]
    return outputs

"""## Cleaning outputs """

def aggregate_token_scores(token2probs, scorer, top_k, sort_flag=True ):
    ''' 
    goal: we want the best scorer to consider:
        (1) frequency: a token that are elicited by multiple promptso
        (2) the probability: higher overall probability 
        (3)

    token2prob: dictionary mapping a token to a list of probs 
    anchor_scorer_list = ['freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply', 'freq', 'probSum', 'probAvg'] #TODO: rank based


    test case:
    token2probs = {'achieve': [0.2, 0.1, 0.03, 0.006], 'tried': [0.008, 0.006, 0.003, 0.001], 'perform':[0.08], 'prevent': [0.06], 'use': [0.02], 'accomplished': [0.1], 'produce':[0.06]}
    for scorer in  [ 'freqProbSum', 'probMultiply', 'probMultiplyAvg', 'freqProbMultiply' ]: #'freq', 'probSum', 'probAvg',
        token2prob = aggregate_token_scores(token2probs, scorer, top_k=7, sort_flag=True)
        print(scorer)
        print(f"\t{token2prob}" )
        print()

    '''
    token2prob = defaultdict()
    all_count = sum([len(item) for item in token2probs.values()])
    # print(json.dumps(token2probs, indent=4))
    for token, probs in token2probs.items(): #rank_score = w * p, w is the frequency weight, p is the probability
            count = len(probs)
            
            freq_weight = count/all_count
            
            new_score = 0 
            
            if scorer=='freq':
                new_score =  freq_weight 

            elif scorer=='probSum':
                new_score = sum(probs)

            elif scorer=='probAvg': #this ignore the frequency factor [not ideal]
                new_score = sum(probs)/ len(probs)

            elif scorer=='freqProbSum': #[close to ideal]
                new_score = freq_weight * sum(probs)
                # print(token, freq_weight,sum(probs), new_score )
            elif scorer=='probLogSum':
                probs_valid = [item for item in probs if item>0]
                if len(probs_valid )==0:
                    new_score= 0
                else:
                    new_score =  sum([math.log(item, 2) for item in probs_valid ])/len(probs_valid)
                    # new_score =  sum([math.log(item, 2) for item in probs if item>0])/len(probs)

            elif scorer=='freqProbLogSum': #[close to ideal, requires a token to be (1) frequent (2) high probs across prompts]
                probs_valid = [item for item in probs if item>0]
                if len(probs_valid )==0:
                    new_score= 0
                else:
                    new_score =   sum([math.log(item*freq_weight, 2) for item in probs_valid ])/len(probs_valid)
                    # new_score =   sum([math.log(item*freq_weight, 2) for item in probs if item>0])/len(probs)
                # print("token: {}, freq_weight: {}, logsum: {}, new_score: {}".format(token, freq_weight, sum([math.log(item, 2) for item in probs])/len(probs), new_score))
            # elif scorer=='probMultiply': #using prob multiply will penelize the tokens with high frequency
            #     new_score =  math.exp(sum([math.log(item, 2) for item in probs]))
            
            token2prob[token] = new_score

    token2prob = sorted(token2prob.items(), key=lambda x: x[1], reverse=True )
    if top_k is not None and isinstance(top_k, int):
        token2prob = token2prob[:top_k] 

    token2prob = dict(token2prob)
    return token2prob


def fillter_outputs_with_probs(inputs, outputs, filter=True, return_probs=True, top_k=None, scorer='freqProbSum', filter_inputs=True):
    '''
    inputs: the original inputs, for example [A] is a type of [B], A is the input
    outputs: the candidates returned by PTLMs

    filter: True 
        filter: non-alpha tokens); 

    top_k: take the top_k outputs. This is important when using multiple prompts for each sub 

    '''
    anchor_list = []
    anchor_scores = [] 
        
    for input, top_outputs in zip(inputs, outputs):  #iterate through the samples (sub)
        filled_tokens  = defaultdict(int) #filter/accumulate predictions for each sample 
        filled_scores = defaultdict(list) 

        if isinstance(top_outputs[0], list):
            flatten_output = [item for top_k_output  in top_outputs for item in top_k_output]
        else:
            flatten_output = [item for item  in top_outputs]
        # print("flatten_output", flatten_output)

        for i, output in enumerate(flatten_output):
            filled_token = output['token_str'].strip().lower()
            filled_score = output['score']
            if filter:
                #####Add conditions to filter unwanted ################
                # filter the repetation of a concept in the explanation. See the the following example
                # [MASK] is the capability to do a particular job . -> capacity 
                if not filled_token.isalpha(): continue
                if filled_token in STOP_WORDS: continue 
                if len(filled_token)<=1: continue 
                if filled_token.strip() in [re.sub("\s+", '', x) for x in input.split()]: continue #filter out the target in input  
                if filled_token.startswith("#"): continue
                #####Add conditions to filter unwanted ################
                
                filled_tokens[filled_token] +=1
                filled_scores[filled_token].append(filled_score)
            else:
                # filled_tokens.append(filled_token)  
                filled_tokens[filled_token] +=1
                filled_scores[filled_token] += filled_score
                # filled_scores.append(filled_score)
        # row.filled_subj_anchor = filled_tokens 
        if len(filled_tokens) ==0: 
            filled_tokens={'MISSING':1}
            filled_scores['MISSING'] = [0]

        # all_count = sum(filled_tokens.values())

        # # select frequent and high-probs preidictions 
        # for (token, count) in filled_tokens.items(): #rank_score = w * p, w is the frequency weight, p is the probability
        #     freq_weight = count/all_count 

        #     anchor_scorer_mapping = {"freq", "probMultiply", "probSum", 'probFreqSum'}

        #     new_score =  freq_weight 
        #     new_score = filled_scores[token] 
        #     new_score = freq_weight   * filled_scores[token] 


        #     filled_scores[token] = new_score

        # filled_scores = sorted(filled_scores.items(), key=lambda x: x[1], reverse=True )
        # if top_k is not None and isinstance(top_k, int):
        #    filled_scores = filled_scores[:top_k] 

        # filled_scores = dict(filled_scores)
        filled_scores_aggregated = aggregate_token_scores(token2probs=filled_scores, scorer=scorer, top_k=top_k, sort_flag=True)
        anchor_list.append(list(filled_scores_aggregated.keys())) 
        anchor_scores.append(filled_scores_aggregated) 
        # print("-"*60)
    return anchor_list if not return_probs  else pd.Series((anchor_list,anchor_scores))


def add_filter_outputs_with_probs(df, outputs,  scorer, return_probs=True, top_k=10,):
    if not return_probs:
        df['subj_anchors'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_anchor_prompts'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['obj_anchors'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs, top_k=top_k)
        # df['obj_mask_incontext'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer) #remove this to 
        df['obj_mask_sap'] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_incontext'].apply(lambda x: x[0])
        return df  
    else:
        df[['subj_anchors', 'subj_anchors_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_anchor_prompts'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df[['obj_anchors', 'obj_anchors_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences_obj_anchor'], return_probs=return_probs)
        # df[['obj_mask_incontext', 'obj_mask_incontext_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer)
        # df['top1_subj_anchor'] = df['subj_anchors'].apply(lambda x: x[0])
        # df['top1_incontext_obj'] = df['obj_mask_incontext'].apply(lambda x: x[0])
        return df

"""### DAP"""

# def unify_mask(model, text, inp_mask_token, out_mask_token):
#     '''
#     text: the input to be transfered 
#         bert: out_mask_token = [MASK]
#         roberta: out_mask_token = <mask> 
#     '''
#     output = text.replace(inp_mask_token, out_mask_token)
#     return output


def insert_multiple_anchors(original_prompt_source, original_prompt, anchors, sub_position_start, sub_label,  use_original_prompt, incorporate_operation ):
    '''
    original_prompt_source: including the original context, mannually crafted templates, 
    original_prompt: the original prompt, 
    anchors: a list of anchors
    sub_position: 

    return:
        a list of prompts with incorporated anchors
    '''
   

    #print(original_prompt)
    #print(sub_position_start, sub_position_end, type(sub_position_start), type(sub_position_end))
    #print(sub_label, sub_label_string)

    anchored_prompts = []
    if isinstance(original_prompt, list):
    # for original_prompt in original_prompts:
        original_prompt = original_prompt[0]
    sub_position_end = int(sub_position_start) + len(sub_label)
    sub_label_string = original_prompt[sub_position_start : sub_position_end ]

    if use_original_prompt:
        anchored_prompts.append(original_prompt)

    # def insert_anchors_with_or(original_prompt, anchors, sub_label_string, anchors ):
    if incorporate_operation == 'concate_or_single':
        for anchor in anchors:
            anchored_string = f"{sub_label_string} or {anchor}"
            anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
            anchored_prompts.append(anchor_prompt)
    elif incorporate_operation == 'concate_comma_multiple':
            anchored_string = "{}, {}".format(sub_label_string, ",".join(anchors))
            anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
            anchored_prompts.append(anchor_prompt)
    elif incorporate_operation == 'replace':
        for anchor in anchors:
            anchored_string = f"{anchor}"
            anchor_prompt  =  original_prompt[:sub_position_start] + anchored_string + original_prompt[sub_position_end:]
            anchored_prompts.append(anchor_prompt)
    return anchored_prompts

def insert_multiple_anchors_by_relacement(original_prompts, anchors, original_string):
    '''
    replace the placeholder [Z] with true values
    '''
    anchored_prompts = []
    for original_prompt in original_prompts:
        for anchor in anchors: 
            anchored_prompts.append(original_prompt.replace(original_string, anchor))
    return anchored_prompts


def fill_anchor_into_dap(df, relation, relation_to_template, use_dap, incorporate_operation, original_prompt_source='template_sap', top_k_anchors = 1,  dap_col_name='masked_sentences_with_subj_anchor',  mask_string = "[MASK]", use_original_prompt=True, ):
    if use_dap:
        if original_prompt_source == 'template_sap':
            template_sap = relation_to_template[relation]['template_sap']
            template_dap = relation_to_template[relation]['template_dap'] 
            df[dap_col_name] = df[['template_sap', 'subj_anchors', 'sub_position','sub_label']].apply(lambda x: insert_multiple_anchors('template_sap', *x) if x[1][0]!='MISSING' else template_sap.replace("[X]", x[0]), axis=1)

        elif original_prompt_source=='original_context':
                df[dap_col_name] = df[['masked_sentences','subj_anchors', 'sub_position', 'sub_label']].apply(lambda x: insert_multiple_anchors(original_prompt_source, x[0][0], anchors=x[1], sub_position_start=x[2], sub_label=x[3], use_original_prompt=use_original_prompt, incorporate_operation=incorporate_operation  ) if x[1][0]!='MISSING' else x[0], axis=1) 
        elif original_prompt_source=='templated_double_anchor':
            df[dap_col_name] = df[['masked_double_anchor_prompts','subj_anchors']].apply(lambda x: insert_multiple_anchors_by_relacement(x[0], anchors=x[1],  original_string='[Z]' ) if x[1][0]!='MISSING' else x[0], axis=1) 

    else: #221005: posy: come back to here when some relaitons are not using dap
        df[dap_col_name] = df[['sub_label', 'subj_anchors']].apply(lambda x: template_sap.replace("[X]", x[:top_k_anchors]), axis=1)
    return df 



''''
args: incorporate_operation ['insert', 'replace', 'concate', ]
output: a list of anchored prompts
'''





def save_dap_templates(df, relation, output_dir):
    # Save DAP to the disk 
    # df_out = df[['pred', 'masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]
    df_out = df[['masked_sentences_with_subj_anchor', 'sub_label', 'obj_label', 'uuid']]
    df_out = df_out.rename(columns={'masked_sentences_with_subj_anchor': 'masked_sentences'})
    df_out['masked_sentences'] = df_out['masked_sentences'].apply(lambda x: [x])
    # os.makedirs(output_dir, mode=0o777, exist_ok=True)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_path = f'{output_dir}/{relation}.jsonl'
    # json.dump(df_out.to_dict(orient='records'),  output_path) 
    df_out_json = df_out.to_dict( orient='records')

    save_dict_to_json(df_out_json , output_path)
    # {"sub": "abdomen", "obj": "organs", "pred": "HasA", "masked_sentences": ["The abdomen contains [MASK]."], "obj_label": "organs", "uuid": "767f3c6c02e42a55f8b1c314f7167dab"}
    # return df_out

# df[['sub_label', 'masked_sentences', 'obj_mask_incontext', 'obj_label']].sample(50)
# .to_csv("IsA.sample50.filled.human_verification.incontext.csv") #, 'masked_sentences', 'obj_mask_incontext']].sample(50)
# df[[ 'masked_anchor_prompts', 'masked_sentences', 'sub_label', 'subj_anchors', 'obj_label']].sample(50)
# df[['sub_label', 'obj_label', 'masked_sentences', 'masked_anchor_prompts', 'subj_anchors']].head()

"""# Analysis

## evaluation p@k
"""

# def get_mrr(df):

#     def get_mrr_single(label, pred):
#         '''
#         pred: a list of words (candidates)
#         label: the true label 
#         '''
#         mrr = 0 
#         if pred is not None and label in pred:
#             rank = pred.index(label) + 1
#             mrr = 1/rank 
#         return mrr 

#     df['mrr_general'] = df[['obj_label', 'obj_mask_sap']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
#     df['mrr_anchor'] = df[['obj_label', 'obj_mask_dap']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
#     df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
#     df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )
#     df['mrr_incontext'] = df[['obj_label', 'obj_mask_incontext']].apply(lambda x: get_mrr_single(x[0], x[1]), axis=1 )

#     MRR = defaultdict()
#     MRR['general'] =   round(df['mrr_general'].mean()*100, 1)
#     MRR['anchor'] =    round(df['mrr_anchor'].mean()*100, 1)
#     MRR['incontext'] = round(df['mrr_incontext'].mean()*100, 1)
#     return MRR



from evaluation import mean_average_precision, average_precision_at_k, precision_at_k
def get_precision_at_k(df, relation, pred_cols, k_list):

    p_at_x = [] #defaultdict() 
    for pred_col in pred_cols: 
        suffix = pred_col.replace("obj_", "")
        prec_cur = defaultdict()
        prec_cur['mask_type'] = suffix
        for k in k_list: 
            suffix = pred_col.replace("obj_", "")
            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=np.array(x[0]), y_pred=np.array(x[1]), k=k), axis=1 )
            # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=x[0], y_pred=x[1], k=k), axis=1 )
            df[f'p{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: 1 if x[0][0] in x[1][:k] else 0, axis=1 )
            prec_cur[f'p@{k}'] = df[f'p{k}_{suffix}'].mean() 

        p_at_x.append(prec_cur)  

    # aggregate the average precision across k 
    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res



def get_mrr(df, relation, pred_cols):
    '''
    mrr is calculated based on the top_k rank, all elements in obj_col are used
    '''

    def get_mrr_single(label, pred):
        '''
        pred: a list of words (candidates)
        label: the true label 
        '''
        mrr = 0 
        if pred is not None and label in pred:
            rank = pred.index(label) + 1
            mrr = 1/rank 
        return mrr 

    obj_cols = pred_cols #['obj_mask_sap', 'obj_mask_dap', 'obj_mask_incontext', 'obj_mask_incontext_dap', 'obj_mask_sap_dap'] 
    mrr_cols = [item.replace('obj_', 'mrr_') for item in obj_cols]
    label_col = 'obj_label'

    mrr = defaultdict()
    for i, (obj_col, mrr_col) in enumerate(zip(obj_cols, mrr_cols)):
        if not obj_col in df.columns: continue
        df[mrr_col] = df[[label_col, obj_col]].apply(lambda x: get_mrr_single(x[0][0], x[1]), axis=1 ) 
        mrr[mrr_col.replace("mrr_", "")] = round(df[mrr_col].mean()*100, 1) 

    mrr_df =  pd.DataFrame(data = mrr.items(), columns=['mask_type', 'mrr'])
    # mrr_df['mask_type']= mrr_df['mask_type'].apply(lambda x: x.replace(""))
    mrr_df['relation'] = relation
    return mrr_df 




# def get_mean_average_precision(df, relation, true_col, pred_cols, k=10):
#     '''
#     return the mean average precision for specified pred_cols
#     '''
#     df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

#     # mAP = mean_average_precision(df['obj_label_list'].to_list(), df['obj_mask_incontext'].to_list(), k=10)

#     mAP_dic = defaultdict()

#     for pred_col in pred_cols:
#         mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
#         mAP_dic[pred_col.replace("obj_", "")] = mAP

#     mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
#     mAP_df['relation'] = relation
#     return mAP_df 


# def get_precision_at_k(df, relation, pred_cols, k_list):

#     for k in k_list: 
#         for pred_col in pred_cols: 
#             suffix = pred_col.replace("obj_", "")
#             df[f'p{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: 1 if x[0] in x[1][:k] else 0, axis=1 )

#     p_at_x = defaultdict()
#     for k in k_list:
#         key = f'p@{k}'
#         p_at_x[key] = defaultdict()
#         for col in df.columns:
#             if 'top' in col: continue 
#             if f'p{k}_' in col:
#                 precision = df[col].value_counts(normalize=True).to_dict().get(1, 0) #1 is the key word of correct examples, 0 is the default value to hanld the case of all examples are wrong 
#                 precision = round(precision, 3)*100
#                 p_at_x[key][col.replace(f'p{k}_', '')] = precision 
#     df_res = pd.DataFrame(p_at_x)
#     df_res['relation'] = [relation]*len(df_res)
#     return df_res


def get_average_precision(df, relation, true_col, pred_cols, k_list):
    '''
    return the mean average precision for specified pred_cols
    '''
    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

    mAP_dic = defaultdict()
    for k in k_list:
        key = f'mAP@{k}'
        mAP_dic[key] = defaultdict()
        for pred_col in pred_cols:
            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
            mAP_dic[key][pred_col.replace("obj_", "")] = mAP

    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])
    mAP_df['relation'] = relation
    mAP_df['mask_type'] = mAP_df.index
    return mAP_df


def get_mean_average_precision(df, relation, true_col, pred_cols, k_list):
    '''
    return the mean average precision for specified pred_cols
    '''
    df['obj_label_list'] = df['obj_label'].apply(lambda x: x if isinstance(x, list) else x)

    mAP_dic = defaultdict()
    for k in k_list:
        key = f'mAP@{k}'
        mAP_dic[key] = defaultdict()
        for pred_col in pred_cols:
            mAP = mean_average_precision(df['obj_label_list'].to_list(), df[pred_col].to_list(), k=k)
            mAP_dic[key][pred_col.replace("obj_", "")] = mAP

    # mAP_df =  pd.DataFrame(data = mAP_dic.items(), columns=['mask_type', 'mAP'])
    mAP_df =  pd.DataFrame(data = mAP_dic) #, columns=['mask_type', 'mAP'])
    mAP_df['relation'] = relation
    mAP_df['mask_type'] = mAP_df.index
    return mAP_df


def get_mean_average_precision_at_k(df, relation, pred_cols, k_list):
    # get the avearage precision per query
    map_at_x = []
    for pred_col in pred_cols: 
        suffix = pred_col.replace("obj_", "")
        map_cur = defaultdict()
        map_cur['mask_type'] = suffix

        for k in k_list: 
            df[f'ap{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: average_precision_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1 )
            map_cur[f'mAP@{k}'] = df[f'ap{k}_{suffix}'].mean()    
        map_at_x.append(map_cur)

    # aggregate the average precision across k 
    df_res = pd.DataFrame(map_at_x) #, columns=['mask_type', 'mAP'])
    df_res['relation'] = [relation]*len(df_res)
    return df_res

# df_mAP = get_mean_average_precision_at_k(df, relation='IsA', pred_cols=['obj_mask_incontext', 'obj_mask_dap'], k_list=[10, 1816])

# def get_precision_at_k(df, relation, pred_cols, k_list):

#     p_at_x = [] #defaultdict() 
#     for pred_col in pred_cols: 
#         suffix = pred_col.replace("obj_", "")
#         prec_cur = defaultdict()
#         prec_cur['mask_type'] = suffix
#         for k in k_list: 
#             suffix = pred_col.replace("obj_", "")
#             # df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=np.array(x[0]), y_pred=np.array(x[1]), k=k), axis=1 )
#             df[f'p@{k}_{suffix}'] = df[['obj_label', pred_col]].apply(lambda x: precision_at_k(y_true=x[0], y_pred=x[1], k=k), axis=1 )
#             prec_cur[f'p@{k}'] = df[f'p@{k}_{suffix}'].mean() 

#         p_at_x.append(prec_cur)  

#     # aggregate the average precision across k 
#     df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])
#     df_res['relation'] = [relation]*len(df_res)
#     return df_res
    
# df_p_at_k = get_precision_at_k(df, relation='Has', pred_cols=['obj_mask_incontext', 'obj_mask_dap'], k_list=[10, 1816])

"""### Analysis helper functions"""

def get_rel_specific_results(df_res_all, mask_type):
# rel = 'mask_sap'
# rel = 'mask_dap'
    dfc = df_res_all.query(f"mask_type == '{mask_type}'").reset_index(drop=True)
    # round(dfc['p@1'].mean(), 2)
    
    overall = {"p@1":   round(dfc['p@1'].mean(), 1),
                "p@3": round(dfc['p@3'].mean(), 1),
                "p@5": round(dfc['p@5'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "p@10": round(dfc['p@10'].mean(), 1),
                "mrr": round(dfc['mrr'].mean(), 1),
                "relation": 'Overall'
                # "mrr":  round(dfc['mrr'].mean(), 1),
                # "support": dfc['support'].sum() 
                } 
    
    dfc_overall = pd.DataFrame(overall, index=['Overall'])
    # dfc = dfc.sort_values(by=['p@1'])
    dfc = dfc.sort_values(by=['relation'])

    dfc = pd.concat([dfc, dfc_overall]).reset_index(drop=True).sort_values(['relation'])
    
    return  dfc[['relation', 'p@1','p@10', 'mrr', 'p@3', 'p@5', 'mask_type']]

def calculate_gains(df1, df2):
    '''
    df1: sap
    df2: dap
    '''
    metrics = ['p@1', 'p@3', 'p@5', 'p@10', 'mrr']
    df_gains = []
    for col in metrics:
        gains = df2[col] - df1[col]
        gains.column = col 
        df_gains.append(gains)
    
    df_gains = pd.concat(df_gains, axis=1 )
    df_gains['relation'] = df2.relation
    return df_gains[['relation', 'p@1', 'p@10', 'mrr', 'p@3', 'p@5']]

def display_gains(col1, col2, df_res_all, output_file=None):
    ''' 
    goal: diaplay the gains from col1 to col2
    col1: the baseline column 
    '''
    df1 = get_rel_specific_results(df_res_all, mask_type=col1)
    df2 = get_rel_specific_results(df_res_all, mask_type=col2)
    print(f'gains: {col2} - {col1}')
    df_gains = calculate_gains(df1, df2)
    df_gains['mask_type'] =  [ 'Gains(DAP-SAP)'] * len(df_gains.index)
    
    df_out1 = pd.concat([df1, df2, df_gains], axis=1)
    display(df_out1)
    display(df1)
    display(df2)
    if output_file is not None:
        print(f"Saving final results to {output_file}")
        df_out1.to_csv(f"{output_file}")

def aggregate_candidates(dic1, dic2, top_k):
    '''
    input are two list of candidates [{token: score}]
    goal: merge the two lists 
    1. co-occurred words 
        strategy 1 - average: (score1 + score)/2 
        strategy 2 - accumulate: score1 + score
    2. cut off the number of all candiates to top_k
    '''
    keys = set(dic1.keys()).union(dic2.keys())
    new_dic = defaultdict() 

    for key in keys:
        new_dic[key] = dic1.get(key, 0) + dic2.get(key, 0)
    new_dic = dict(Counter(new_dic).most_common(top_k))
    return list(new_dic.keys())

"""# The main function"""

# ---------- config ----------------
debug=False 
# debug = True 
# test_relations =  ['Has', 'HasA', "Is", 'IsA'] #['Has', 'Is', 'MadeOf', 'HasA', 'IsA']
# test_relations = ['IsA'] #['HasA', 'IsA', "MadeOf"]
test_relations = ['HasA', 'HasProperty', 'IsA'] #, 'PartOf', 'MadeOf']

save_all_data= True  
#save_all_data =  False
top_k=10
# data_dir = 'data/WNLaMPro/'
data_dir = 'data/sap_filter/'
# data_dir = 'data/clsb/everyone_knows/'
# data_dir = 'data/clsb/blank/'
# data_dir = 'data/clsb/best/'
# data_dir = './'
mask_string_mapping = {
                "roberta-large": "<mask>", 
            #    "bert-base-cased": "[MASK]",
            #    "bert-large-cased": "[MASK]"
               }

# use_dap_global = False #True
use_dap_global = True
max_anchor_num = 3 
return_probs = True
dataset_name = 'LAMA'
scorer_list = ['freqProbLogSum'] #[  'freq', 'probSum', 'probAvg', 'freqProbSum', 'probLogSum', 'freqProbLogSum']
scorer_anchor = 'freq' #'freqProbSum'#anchor_scorer_list[0]
scorer_target_1_prompt = 'probSum'
scorer_target_N_prompts = 'probLogSum'

use_original_prompt = True
constrain_targets= False#True #False
original_prompt_source =  'original_context' #'templated_double_anchor'
incorporate_operations = [ "concate_or_single"] #, "concate_comma_multiple", "replace" ]


# ---------- config ----------------
max_anchor_num_list =[5] #[1,3,5,10]
for incorporate_operation in incorporate_operations :
    for max_anchor_num in  max_anchor_num_list :
        for scorer_anchor in scorer_list :

            for model in mask_string_mapping.keys(): #['bert-large-cased']

                # get the unmasker 
                if constrain_targets: 
                    target_vocab = get_target_vocab(file_path = data_dir+"vocab.txt")
                    unmasker_obj = get_unmasker(model, targets=target_vocab) #### initialize the fill-the-blank model 
                    top_k = len(target_vocab) 
                else:
                    unmasker_obj = get_unmasker(model)
                unmasker_anchor = get_unmasker(model) #### initialize the fill-the-blank model 


                mask_string = mask_string_mapping[model]
                save_dir = f'log/{model}/{data_dir.split("/")[-2]}' 
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                for anchor_type in ["coordinate"]: # ,  "synonym"]:
                    double_anchor_type = 'hypernym'
                # anchor_type="synonym" 
                    print("-"*40, f"{model}-anchor_type: {anchor_type}-scorer_anchor:{scorer_anchor}-op:{incorporate_operation}", "-"*40)
                    relation_to_template  = get_relation_templates(relations, model, anchor_type, anchor_type_to_prompts, double_anchor_type, double_anchor_type_to_prompts)
                    if debug:
                        print(json.dumps(relation_to_template, indent=4))

                    dfs = []
                    df_res_all = []
                    # for relation in tqdm(relation_to_template.keys()):
                    # for relation in tqdm(['MadeOf']): #,'WAX''ReceivesAction', 'CapableOf',   'HasA']):'HasProperty' 'HasProperty'
                    for relation in test_relations: #, 'IsA', 'Has', 'HasA', 'MadeOf']: #['IsA', 'HasA']:
                    # for relation in tqdm (['HasProperty']): #(["MotivatedByGoal"] ): 
                    # for relation in tqdm(['CapableOf', 'HasProperty', 'UsedFor', 'ReceivesAction', 'HasProperty']):

                        if not relation_to_template[relation]['use_dap'] and not use_dap_global : 
                            print(f"skipping {relation}")
                            continue 
                        filepath = f'{data_dir}/{relation}.jsonl'
                        print(f"Processing {relation} ... {filepath}")
                        df = get_masked_data(filepath, relation, relation_to_template, model=model) #anchor mask, general mask,
                        if debug:
                            df = df.head(5)

                        print("\t step1: get anchors (fill and filter)")
                        # step1: get the anchor
                        outputs = fill_mask_obj(df, unmasker_obj)  #fill the original prompts with PTLM 
                        outputs.update(fill_mask_anchor(df, unmasker_anchor))  #fill the anchor prompts with PTLM 

                        df[['obj_mask_incontext', 'obj_mask_incontext_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sentences'], return_probs=return_probs, top_k=top_k, scorer=scorer_target_1_prompt )
                        df[['obj_mask_sap', 'obj_mask_sap_score']] = fillter_outputs_with_probs(df.sub_label.to_list(), outputs['masked_sap'], return_probs=return_probs, top_k=top_k, scorer=scorer_target_1_prompt )

                        # df = add_filter_outputs(df, outputs) #filter the anchor candiates
                        if use_dap_global:
                            df = add_filter_outputs_with_probs(df, outputs, scorer = scorer_anchor,  return_probs=True, top_k=max_anchor_num) #filter the anchor candiates

                            use_dap = relation_to_template[relation]['use_dap']  if not use_dap_global else use_dap_global 
                            
                            print(f"relation: {relation} \t use_dap: {use_dap}")
                            # step2: fill DAP
                            print("\t step2: insert anchors")
                            df = fill_anchor_into_dap(df, relation, relation_to_template, use_dap, incorporate_operation = incorporate_operation , original_prompt_source= original_prompt_source, top_k_anchors = max_anchor_num, dap_col_name='masked_sentences_with_subj_anchor',mask_string = mask_string, use_original_prompt=use_original_prompt) #create the dap promtps/masks 

                            if use_dap:
                                #step3: filter outputs 
                                print("\t step3: fill mask in dap anchors")
                                outputs['obj_mask_dap'] = [unmasker_obj(x, top_k=top_k) for x in tqdm(df.masked_sentences_with_subj_anchor.to_list())]
                                df['subj_anchors_combined'] = df[['sub_label', 'subj_anchors']].apply(lambda x: x[0] + " " + " ".join(x[1]), axis=1)

                                #TODO: 221014 note that the follwing code is likely to affect the results a lot: whether using the anchors to filter the targets.
                                # need experiments to compare
                                df[['obj_mask_dap', 'obj_mask_dap_score']] = fillter_outputs_with_probs(df.subj_anchors_combined.to_list(), outputs['obj_mask_dap'],  return_probs=return_probs, top_k=top_k, scorer= scorer_target_N_prompts)

                                # step4: merge dap and sap
                                df['obj_mask_sap_dap'] = df[[ 'obj_mask_sap_score', 'obj_mask_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                                df['obj_mask_incontext_dap'] = df[['obj_mask_incontext_score', 'obj_mask_dap_score']].apply(lambda x: aggregate_candidates(x[0], x[1], top_k=top_k), axis=1)
                            else:
                                df['subj_anchors_combined'] = df[['sub_label', 'subj_anchors']].apply(lambda x: x[0] + " " + " ".join(x[1]), axis=1)
                                df['obj_mask_dap'] = df.obj_mask_sap

                            display(df.head())
                            save_dap_templates(df, relation, output_dir=f'{save_dir}/dap/{anchor_type}')

                        # Evaluations
                        pred_cols  =[col  for col in df.columns if col.startswith("obj_mask_") and "_score" not in col]  #predicted target cols, e
                        # df_prec = get_precision_at_k(df, relation, pred_cols, k_list = [1,3,10,top_k])
                        df_prec = get_precision_at_k(df, relation, pred_cols, k_list=[1, 10, top_k]) ##note that this would be super slow when top_k is large (>1000) 
                        df_mrr =  get_mrr(df, relation, pred_cols)

                        df_prec['mrr'] = df_prec['mask_type'].apply(lambda x:  df_mrr.loc[df_mrr['mask_type']==x, 'mrr'].values[0])
                        # df_prec['mask_type'] = df_prec.index
                        df_mAP = get_mean_average_precision_at_k(df, relation, pred_cols, k_list=[1, 10, top_k])

                        # df_mAP = get_mean_average_precision(df, relation, 'obj_label', pred_cols, k_list=[10, top_k]) 
                        for k in [1, 10, top_k]:
                            df_prec[f'mAP@{k}'] = df_prec['mask_type'].apply(lambda x:  df_mAP.loc[df_mAP['mask_type']==x, f'mAP@{k}'].values[0])
                        
                        # df_prec = df_prec[['mask_type', 'p@1', 'p@10', 'mrr', 'p@3', 'mAP', 'relation' ]]
                        display(df_prec)
                        df_res_all.append( df_prec )
                        df['relation'] = [relation]*len(df)
                        dfs.append(df)

                    # Aggregate and save results and saving 
                    
                    df_res_all = pd.concat(df_res_all).reset_index(drop=True)
                    if len(test_relations)>1:
                        for name,group in df_res_all.groupby('relation'):
                            display(group)
                    # if not debug:
                    file_results = f'{save_dir}/df_all_use_global_dap_{use_dap_global}_anchor_type_{anchor_type}_{incorporate_operation}_max_anchor_num_{max_anchor_num}_anchor_scorer_{scorer_anchor}.csv'
                    df_res_all.to_csv(file_results)
                    print(f"save {file_results}")
                    
                    if save_all_data:
                        file_data_results = f'{save_dir}/exp_data_results_anchor_type_{anchor_type}_{incorporate_operation}_max_anchor_num_{max_anchor_num}_anchor_scorer_{scorer_anchor}.csv' 
                        dfs_data = pd.concat(dfs)
                        dfs_data.to_csv(file_data_results)
                        print(f"save {file_data_results}")
                    

                ## df_res_all = pd.read_csv(file_results , index_col=0)
                ## df_res_all.head()
                #col1 = 'mask_incontext'  #baseline columns
                #col2 = 'mask_dap'
                
                #output_file = f"{save_dir}/{anchor_type}_{col1}_vs_{col2}.csv"
                #display_gains(col1, col2, df_res_all, output_file= output_file if not debug else None )

# print("#incontext", len(df.query("ap10_mask_incontext>0").index))
# print("#dap", len(df.query("ap10_mask_dap>0").index))

# df1= df.query("ap10_mask_incontext>0 and ap10_mask_dap==0")
# df2= df.query("ap10_mask_incontext==0 and ap10_mask_dap>0")
# print(len(df1.index))
# print(len(df2.index))

# df2['obj_mask_incontext'] = df2['obj_mask_incontext'].apply(lambda x: x[:10])
# df2['obj_mask_dap'] = df2['obj_mask_dap'].apply(lambda x: x[:10])

# df2[['sub_label', 'subj_anchors', 'obj_label','obj_mask_incontext', 'obj_mask_dap']]

# df[[ 'sub_label', 'subj_anchors',  'masked_sap', 'obj_label']].head(50) #'masked_sentences_with_subj_anchor',
# df['']
# df.head()
# df.query
# from evaluation import average_precision_at_k 

# df['masked_anchor_prompts'].head()
# df[['average_precision_at_10','sub_label', 'obj_label', 'obj_mask_incontext', 'obj_mask_dap', 'subj_anchors']]

# #
# df['obj_label_list'] = df['obj_label'].apply(lambda x: [x])
# mAP = mean_average_precision(df['obj_label_list'].to_list(), df['obj_mask_incontext'].to_list(), k=10)
# print(mAP)

# test_mean_average_precision()

"""# Analysis"""

# df.head()

# df.head()
# df.query("p1_mask_incontext_dap==1 and p1_mask_incontext==0")[['obj_label', 'obj_mask_incontext', 'obj_mask_dap']]

# df_positive = df.query("p1_mask_dap ==1 and p1_mask_incontext ==0")[['sub_label', 'obj_label', 'masked_sentences', 'obj_mask_incontext', 'obj_mask_dap', 'subj_anchors' ]]
# print(len(df_positive.index))
# display(df_positive)
# uncommon_words = ["talc", "genocide", "xenon", "waxwing", "bourbon", "zork", "wok", "pewter", "lyre"]
# df_positive.query(f"sub_label in {uncommon_words}")

# # df[['obj_mask_incontext_score', 'obj_mask_dap_score', 'obj_mask_incontext_dap', 'obj_label']].sample(50)
# df[['obj_mask_incontext', 'obj_mask_dap', 'obj_mask_incontext_dap', 'obj_label']].sample(50)
# # df.columns
# # df['obj_mask_sap_score'].to_list()
# list_incontext = df['obj_mask_incontext'].to_list()
# list_dap = df['obj_mask_dap'].to_list()
# list_obj = df['obj_label'].to_list()

# counts = dict({'1':0, '3':0, '5':0, '10':0})
# correct_incontext =[]
# correct_dap = [] 
# both_wrong = []

# for k in [1,5,10]:
#     for x,y,z in zip(list_incontext, list_dap, list_obj):
#         # print(type(x),type(y))
#         if set(x[:k])!=set(y[:k]):
#             # count +=1
#             counts[str(k)] +=1
#             if z in x[:k]: 
#                 correct_incontext.append((x[:k], y[:k], z))
#             elif z in y[:k]:
#                 correct_dap.append((x[:k], y[:k], z))
#             else:
#                 both_wrong.append((x[:k], y[:k], z))
# # print(counts)

# print(len(correct_incontext ))
# print((correct_incontext ))
# print("-"*80)
# print(len(correct_dap ))
# print((correct_dap ))
# print("-"*80)
# print(len(both_wrong ))
# print((both_wrong ))

# display(dfs_data.columns)
# dfs_data.head()

# # dfe = dfs_data.query("p3_mask_incontext==1 and p3_mask_dap==0")[['relation','sub_label', 'obj_label', 'masked_sentences', 'subj_anchors', 'obj_mask_incontext', 'obj_mask_dap', 'masked_sentences_with_subj_anchor']]
# # dfe['relation'].value_counts(normalize=True).to_frame()
# # dfs_data['relation'].value_counts(normalize=False).to_frame().plot(kind='bar')

# # .to_csv("log/221011_ErrorAnalysis.csv")

# # dfs_property.query("obj_label")
# # chekc whether obj_label occurred in anchors

# dfs_r = dfs_data.copy().query("relation=='IsA' and p3_mask_incontext==1 and p3_mask_dap==0")
# dfs_r ['obj_label_in_anchors'] = dfs_r[['obj_label', 'subj_anchors']].apply(lambda x: 'obj_label_in_anchors' if x[0] in x[1] else 'obj_label_not_in_anchors', axis=1)
# tmp_dict= defaultdict()
# for name, group in dfs_r.groupby('relation'):
#      tmp_dict[name]= group['obj_label_in_anchors'].value_counts(normalize=False).to_dict()
# pd.DataFrame(data=tmp_dict).T
# dfs_r[['relation','sub_label', 'obj_label', 'masked_sentences', 'subj_anchors', 'obj_mask_incontext', 'obj_mask_dap', 'masked_sentences_with_subj_anchor', 'obj_label_in_anchors']].to_csv("log/221011_ErrorAnalysis_IsA.csv")

# # dfs_r.query("obj_label_in_anchors= 'obj_label_not_in_anchors'")

#     #  tmp_dict[name] = 
# # ['sub_label', 'obj_label', 'masked_sentences', 'uuid', 'sub_position',
# #        'masked_sap', 'masked_anchor_prompts', 'subj_anchors',
# #        'subj_anchors_score', 'obj_mask_incontext', 'obj_mask_incontext_score',
# #        'obj_mask_sap', 'obj_mask_sap_score', 'top1_subj_anchor',
# #        'top1_incontext_obj', 'masked_sentences_with_subj_anchor',
# #        'subj_anchors_combined', 'obj_mask_dap', 'obj_mask_dap_score',
# #        'obj_mask_sap_dap', 'obj_mask_incontext_dap', 'p1_mask_sap',
# #        'p1_mask_dap', 'p1_mask_incontext', 'p1_mask_incontext_dap',
# #        'p1_mask_sap_dap', 'p3_mask_sap', 'p3_mask_dap', 'p3_mask_incontext',
# #        'p3_mask_incontext_dap', 'p3_mask_sap_dap', 'p5_mask_sap',
# #        'p5_mask_dap', 'p5_mask_incontext', 'p5_mask_incontext_dap',
# #        'p5_mask_sap_dap', 'p10_mask_sap', 'p10_mask_dap', 'p10_mask_incontext',
# #        'p10_mask_incontext_dap', 'p10_mask_sap_dap', 'mrr_mask_sap',
# #        'mrr_mask_dap', 'mrr_mask_incontext', 'mrr_mask_incontext_dap',
# #        'mrr_mask_sap_dap', 'relation']

# import pandas as pd

# inspect_types = ['mask_sap', 'mask_dap', 'mask_incontext', 'mask_']

# # for name, group in df_res_all.groupby('relation'):
# #     group = group.query(f"mask_type in {inspect_types}")
# #     # group = group[group.drop('Unnamed: 0', axis=1)]
# #     display(group)

"""## Development"""

# # df_hasa = load_data(filepath='data/sap_filter/HasA.jsonl')
# df_property = load_data(filepath='data/sap_filter/HasProperty.jsonl')
# df_property.head()

# def get_mrr_single(label, pred):
#     '''
#     pred: a list of words (candidates)
#     label: the true label 
#     '''
#     mrr = 0 
#     if pred is not None and label in pred:
#         rank = pred.index(label) + 1
#         mrr = 1/rank 
#     return mrr 

# pred_sap = [('happy', 6), ('good', 6), ('important', 6), ('nice', 5), ('powerful', 4), ('angry', 3), ('sad', 3), ('rich', 3), ('friendly', 3), ('busy', 3), ('popular', 3), ('special', 3), ('poor', 2), ('intelligent', 2), ('vulnerable', 2), ('emotional', 2), ('close', 2), ('comfortable', 2), ('different', 2), ('dangerous', 2), ('hungry', 1), ('surprised', 1), ('excited', 1), ('active', 1), ('polite', 1), ('aggressive', 1), ('lucky', 1), ('bad', 1), ('stupid', 1), ('strong', 1), ('interesting', 1), ('smart', 1), ('much', 1), ('aware', 1), ('...', 1), ('proud', 1)]

# pred_dap = [('different', 1727), ('important', 1494), ('powerful', 1085), ('complex', 938), ('similar', 889), ('common', 778), ('interesting', 744), ('special', 668), ('good', 586), ('social', 513), ('personal', 493), ('big', 489), ('specific', 474), ('rare', 461), ('simple', 460), ('complicated', 447), ('intelligent', 425), ('dangerous', 423), ('large', 413), ('valuable', 381), ('separate', 338), ('human', 311), ('expensive', 311), ('real', 309), ('unique', 305), ('small', 302), ('strange', 283), ('significant', 278), ('strong', 244), ('useful', 230), ('diverse', 216), ('much', 213), ('distinct', 202), ('popular', 190), ('few', 179), ('emotional', 179), ('sensitive', 169), ('living', 157), ('natural', 147), ('subjective', 144), ('difficult', 137), ('smart', 131), ('stressful', 122), ('sentient', 119), ('fragile', 117), ('like', 109), ('endangered', 108), ('limited', 108), ('confusing', 102), ('serious', 102), ('aggressive', 100), ('effective', 100), ('hostile', 100), ('of', 98), ('for', 93), ('bad', 91), ('political', 90), ('same', 90), ('great', 88), ('funny', 86), ('controlled', 84), ('precious', 82), ('close', 81), ('nice', 79), ('little', 79), ('influential', 78), ('scarce', 77), ('dynamic', 76), ('toxic', 74), ('often', 72), ('successful', 67), ('biological', 63), ('public', 63), ('near', 62), ('unlike', 61), ('many', 59), ('noun', 58), ('fictional', 58), ('rarely', 56), ('private', 55), ('harsh', 55), ('scary', 52), ('person', 52), ('single', 50), ('legal', 49), ('nearly', 49), ('vulnerable', 48), ('word', 45), ('religious', 44), ('unusual', 44), ('two', 43), ('abstract', 43), ('protected', 43), ('selfish', 42), ('household', 42), ('group', 40), ('competitive', 40), ('resilient', 40), ('negative', 39), ('major', 39), ('well', 39), ('new', 38), ('challenging', 38), ('business', 37), ('easily', 36), ('and', 35), ('finite', 35), ('physical', 34), ('helpful', 34), ('individual', 33), ('or', 33), ('defined', 33), ('loving', 33), ('happy', 32), ('complete', 32), ('wonderful', 31), ('related', 31), ('professional', 31), ('beautiful', 30), ('given', 30), ('family', 30), ('supernatural', 30), ('traumatic', 30), ('positive', 29), ('pet', 29), ('tangible', 29), ('safe', 29), ('list', 28), ('whole', 28), ('known', 28), ('news', 28), ('seldom', 28), ('wild', 27), ('loyal', 26), ('hard', 25), ('plural', 25), ('spiritual', 25), ('invasive', 25), ('such', 25), ('basic', 24), ('particular', 24), ('nonprofit', 24), ('classified', 23), ('friendly', 22), ('mysterious', 22), ('global', 22), ('very', 21), ('huge', 21), ('pronounced', 21), ('charitable', 21), ('stimulating', 21), ('corporate', 20), ('native', 20), ('thing', 20), ('tragic', 20), ('collective', 19), ('curious', 19), ('dog', 19), ('old', 19), ('destructive', 19), ('likely', 19), ('each', 19), ('to', 18), ('rich', 18), ('uncommon', 18), ('current', 18), ('singular', 17), ('familiar', 17), ('famous', 17), ('sad', 17), ('healthy', 17), ('lot', 16), ('certain', 16), ('financial', 16), ('active', 16), ('free', 16), ('bird', 16), ('virtual', 16), ('work', 16), ('really', 16), ('term', 15), ('costly', 15), ('commercial', 15), ('japanese', 15), ('passionate', 15), ('forgiving', 15), ('supportive', 15), ('mammalian', 15), ('plant', 15), ('written', 15), ('into', 15), ('couple', 14), ('domestic', 14), ('violent', 14), ('relative', 14), ('rewarding', 14), ('not', 14), ('unpredictable', 14), ('controversial', 13), ('chinese', 13), ('world', 13), ('noisy', 13), ('intimate', 12), ('mental', 12), ('sexual', 12), ('annoying', 12), ('unlikely', 12), ('volunteer', 12), ('recent', 12), ('cat', 11), ('manipulative', 11), ('autonomous', 11), ('normal', 11), ('conservative', 11), ('controlling', 11), ('mammal', 11), ('demanding', 11), ('past', 11), ('polluted', 11), ('intimidating', 11), ('in', 10), ('medical', 10), ('government', 10), ('comfortable', 10), ('category', 9), ('material', 9), ('cultural', 9), ('generic', 9), ('humanoid', 9), ('food', 9), ('fish', 9), ('community', 9), ('favorite', 9), ('worldwide', 9), ('historical', 9), ('mythical', 8), ('young', 8), ('national', 8), ('confused', 8), ('threatening', 8), ('calendar', 8), ('nurturing', 8), ('company', 7), ('stable', 7), ('wealthy', 7), ('misunderstood', 7), ('threatened', 7), ('critical', 7), ('media', 7), ('random', 7), ('closed', 7), ('compound', 6), (',', 6), ('terrestrial', 6), ('tricky', 6), ('angry', 6), ('the', 6), ('tree', 6), ('microscopic', 6), ('contraction', 6), ('long', 6), ('efficient', 6), ('life', 6), ('exciting', 6), ('just', 6), ('love', 5), ('cruel', 5), ('korean', 5), ('primitive', 5), ('short', 5), ('rodent', 5), ('web', 5), ('about', 5), ('bodily', 5), ('general', 5), ('painful', 5), ('species', 5), ('canadian', 5), ('true', 5), ('humanitarian', 5), ('charity', 5), ('search', 5), ('marketing', 5), ('dictionary', 5), ('clean', 5), ('thousand', 4), ('terrible', 4), ('sacred', 4), ('universal', 4), ('companion', 4), ('protective', 4), ('clever', 4), ('better', 4), ('independent', 4), ('deceptive', 4), ('tolerant', 4), ('ancient', 4), ('unpleasant', 4), ('divided', 4), ('jewish', 4), ('vital', 4), ('from', 4), ('used', 4), ('local', 4), ('service', 4), ('memorable', 4), ('unhealthy', 4), ('pleasant', 4), ('hypothetical', 4), ('broad', 4), ('than', 4), ('three', 3), ('binary', 3), ('intangible', 3), ('monetary', 3), ('money', 3), ('between', 3), ('heavy', 3), ('difference', 3), ('farm', 3), ('stubborn', 3), ('nasty', 3), ('foreign', 3), ('secretive', 3), ('named', 3), ('delicate', 3), ('poor', 3), ('a', 3), ('muslim', 3), ('mosquito', 3), ('reliable', 3), ('cheap', 3), ('people', 3), ('with', 3), ('key', 3), ('chemical', 3), ('genetic', 3), ('frightening', 3), ('talented', 3), ('cute', 3), ('beloved', 3), ('loved', 3), ('rescue', 3), ('terrorist', 3), ('multinational', 3), ('structured', 3), ('collection', 3), ('romantic', 3), ('calming', 3), ('sterile', 3), ('closely', 3), ('.', 3), ('literally', 3), ('is', 3), ('worth', 3), ('several', 2), ('linguistic', 2), ('shared', 2), ('relationship', 2), ('classification', 2), ('gentle', 2), ('place', 2), ('herd', 2), ('fellow', 2), ('among', 2), ('mean', 2), ('house', 2), ('geographical', 2), ('governmental', 2), ('solitary', 2), ('magical', 2), ('varied', 2), ('fundamental', 2), ('marginalized', 2), ('proud', 2), ('creative', 2), ('greedy', 2), ('persuasive', 2), ('focused', 2), ('attractive', 2), ('greek', 2), ('turkish', 2), ('german', 2), ('canine', 2), ('horse', 2), ('downloadable', 2), ('handy', 2), ('fantastic', 2), ('listed', 2), ('google', 2), ('marine', 2), ('evil', 2), ('intense', 2), ('frustrating', 2), ('busy', 2), ('harmful', 2), ('potent', 2), ('american', 2), ('proper', 2), ('responsible', 2), ('distracting', 2), ('their', 2), ('essential', 2), ('pets', 2), ('sports', 2), ('defunct', 2), ('international', 2), ('profitable', 2), ('membership', 2), ('damaging', 2), ('predictable', 2), ('sporting', 2), ('notable', 2), ('dramatic', 2), ('volatile', 2), ('working', 2), ('welcoming', 2), ('collaborative', 2), ('pronoun', 2), ('no', 2), ('versatile', 2), ('vague', 2), ('sentimental', 1), ('animate', 1), ('metal', 1), ('kitchen', 1), ('trusting', 1), ('shy', 1), ('prey', 1), ('passion', 1), ('governing', 1), ('sea', 1), ('minority', 1), ('peculiar', 1), ('white', 1), ('peaceful', 1), ('trustworthy', 1), ('suspicious', 1), ('displaced', 1), ('educated', 1), ('generous', 1), ('dishonest', 1), ('lonely', 1), ('boring', 1), ('vietnamese', 1), ('ill', 1), ('tall', 1), ('finnish', 1), ('computer', 1), ('software', 1), ('bacterial', 1), ('rat', 1), ('selective', 1), ('comprehensive', 1), ('informative', 1), ('geographic', 1), ('literary', 1), ('physiological', 1), ('tiny', 1), ('superior', 1), ('evolved', 1), ('helpless', 1), ('immature', 1), ('metaphysical', 1), ('united', 1), ('communal', 1), ('connected', 1), ('divine', 1), ('recognizable', 1), ('verb', 1), ('position', 1), ('experienced', 1), ('irritating', 1), ('frustrated', 1), ('translated', 1), ('commonly', 1), ('far', 1), ('form', 1), ('sign', 1), ('charismatic', 1), ('hybrid', 1), ('land', 1), ('australian', 1), ('disruptive', 1), ('brazilian', 1), ('nigerian', 1), ('filipino', 1), ('addictive', 1), ("'s", 1), ('persons', 1), ('entertaining', 1), ('veterinary', 1), ('flexible', 1), ('criminal', 1), ('welfare', 1), ('networking', 1), ('informal', 1), ('registered', 1), ('various', 1), ('hierarchical', 1), ('voluntary', 1), ('military', 1), ('paramilitary', 1), ('christian', 1), ('french', 1), ('multiple', 1), ('currency', 1), ('cash', 1), ('storage', 1), ('ballot', 1), ('technical', 1), ('missing', 1), ('text', 1), ('temporal', 1), ('running', 1), ('facebook', 1), ('crucial', 1), ('changing', 1), ('variable', 1), ('visual', 1), ('hazardous', 1), ('learning', 1), ('relaxed', 1), ('coercive', 1), ('contaminated', 1), ('workplace', 1), ('simulated', 1), ('humid', 1), ('substantial', 1), ('female', 1), ('total', 1), ('conceptual', 1), ('wide', 1), ('child', 1), ('friend', 1), ('int', 1), ('interested', 1), ('means', 1), ('almost', 1), ('damn', 1), (':', 1)]
# labels = set(list(df_property.query("sub_label =='people'")['obj_label']))
# # labels = ['creative', 'intelligent', 'unemployed', 'jealous', 'ridiculous', 'friendly', 'silly', 'special', 'strange', 'talented', 'crazy', 'dangerous', 'ignorant', 'unique', 'kind', 'lazy', 'unfair', 'famous', 'vicious', 'optimistic', 'evil', 'bad', 'messy', 'sad', 'short', 'blond', 'stupid', 'neat', 'thoughtful', 'deaf', 'caring', 'bold', 'mad', 'rude', 'selfish', 'thin', 'sick', 'generous', 'dumb', 'cruel', 'curious', 'different', 'musical', 'blind', 'insane', 'skinny', 'pagan', 'smart', 'tall', 'honest', 'good', 'nice', 'normal', 'mean', 'ethical', 'weird', 'racist', 'charitable', 'cold', 'poor', 'fat', 'angry', 'wise']
# # label = 'good'
# dfs_tmp = []
# for k  in [30]:
# # [1,5, 10]:# , 30]:
#     print("-"*70)
#     shared_all = defaultdict() 
#     for pred, anchor_type in zip([pred_sap, pred_dap], ['sap', 'dap']):
#         mrrs = []
#         tmp = {}
#         for label in labels:
#             mrr = get_mrr_single(label, [x[0] for x in pred[:k]])
#             mrrs.append(mrr)
#         print( "K:{} {} Mean MRR:{}".format(k,anchor_type, sum(mrrs)/len(mrrs)))
#         # shared = labels.intersection(set(dict(pred[:k]).keys()))
#         shared = labels.intersection(set(dict(pred[:]).keys()))
#         tmp[k] = shared
#         shared_all[anchor_type] = tmp
#         # shared_all[anchor_type][k] = shared
#         print( "#Shared/#Pred: {}/{}".format(len(shared), len(pred)))
#         print( "#Shared/#True: {}/{}".format(len(shared), len(labels)))
#         # print( "#Shared", shared)

# shared_all

# print(list(labels))

# df_property.query("sub_label =='cycling'")
# Counter(df_property['sub_label']).most_common()
# len(pred_sap)

# for sub in ['flowers', 'people', 'cycling', 'apples']: 
# # for sub in df_property
#     print("-"*45, sub, "-"*45)
#     template_anchors_sap = [
#         f"the {sub} is very <mask> .",
#         f"the {sub} was very <mask> .",
#         f"{sub} is usually very <mask> .",
#         f"this {sub} is very <mask> .",
#         f"that is a very <mask> {sub} .",
#         f"something {sub} is very <mask> .",
#     ]

#     template_anchors_similar = [
#         f"{sub} and <mask> have similar properties .",
#         f"{sub} and <mask> share similar properties .",
#         f"{sub} and <mask> are similar .",
#         f"{sub} and <mask> have similar meanings .",
#         f"{sub} have a similar meaning as <mask> .",
#         f"{sub} and <mask> are synonyms .",
#         f"{sub} and <mask> are synonymous .",
#         f'{sub} and <mask> mean the same thing .',
#         f'{sub} and <mask> are the same thing .',
#         f'{sub} means the same thing as <mask> .',
#         ]

#     template_anchors_coordinate = [
#         f"such as {sub} and <mask> .",
#         f"such as {sub} or <mask> .",
#         f", such as {sub} and <mask> .", 
#         f"as {sub} or <mask> .", 
#         f", such as {sub} and <mask> .", 
#         f", such as {sub} and <mask> .", 
#         ]


#     template_anchors_hyponym = [
#         f"{sub} is a type of <mask> .",
#         f"such <mask> as {sub}, and ",
#         f"such <mask> as {sub}, or ",
#         f"such as {sub} and other <mask> .",
#         f"such as {sub} or other <mask> .",
#         f", {sub}, and other <mask> .",
#         f", {sub}, or other <mask> .",
#         f"<mask>, including {sub} and ",
#         f"<mask>, including {sub}, or ",
#         ]

#     targets_sap = [] 
#     print("-"*80)
#     for template_anchor  in template_anchors_sap: 
#         output = unmasker(template_anchor , top_k=10)
#         output_tokens = [item['token_str'].strip().lower() for item in output]
#         print(template_anchor, "|\t", " ".join(output_tokens))
#         targets_sap.extend(output_tokens)
#     targets_sap = Counter(targets_sap).most_common()
#     print("="*40,"SAP", "="*40 )
#     print(targets_sap)
#     print("="*80)


#     anchors_coordinate = []
#     anchor_types = ['similar', 'coordinate'] #, 'hyponym']
#     anchor_lists = [template_anchors_similar, template_anchors_coordinate]#, template_anchors_hyponym]
#     for template_anchors, anchor_type in zip(anchor_lists, anchor_types):
#         print("-"*80)
#         for template_anchor  in template_anchors: 
#             output = unmasker(template_anchor , top_k=10)
#             output_tokens = [item['token_str'].strip().lower() for item in output]
#             print(template_anchor, "|\t", " ".join(output_tokens))
#             anchors_coordinate.extend(output_tokens)


#     anchors_hyponym = []
#     for template_anchors, anchor_type in zip([template_anchors_hyponym], ['hyponym']):
#         print("-"*80)
#         for template_anchor  in template_anchors: 
#             output = unmasker(template_anchor , top_k=10)
#             output_tokens = [item['token_str'].strip().lower() for item in output]
#             print(template_anchor, "|\t", " ".join(output_tokens))
#             anchors_hyponym .extend(output_tokens)


#     # dap_hyponym_hypernym = [
#     #     f"{anchor} and {sub} are <mask>",
#     #     f"{anchor} and {anchor} is <mask> ."

#     #     f"{anchor} such as "
#     #     f"such <mask> as {sub} and  {anchor}.",
#     #     f"such <mask> as {sub}, or  {anchor}.",
#     #     f", {sub}, and other <mask> .",
#     #     f", {sub}, or other <mask> .",
#     #     f"<mask>, including {sub} and ",
#     #     f"<mask>, including {sub}, or ",
#     #     ]
#     # ap_subtype = [

#     # # ]

#     anchors_hyponym = Counter(anchors_hyponym).most_common()
#     anchors_coordinate = Counter(anchors_coordinate).most_common()
#     print("coordinates:", anchors_coordinate)
#     print("hyponyms:", anchors_hyponym)

#     # template_anchor = f", such as cycling and <mask> ."
#     # output = unmasker(template_anchor , top_k=10)
#     # output_tokens = [item['token_str'] for item in output]
#     # print(output_tokens)


#     targets = []

#     for anchor_hyponym, count1 in anchors_hyponym: 
#         if count1<2: continue
#         for anchor_coordinate, count2 in anchors_coordinate: 
#             if count2<2: continue
#     # for (anchor, count) in anchors:
#         # if count<2: continue  
#         # template_dap_tmp = f"activities such as cycling and {anchor} is often <mask> ."
#             # print(anchor_coordinate, anchor_hyponym)
#             # template_dap_tmp = f"cycling or {anchor_coordinate} is a <mask> {anchor_hyponym}."
#             dap_list = [f"{sub} or {anchor_coordinate} is a <mask> {anchor_hyponym}.", 
#                 f"{sub} or {anchor_coordinate} is a very <mask> {anchor_hyponym}.", 
#                 f"{sub} or {anchor_coordinate} can be a very <mask> {anchor_hyponym}.", 
#                 f"{sub} or {anchor_coordinate} are very <mask> {anchor_hyponym}." ]

#             for template in dap_list :
#                 template_dap_tmp = template 
#                 # print(template_dap_tmp )
#                 output_tmp = unmasker(template_dap_tmp  , top_k=10)
#                 output_tokens_tmp = [item['token_str'].strip().lower() for item in output_tmp]
#                 # print("{}\t{}".format(anchor, " ".join(output_tokens_tmp[:10])))
#                 targets.extend(output_tokens_tmp)

#     targets = Counter(targets).most_common()
#     print("="*40,"DAP", "="*40 )
#     print(targets)
#     print("="*80)

# # print("-"*80)
# # for anchor in output_tokens:
# #     template_dap_tmp = f"{anchor} and cycling is <mask> ."
# #     output_tmp = unmasker(template_dap_tmp  , top_k=20)
# #     output_tokens_tmp = [item['token_str'] for item in output_tmp]
# #     print("{}\t{}".format(anchor, " ".join(output_tokens_tmp[:10])))


# #SAP for hasproperty

# template_anchor  = ''
# output = unmasker(template_anchor , top_k=10)

# # explorign the reverse relations
# df_hasa = load_data(filepath='data/sap_filter/HasA.jsonl')
# df_partof  = load_data(filepath='data/sap_filter/PartOf.jsonl')

# # df_hasa['sub_label'], df_hasa['obj']
# display( df_hasa.query("sub_label == 'animals'") )
# display( df_partof.query("obj_label  == 'animal'") )
# x= 'animals'
# # template_anchor = f"Usually, we would expect {x} to have [MASK] ."
# template_anchor = f"Usually, {x} have [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# display(df.columns)
# # dfr = dfs.query("relation == 'HasProperty'")
# dfr = dfs.copy()
# dfsap_error = dfr.query("p1_mask_sap==0 and p1_mask_dap==0 ")
# # dfsap_only = dfr.query("p1_mask_sap==1 and p3_mask_dap==0")
# dfdap_only = dfr.query("p1_mask_sap==0 and p1_mask_dap==1")
# dfdap_error = dfr.query("p1_mask_sap==0 and p1_mask_dap==0 ")
# # print(len(dfsap_only.index))
# # print(len(dfdap_only.index))
# print(len(dfsap_error.index))

# # dfsap_only[['sub_label',  'masked_sentences','obj_label', 'masked_sentences_with_subj_anchor',   'obj_mask_dap', 'subj_anchors']] #'obj_mask_sap',
# for rel in ['CapableOf']: #['MadeOf', 'IsA', 'CauseDesire', 'UsedFor',  'MotivatedByGoal', 'PartOf', 'AtLocation']:
#     dfsap_error.query(f"relation == '{rel}'")[['sub_label', 'masked_sap', 'obj_label', 'obj_mask_sap']].sort_values('obj_mask_sap').sample(10)
#     # to_csv("log/error_analysis_SAP_HasProperty_sample100.csv") #'obj_mask_sap',
# # dfsap_error[['sub_label', 'masked_sap', 'obj_label', 'obj_mask_sap']].sort_values('obj_mask_sap').to_csv("log/error_analysis_SAP_HasProperty.csv") #'obj_mask_sap',
# # Counter([" ".join(x[:5]) for x in dfsap_error['obj_mask_sap']]).most_common()

# for rel in ['CausesDesire','MadeOf', 'IsA',  'UsedFor',  'MotivatedByGoal', 'PartOf', 'AtLocation']:
#     out = dfdap_error.query(f"relation == '{rel}'")[['sub_label',  'masked_sentences', 'masked_sentences_with_subj_anchor', 'obj_label', 'obj_mask_sap', 'obj_mask_dap', 'subj_anchors', 'relation']]
#     display(out)

# # dfs.query("relation == 'HasProperty'")[['sub_label', 'obj_label', 'masked_anchor_prompts', 'subj_anchors', 'obj_mask_sap']].sample(50)
# dfs.query("relation == 'HasProperty'")[['sub_label', 'obj_label', 'subj_anchors', 'obj_mask_sap', 'obj_mask_dap']].sample(50)

"""## Test Sample"""

# template_anchor = "Such as bears and woofs have <mask> ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# for anchor in output_tokens:
#     cur_prompt = f"Such as bears and woofs have {anchor}, which is <mask> ."
#     cur_output = unmasker(cur_prompt , top_k=20)
#     cur_output_tokens = [item['token_str'] for item in cur_output]
#     print(cur_prompt)
#     print(cur_output_tokens)
#     print("-"*80)

# df_property = load_data('data/sap_filter/HasProperty.jsonl')
# # df_apple = df_property.query("sub_label =='apple' ") 
# # df_property 
# # Counter(df_apple['obj_label'])
# df_property.head()



# template_anchor = f"cycling is <mask> ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# template_anchor = f"cycling and biking is <mask> ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# template_anchor = f"cycling and biking can be <mask> ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# Counter(df_property['obj_label']).most_common()

# df_property.to_excel('log/lama_hasproperty.xlsx')

# template_anchor = "Usually, we would expect animals to have [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)


# template_anchor = "Usually, we would expect animals or plants to have [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# template_anchor = f"Such as apple and [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens_anchors_1 = [item['token_str'] for item in output]
# print(output_tokens_anchors )



# template_anchor = f"An apple is a type of [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens_anchors = [item['token_str'] for item in output]
# print(output_tokens_anchors )

# template_anchor = f"An apple can be [MASK] ."
# output = unmasker(template_anchor , top_k=20)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)


# # dap 
# df_res_tmp = []
# for anchor in output_tokens_anchors : 
#     template_anchor = f"An apple is a type of {anchor} can be [MASK] ."
#     output = unmasker(template_anchor , top_k=20)
#     output_tokens = [item['token_str'] for item in output]
#     print(template_anchor)
#     print(output_tokens)
#     print("-"*80)

# #   df['masked_anchor_prompts'] = df['sub_label'].apply(lambda x: f"{x} and [MASK] share similar property.")
# # x = 'apple'
# # x = 'circle'
# # x = 'donuts'
# x = 'lemon'
# pd.set_option('display.max_columns', 300)
# pd.set_option('display.max_rows', 300)
# pd.set_option('display.max_colwidth', 100)

# # template_anchor = f"{x} is typically [MASK] ."
# # template_anchor = f"the {x} was very [MASK] ."
# template_anchor = f"the {x} is [MASK] ."
# df_test  = []
# output = unmasker(template_anchor , top_k=20)
# print(output)
# output_tokens = [item['token_str'] for item in output]
# print(output_tokens)

# for z in output_tokens:
#     dap_prompt = f"{x} was {z} and [MASK] ." 
#     output_sinlge = unmasker(dap_prompt , top_k=10)
#     # output_single_tokens = [item['token_str'] for item in output_sinlge ]
#     # print(output_single_tokens)
#     for item in output_sinlge:
#         df_test.append({"sequence":item['sequence'], "token_str": item['token_str']})

# df_test = pd.DataFrame(df_test)
# print(Counter(df_test['token_str'].to_list()))
# # display(df_test.head(200))
# display(df_test.query("token_str == 'sour'"))

"""### SAP correct but DAP wrong"""

# # df.query("p5==1 and p5_withanchor==0")[['masked_sentences','sub_label', 'subj_anchors','masked_sentences_with_subj_anchor', 'obj_label', 'obj_mask_dap', 'obj_mask_incontext']]
# df.query("p@5==1 and p5_withanchor==0").to_csv('./drive/MyDrive/DAP/SAP_better.xlsx')
# # .iloc[0:50, :]

"""### SAP wrong but DAP correct"""

# # df.query("p5==0 and p5_withanchor==1")[['sub_label','masked_sentences','masked_sentences_with_subj_anchor', 'obj_label', 'obj_mask_dap', 'obj_mask_incontext']].to_csv("DAP_better.csv")
# df.query("p5==0 and p5_withanchor==1")[['sub_label','masked_sentences','obj_mask_incontext','obj_label',  'masked_sentences_with_subj_anchor', 'obj_mask_dap']].iloc[:50, :]
# # df.query("p5==0 and p5_withanchor==1").to_csv("./drive/MyDrive/DAP/DAP_better.xlsx")

# df[['subj', 'top1_subj_anchor', 'obj_mask_dap', 'obj_label']].head()
# df[['masked_sentences', 'masked_sentences_with_subj_anchor', 'obj_label', 'obj_mask_dap', 'obj_mask_incontext']].sample(100)
# df.to_csv("DAP.dev.csv")