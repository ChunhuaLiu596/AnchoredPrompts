debug: False
---------------------------------------- config ----------------------------------------
{'debug': False, 'test_relations': ['IsA'], 'save_all_data': True, 'top_k': 10, 'data_dir': 'data/hypernymsuite/SHWARTZ/', 'mask_string_mapping': {'bert-large-uncased': '[MASK]'}, 'use_dap_global': True, 'max_anchor_num': 3, 'return_probs': True, 'anchor_scorer_list': ['probAvg'], 'scorer_target_1_prompt': 'probSum', 'scorer_target_N_prompts': 'probAvg', 'use_original_prompt': True, 'original_prompt_source': 'masked_sentence', 'incorporate_operations': ['concate_or_single'], 'filter_anchors_flag': True, 'filter_objects_flag': False, 'cpt_only': False, 'max_anchor_num_list': [5], 'anchor_types': ['Coordinate_remove_Y_PUNC_FULL'], 'add_cpt_score': True, 'constrain_targets': False, 'add_wordnet_path_score': False}
---------------------------------------- config ----------------------------------------
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
---------------------------------------- bert-large-uncased-anchor_type: Coordinate_remove_Y_PUNC_FULL-scorer_anchor:probAvg-op:concate_or_single ----------------------------------------
{
    "IsA": {
        "relation": "IsA",
        "def_sap": [
            "A [X] is a [MASK].",
            "A [X] is an [MASK].",
            "An [X] is a [MASK].",
            "An [X] is an [MASK]."
        ],
        "def_dap": [
            "A [X] or [Z] is a [MASK].",
            "A [X] or [Z] is an [MASK].",
            "An [X] or [Z] is a [MASK].",
            "An [X] or [Z] is an [MASK]."
        ],
        "lsp_sap": [
            "[MASK] such as [X].",
            "[MASK], including [X].",
            "[MASK], especially [X].",
            "[X] or other [MASK].",
            "[X] and other [MASK].",
            "such [MASK] as [X]."
        ],
        "lsp_dap": [
            "[MASK] such as [X] and [Z].",
            "[MASK], including [X] and [Z].",
            "[MASK], especially [X] and [Z].",
            "[X], [Z] and other [MASK].",
            "[X], [Z] or other [MASK].",
            "such [MASK] as [X] and [Z]."
        ],
        "use_dap": true,
        "anchor_target": "[X]",
        "anchor_lsp_sap": [
            "such as [X] and [MASK].",
            "such as [X] or [MASK].",
            "such as [X], [MASK],",
            "including [X] and [MASK].",
            "including [X] or [MASK].",
            "including [X], [MASK],",
            "especially [X] and [MASK].",
            "especially [X] or [MASK].",
            "especially [X], [MASK],",
            "[X], [MASK] or other",
            "[X], [MASK] and other"
        ]
    }
}
Processing IsA ... data/hypernymsuite/SHWARTZ//IsA.jsonl
#Test_instances: 13104
	 step1: get anchors (fill and filter)
/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
--- def_sap
--- lsp_sap
/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
Traceback (most recent call last):
  File "anchored_prompts.py", line 1148, in <module>
    df = filter_anchors_with_probs(df, outputs,
  File "anchored_prompts.py", line 505, in filter_anchors_with_probs
    df[['subj_anchors', 'subj_anchors_score']] = filter_outputs_with_probs(df.sub_label.to_list(), 
  File "anchored_prompts.py", line 461, in filter_outputs_with_probs
    token2cpt[filled_token].append(concpet_positioning_test(stimulus_word=input_word,
  File "/data/gpfs/projects/punim0478/chunhua/cogsci/DAP/utils_concept_positioning_test.py", line 34, in concpet_positioning_test
    stimulus_position = sequence_list.index(stimulus_word)
ValueError: 'respect.' is not in list
