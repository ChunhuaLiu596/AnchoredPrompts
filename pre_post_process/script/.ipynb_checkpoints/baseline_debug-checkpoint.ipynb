{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "import copy\n",
    "import re \n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from copy import deepcopy\n",
    "import pathlib\n",
    "\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import string\n",
    "from inflection import pluralize, singularize\n",
    "from util_wordnet import get_sister_terms\n",
    "from transformers import pipeline\n",
    "\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "STOP_WORDS = en.Defaults.stop_words\n",
    "\n",
    "from IPython.display import display\n",
    "from df_to_latex import DataFrame2Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_article(word):\n",
    "    if word[0] in ['a', 'e', 'i', 'o', 'u']:\n",
    "        return 'an'\n",
    "    return 'a'\n",
    "\n",
    "\n",
    "def save_dict_to_json(examples, output_path):\n",
    "    ''' \n",
    "    save a list of dicts into otuput_path, orient='records' (each line is a dict) \n",
    "    examples: a list of dicts\n",
    "    output_path: \n",
    "    '''\n",
    "\n",
    "    with open(output_path, 'w') as fout:\n",
    "        for example in examples:\n",
    "            json.dump(example, fout)\n",
    "            fout.write(\"\\n\")\n",
    "        print(f\"save {output_path} with {len(examples)} lines\")\n",
    "\n",
    "def add_period_at_the_end_of_sentence(sentence):\n",
    "    last_token = sentence[-1]\n",
    "    if last_token != '.': \n",
    "        return sentence + '.'\n",
    "    return [sentence]\n",
    "\n",
    "def get_unmasker(model, device, targets=None):\n",
    "    if targets is None: \n",
    "        unmasker = pipeline('fill-mask', model=model)# 'bert-large-uncased') #initialize the masker\n",
    "    else:\n",
    "        unmasker = pipeline('fill-mask', model=model, targets=targets )# 'bert-large-uncased') #initialize the masker\n",
    "    return unmasker\n",
    "\n",
    "\n",
    "\n",
    "def remove_noisy_test_data(df):\n",
    "  ''' \n",
    "  relation=\"hasproperty\"\n",
    "  why? some data points don't belong to this relation types \n",
    "  case1., sub_label=number, such as \"10 is ten.\"  We don't say ten is the property of 10\n",
    "  case2, sub_label = 'person_name' and obj_label = 'nuts;, such as \"\"Andrew is [MASK].\", [MASK]=nuts\n",
    "  '''\n",
    "  sub_labels_to_exclude = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '30', '5', '50', '60', '7', '70', '70s', '80', '9', '90']\n",
    "  obj_labels_to_exclude  = ['nuts']\n",
    "  df = df.query(f\"sub_label not in {sub_labels_to_exclude}\")\n",
    "  df = df.query(f\"sub_label not in {obj_labels_to_exclude}\")\n",
    "  return  df.reset_index(drop=True)\n",
    "\n",
    "def locate_sub_obj_position(ent, sentence, index_not_in) :\n",
    "  ''' \n",
    "  function: find the index of ent in a sentence, the result will be used to filter instances whose ent cannot be find at their sentences\n",
    "  args: \n",
    "    sentence: the sentnces to mask, could be the string or a list of tokens \n",
    "    ent: the ent to be found (sub_label) \n",
    "    index_not_in: the default index for failed instances (an ent not in a sentence)\n",
    "  ''' \n",
    "\n",
    "  if isinstance(sentence, list):\n",
    "    if ent not in sentence:\n",
    "      return index_not_in\n",
    "    return sentence.index(ent)  \n",
    "  else:\n",
    "    sentence = copy.deepcopy(sentence).lower()\n",
    "    if isinstance(sentence, str):\n",
    "      try:\n",
    "        index = sentence.index(ent)\n",
    "        return  index \n",
    "      except: \n",
    "        print(f\"NOT FOUND sub_label: {ent} -> in sentence: {sentence}\")\n",
    "        return index_not_in\n",
    "      \n",
    "        print(ent, sentence)\n",
    "        return index_not_in\n",
    "\n",
    "def load_data(filepath, clean_test=True, tokenize=False):\n",
    "  '''\n",
    "  return the cleaned data\n",
    "  args:\n",
    "    tokenize: if True: the maksed_sentences will be tokenzied (this is slwoers); \n",
    "            otherwise, we use the string match to filter the failed sentences\n",
    "    clean_test: default is True. We filter out some noisy samples spoted by huamns \n",
    "               Note that this is relation specific \n",
    "\n",
    "  '''\n",
    "  index_not_in = 10000\n",
    "\n",
    "  with open(filepath, 'r', encoding='utf-8') as fin:\n",
    "    data = fin.readlines()\n",
    "    data = [eval(x) for x in data]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df['obj_label'] = df['obj_label'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "  if tokenize:\n",
    "    df['masked_sentence_tokens'] = df['masked_sentences'].apply(lambda x: tokenize_sentence(x[0]))\n",
    "    df['sub_position'] = df[['sub_label', 'masked_sentence_tokens']].apply(lambda x: locate_sub_obj_position(x[0], x[1], index_not_in=index_not_in), axis=1)\n",
    "\n",
    "  if clean_test: \n",
    "    df = remove_noisy_test_data(df)\n",
    "    df['sub_position'] = df[['sub_label', 'masked_sentences']].apply(lambda x: locate_sub_obj_position(x[0], x[1][0], index_not_in), axis=1)\n",
    "    df = df.query(f\"sub_position !={index_not_in}\") #.reset_index() #cue can not be matched in the sentence\n",
    "\n",
    "  print(f\"#Test_instances: {len(df.index)}\")\n",
    "  return df.reset_index(drop=True)\n",
    "\n",
    "def get_unmasker(model, targets=None):\n",
    "    if targets is None: \n",
    "        unmasker = pipeline('fill-mask', model=model)# 'bert-large-uncased') #initialize the masker\n",
    "    else:\n",
    "        unmasker = pipeline('fill-mask', model=model, targets=targets )# 'bert-large-uncased') #initialize the masker\n",
    "    return unmasker\n",
    "\n",
    "\n",
    "def get_highest_mrr_among_labels(label, pred):\n",
    "    '''\n",
    "    return the highest rank among the multiple labels. This is applicable to single labels as well, if we the single label is put in a list\n",
    "\n",
    "    pred: a list of words (candidates)\n",
    "    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)\n",
    "    '''\n",
    "    mrr = 0 \n",
    "    if pred is None: return mrr \n",
    "\n",
    "    rank_list = [ pred.index(item) + 1 for item in label if item in pred] \n",
    "    if len(rank_list)>0:\n",
    "        mrr = 1/min(rank_list)\n",
    "    return mrr \n",
    "\n",
    "\n",
    "def get_predictions(input_words, outputs, filter_objects_flag=True, filter_objects_with_input=True):\n",
    "    '''\n",
    "    excluding x from outputs\n",
    "    '''\n",
    "    filled_tokens = list()\n",
    "    filled_scores = defaultdict()\n",
    "    for i, output in enumerate(outputs):\n",
    "#         print(output)\n",
    "        filled_token = output['token_str'].strip().lower()\n",
    "        filled_score = output['score']\n",
    "        if filter_objects_flag:\n",
    "            \n",
    "            #####Add conditions to filter unwanted ################\n",
    "            # filter the repetation of a concept in the explanation. See the the following example\n",
    "            # [MASK] is the capability to do a particular job . -> capacity \n",
    "            if not filled_token.isalpha(): continue\n",
    "            if filled_token in STOP_WORDS: continue \n",
    "            if len(filled_token)<=1: continue \n",
    "            if filter_objects_with_input:\n",
    "                if filled_token in [input_words]: continue\n",
    "                # [re.sub(\"\\s+\", '', x) for x in input_word.split()]: continue #filter out the target in input  \n",
    "            if filled_token.startswith(\"#\"): continue\n",
    "            #####Add conditions to filter unwanted ################\n",
    "\n",
    "            filled_tokens.append(filled_token)\n",
    "            filled_scores[filled_token] = filled_score\n",
    "        else:\n",
    "            filled_tokens.append(filled_token)\n",
    "            filled_scores[filled_token] = filled_score\n",
    "    \n",
    "    return pd.Series((filled_tokens, filled_scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_to_jsonl_path={\n",
    "    \"EVAL\": \"../data/hypernymysuite/data/hypernymsuite/EVAL/IsA.jsonl\",\n",
    "    \"BLESS\": \"../data/hypernymysuite/data/hypernymsuite/BLESS/IsA.jsonl\",\n",
    "    \"LEDS\": \"../data/hypernymysuite/data/hypernymsuite/LEDS/IsA.jsonl\",\n",
    "    \"LMDIAG\": \"../data/probe-generalization/Syntagmatic/LM-Diagnostic-Extended/singular/IsA.jsonl\",\n",
    "    \"CLSB\": \"../data/CLSB/single_label/IsA.jsonl\",\n",
    "    \"SHWARTZ\": \"../data/hypernymysuite/data/hypernymsuite/SHWARTZ/IsA.jsonl\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_table(df, dataset_list =['BLESS','LMDIAG', 'CLSB', 'SHWARTZ', 'EVAL', 'LEDS']):\n",
    "    '''\n",
    "    format the output with desired dataset layout and metrics \n",
    "    '''\n",
    "    df_groups = []\n",
    "    for dataset in dataset_list: \n",
    "       \n",
    "        df_group = df.query(f\"dataset == '{dataset}'\")\n",
    "        df_group = df_group.pivot(index=\"pattern_id\", columns=['dataset'], values=['MRR', 'P@K'])\n",
    "        df_group = df_group.swaplevel(0, 1, axis=1)\n",
    "        df_groups.append(df_group)\n",
    "\n",
    "    df_groups = pd.concat(df_groups, axis=1)\n",
    "    return df_groups\n",
    "\n",
    "def merge_predictions_in_concept_level(uniform_funcion, words, top_k=None ):\n",
    "    '''\n",
    "    uniform_function: either signualarize or pluralize \n",
    "    '''\n",
    "    words_uniformed = [uniform_funcion(word) for word in words]\n",
    "    concepts = list(OrderedDict.fromkeys(words_uniformed))\n",
    "    return concepts[:top_k] if top_k is not None else concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitional Patterns (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e7bed5044214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                                       axis=1)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'pred_{idx}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'pred_{idx}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerge_predictions_in_concept_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniform_funcion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label_sg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e7bed5044214>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                                       axis=1)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'pred_{idx}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'pred_{idx}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerge_predictions_in_concept_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniform_funcion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label_sg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2832bdfdc008>\u001b[0m in \u001b[0;36mmerge_predictions_in_concept_level\u001b[0;34m(uniform_funcion, words, top_k)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0muniform_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meither\u001b[0m \u001b[0msignualarize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpluralize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     '''\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mwords_uniformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muniform_funcion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mconcepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_uniformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2832bdfdc008>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0muniform_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meither\u001b[0m \u001b[0msignualarize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpluralize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     '''\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mwords_uniformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muniform_funcion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mconcepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_uniformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gpfs/projects/punim0478/chunhua/cogsci/DAP/pre_post_process/script/inflection.py\u001b[0m in \u001b[0;36msingularize\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     '''\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mset_dropout_rate\u001b[0;34m(model, drop, attrs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mset_dropout_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_ModelT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dropout_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_ModelT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \"\"\"Walk over the model's nodes, setting the dropout rate. You can specify\n\u001b[1;32m    761\u001b[0m     \u001b[0mone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mit\u001b[0m \u001b[0mlooks\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"dropout_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def_sap_id_to_patterns = {\n",
    "         \"1\": \"[X] is a [Y].\", \n",
    "         \"2\": \"[X] is a type of [Y].\", \n",
    "         \"3\": \"[X] is a kind [Y].\", \n",
    "        }\n",
    "\n",
    "unmasker = unmasker = pipeline('fill-mask', model= 'bert-large-uncased', device=0)\n",
    "top_k=10\n",
    "batch_size = 100 \n",
    "df_res_def = []\n",
    "debug =  False #True \n",
    "# debug =  True\n",
    "\n",
    "for dataset, filepath in dataset_to_jsonl_path.items():\n",
    "    if dataset !=\"LMDIAG\": continue \n",
    "        \n",
    "    df = load_data(filepath)\n",
    "    \n",
    "    for idx, pattern in def_sap_id_to_patterns.items():\n",
    "        df[f'masked_sentences_{idx}'] = df['sub_label'].apply(lambda x: [pattern.replace(\"[Y]\", \"[MASK]\").replace(\"[X]\", f\"{_get_article(x)} {x}\")])\n",
    "    \n",
    "    if debug: \n",
    "        df = df.head(5)\n",
    "#         display(df.head())    \n",
    "    for idx in range(1, len(def_sap_id_to_patterns.keys())+1 ):\n",
    "        df[f'outputs_{idx}']  = unmasker(df[f'masked_sentences_{idx}'].to_list(), top_k= 2*top_k, batch_size=batch_size)\n",
    "        df[[f'pred_{idx}', f'pred_{idx}_score']] = df[['sub_label',f'outputs_{idx}']].apply(lambda x: get_predictions(input_words=x[0], outputs=x[1], \n",
    "                                                                                           filter_objects_flag=True, \n",
    "                                                                                           filter_objects_with_input=True), \n",
    "                                                                                      axis=1)\n",
    "        \n",
    "        df[f'pred_{idx}']= df[f'pred_{idx}'].apply(lambda x: merge_predictions_in_concept_level(uniform_funcion=singularize, words=x, top_k=top_k))\n",
    "        df['obj_label_sg'] = df['obj_label'].apply(lambda x: [singularize(x[0])])\n",
    "        \n",
    "        df[f'p@{top_k}_{idx}'] = df[['obj_label_sg', f'pred_{idx}']].apply(lambda x: 1 if x[0][0] in x[1]  else 0, axis=1)\n",
    "        df[f'mrr@{top_k}_{idx}'] = df[['obj_label_sg', f'pred_{idx}']].apply(lambda x: get_highest_mrr_among_labels(x[0], x[1]), axis=1)\n",
    "        \n",
    "        p_at_k = df[f'p@{top_k}_{idx}'].sum()/len(df.index)\n",
    "        mrr = df[f'mrr@{top_k}_{idx}'].sum()/len(df.index)\n",
    "        df_res_def.append({\"dataset\": dataset, \"pattern_id\": idx, \"P@K\": round(p_at_k, 3)*100, 'MRR': round(mrr,3)*100 })\n",
    "        \n",
    "df_res_def = pd.DataFrame(df_res_def)\n",
    "df_res_def_pivot = layout_table(df_res_def, dataset_list =['BLESS','LMDIAG', 'CLSB',  'EVAL', 'LEDS']) #'SHWARTZ',\n",
    "display(df_res_def_pivot)\n",
    "\n",
    "DataFrame2Latex(df= df_res_def_pivot , label=f'tab:def_single_pattern_ablation', \n",
    "            caption=f'Experimental results on definitional single patterns.', \n",
    "            output_file= None , #'../log/paper_results/latex.test.tex',\n",
    "            adjustbox_width = 'textwidth',\n",
    "            precision = 1,\n",
    "            column_format='l|ll|ll|ll|ll|ll|ll',\n",
    "            multicolumn_format='c|'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexico-Synatactic Patterns (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 957\n",
      "#Test_instances: 1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Test_instances: 12994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/chunhua/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"2\" halign=\"left\">BLESS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CLSB</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LMDIAG</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SHWARTZ</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LEDS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">EVAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "      <th>MRR</th>\n",
       "      <th>P@K</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pattern_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.3</td>\n",
       "      <td>37.4</td>\n",
       "      <td>33.6</td>\n",
       "      <td>57.5</td>\n",
       "      <td>33.9</td>\n",
       "      <td>51.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>24.9</td>\n",
       "      <td>47.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>32.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>26.7</td>\n",
       "      <td>49.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>38.1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>46.9</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>37.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.3</td>\n",
       "      <td>31.7</td>\n",
       "      <td>33.2</td>\n",
       "      <td>58.1</td>\n",
       "      <td>35.8</td>\n",
       "      <td>54.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>19.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.9</td>\n",
       "      <td>16.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>32.6</td>\n",
       "      <td>56.9</td>\n",
       "      <td>33.8</td>\n",
       "      <td>51.4</td>\n",
       "      <td>8.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>26.2</td>\n",
       "      <td>46.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.3</td>\n",
       "      <td>35.5</td>\n",
       "      <td>19.7</td>\n",
       "      <td>47.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>40.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>32.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset    BLESS        CLSB       LMDIAG       SHWARTZ        LEDS        \\\n",
       "             MRR   P@K   MRR   P@K    MRR   P@K     MRR   P@K   MRR   P@K   \n",
       "pattern_id                                                                  \n",
       "1           18.3  37.4  33.6  57.5   33.9  51.0    13.4  26.9  24.9  47.0   \n",
       "2           12.8  22.8  26.7  49.0   31.0  46.2     3.6   7.6  19.5  38.1   \n",
       "3           11.8  20.4  25.2  46.9   28.3  42.5     2.5   6.0  19.7  37.2   \n",
       "4           16.3  31.7  33.2  58.1   35.8  54.2     8.8  19.2  28.0  48.9   \n",
       "5           17.7  34.9  32.6  56.9   33.8  51.4     8.1  17.9  26.2  46.1   \n",
       "6           12.3  35.5  19.7  47.6   16.6  40.8     8.8  20.8  11.6  32.5   \n",
       "\n",
       "dataset     EVAL        \n",
       "             MRR   P@K  \n",
       "pattern_id              \n",
       "1           14.4  32.7  \n",
       "2           10.5  22.2  \n",
       "3            9.3  21.1  \n",
       "4           16.3  33.3  \n",
       "5           14.6  31.3  \n",
       "6            9.0  27.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "17\n",
      "\\begin{table*}[!h]\n",
      "\\centering\n",
      "\\begin{adjustbox}{width=\\textwidth}\n",
      "\\label{tab:lsp_single_pattern_ablation}\n",
      "\\begin{tabular}{l|ll|ll|ll|ll|ll|ll}\n",
      "\\toprule\n",
      "dataset & \\multicolumn{2}{c|}{BLESS} & \\multicolumn{2}{c|}{CLSB} & \\multicolumn{2}{c|}{LMDIAG} & \\multicolumn{2}{c|}{SHWARTZ} & \\multicolumn{2}{c|}{LEDS} & \\multicolumn{2}{c|}{EVAL} \\\\\n",
      " & MRR & P@K & MRR & P@K & MRR & P@K & MRR & P@K & MRR & P@K & MRR & P@K \\\\\n",
      "pattern_id &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "1 & \\textbf{18.3} & \\textbf{37.4} & \\textbf{33.6} & 57.5 & 33.9 & 51.0 & \\textbf{13.4} & \\textbf{26.9} & 24.9 & 47.0 & 14.4 & 32.7 \\\\\n",
      "2 & 12.8 & 22.8 & 26.7 & 49.0 & 31.0 & 46.2 & 3.6 & 7.6 & 19.5 & 38.1 & 10.5 & 22.2 \\\\\n",
      "3 & 11.8 & 20.4 & 25.2 & 46.9 & 28.3 & 42.5 & 2.5 & 6.0 & 19.7 & 37.2 & 9.3 & 21.1 \\\\\n",
      "4 & 16.3 & 31.7 & 33.2 & \\textbf{58.1} & \\textbf{35.8} & \\textbf{54.2} & 8.8 & 19.2 & \\textbf{28.0} & \\textbf{48.9} & \\textbf{16.3} & \\textbf{33.3} \\\\\n",
      "5 & 17.7 & 34.9 & 32.6 & 56.9 & 33.8 & 51.4 & 8.1 & 17.9 & 26.2 & 46.1 & 14.6 & 31.3 \\\\\n",
      "6 & 12.3 & 35.5 & 19.7 & 47.6 & 16.6 & 40.8 & 8.8 & 20.8 & 11.6 & 32.5 & 9.0 & 27.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      " \\end{adjustbox}\n",
      "\\caption{Experimental results on LSP single patterns.}\n",
      "\\end{table*}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<df_to_latex.DataFrame2Latex at 0x2b98bc5eb0a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lsp_sap_id_to_patterns = {\n",
    "         \"1\": \"[Y] such as [X].\", \n",
    "         \"2\": \"[Y], including [X].\", \n",
    "         \"3\": \"[Y], especially [X].\", \n",
    "         \"4\": \"[X] or other [Y].\", \n",
    "         \"5\": \"[X] and other [Y].\", \n",
    "         \"6\": \"such [Y] as [X].\", \n",
    "        }\n",
    "\n",
    "\n",
    "debug =  False #True \n",
    "# debug = True \n",
    "unmasker = unmasker = pipeline('fill-mask', model= 'bert-large-uncased', device=0)\n",
    "top_k=10\n",
    "\n",
    "df_res_lsp = []\n",
    "for dataset, filepath in dataset_to_jsonl_path.items():\n",
    "    df = load_data(filepath)\n",
    "    df['sub_label_pl'] = df['sub_label'].apply(lambda x: pluralize(x))\n",
    "    df['obj_label_pl'] = df['obj_label'].apply(lambda x: [pluralize(x[0])])\n",
    "    for idx, pattern in lsp_sap_id_to_patterns.items():\n",
    "        df[f'masked_sentences_{idx}'] = df['sub_label_pl'].apply(lambda x: [pattern.replace(\"[Y]\", \"[MASK]\").replace(\"[X]\", x)])\n",
    "    \n",
    "    if debug: \n",
    "        df = df.head(5)\n",
    "#         display(df.head(5))\n",
    "    \n",
    "    for idx, pattern in lsp_sap_id_to_patterns.items():\n",
    "        df[f'outputs_{idx}']  = unmasker(df[f'masked_sentences_{idx}'].to_list(), top_k= 2*top_k, batch_size=batch_size)\n",
    "        df[[f'pred_{idx}', f'pred_{idx}_score']] = df[['sub_label_pl',f'outputs_{idx}']].apply(lambda x: get_predictions(input_words=x[0], outputs=x[1], \n",
    "                                                                                           filter_objects_flag=True, \n",
    "                                                                                           filter_objects_with_input=True), \n",
    "                                                                                      axis=1)\n",
    "        \n",
    "        df[f'pred_{idx}']= df[f'pred_{idx}'].apply(lambda x: merge_predictions_in_concept_level(uniform_funcion=pluralize, words=x, top_k=top_k))\n",
    "        df[f'p@{top_k}_{idx}'] = df[['obj_label_pl', f'pred_{idx}']].apply(lambda x: 1 if x[0][0] in x[1] else 0, axis=1)\n",
    "        df[f'mrr@{top_k}_{idx}'] = df[['obj_label_pl', f'pred_{idx}']].apply(lambda x: get_highest_mrr_among_labels(x[0], x[1]), axis=1)\n",
    "        \n",
    "        p_at_k = df[f'p@{top_k}_{idx}'].sum()/len(df.index)\n",
    "        mrr = df[f'mrr@{top_k}_{idx}'].sum()/len(df.index)\n",
    "        df_res_lsp.append({\"dataset\": dataset, \"pattern_id\": idx, \"P@K\": round(p_at_k, 3) *100 , 'MRR': round(mrr, 3)*100})\n",
    "\n",
    "\n",
    "df_res_lsp = pd.DataFrame(df_res_lsp)\n",
    "df_res_lsp_pivot = layout_table(df_res_lsp)\n",
    "display(df_res_lsp_pivot)\n",
    "\n",
    "\n",
    "DataFrame2Latex(df= df_res_lsp_pivot , label=f'tab:lsp_single_pattern_ablation', \n",
    "            caption=f'Experimental results on LSP single patterns.', \n",
    "            output_file= None , #'../log/paper_results/latex.test.tex',\n",
    "            adjustbox_width = 'textwidth',\n",
    "            precision = 1,\n",
    "            column_format='l|ll|ll|ll|ll|ll|ll',\n",
    "            multicolumn_format='c|'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset vs group\n",
    "- we want to examine what's the effect of pattern numbers to the final performance\n",
    "- plot: x is the number of patterns, y is the performance on the subset of the patterns with corresponding number on x \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
