{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This file generates a test set for probing PTLMs\n",
    "The process follow the descriptions from Wier et al., (2020). \n",
    "Title: Wier et al., (2020) Probing Neural Language Models for Human Tacit Assumptions\n",
    "\n",
    "**Description**\n",
    "* property/association: single token\n",
    "* relation: is, is a, has, has a, made of\n",
    "* prompt: \"Everyone knows that a [X] has [MASK] .\" , where [X] is the cue concept, and [MASK] is the properties to be filled by LMs.\n",
    "* output format: \n",
    "  ```\n",
    "  [{\"sub_label\": , \n",
    "    \"obj_label\": , \n",
    "    \"masked_sentences\": [],\n",
    "    \"uuid\": int,\n",
    "    \"relation\": []  #this follows Weir et al., (2020), falling into [is, is a, has, has a ]\n",
    "    \"feature_type\": [] #this is from the original data, including [functional, visual perceptual, other perceptual, encyclopedia]\n",
    "  * }, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chunhua/anaconda3/lib/python3.8/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import json\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm', disable=[ 'parser', 'ner'])\n",
    "from collections import defaultdict \n",
    "from inflection import pluralize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-4d503994744b>:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['feature_type'] = df['feature_type'].apply(lambda x: \"_\".join(x.split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#qualified instances: 13386\n",
      "save ./cloze_form_prompts/clsb2014.jsonl with 13386 lines\n",
      "save ./cloze_form_prompts/clsb2014.jsonl 13386 lines\n",
      "save ./cloze_form_prompts/clsb_ISA.jsonl with 1042 lines\n",
      "save ./cloze_form_prompts/clsb_MADEOF.jsonl with 1496 lines\n",
      "save ./cloze_form_prompts/clsb_IS.jsonl with 6859 lines\n",
      "save ./cloze_form_prompts/clsb_HASA.jsonl with 1422 lines\n",
      "save ./cloze_form_prompts/clsb_HAS.jsonl with 2299 lines\n",
      "save ./cloze_form_prompts/clsb_ISAN.jsonl with 268 lines\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4d503994744b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{save_dir}/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"IsA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0mmerge_multiple_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is a '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is an '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4d503994744b>\u001b[0m in \u001b[0;36mmerge_multiple_labels\u001b[0;34m(df, relations, output_file)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sub_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m#().to_list()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_pattern(list):\n",
    "    pattern = re.compile(\" |\".join(re.escape(word) for word in list), re.IGNORECASE)\n",
    "    return pattern \n",
    "\n",
    "def normalize_feature(text):\n",
    "    '''\n",
    "    identify feature could be multiple tokens\n",
    "    ''' \n",
    "\n",
    "    text =  text.replace(\"_\", \" \")\n",
    "    for words in words_list:\n",
    "        text  = re.sub(pattern = create_pattern(words), \n",
    "                       repl='', \n",
    "                       string=text)\n",
    "    return text.strip() \n",
    "\n",
    "\n",
    "\n",
    "def normalize_concept(text):\n",
    "    '''\n",
    "    remove the bracket like: organ_(musical instrument),\n",
    "    '''\n",
    "    if \"_\" in concept:\n",
    "        return \n",
    "    text =  text.replace(\"_\", \" \")\n",
    "    pattern = re.compile(r' \\(.*?\\)')\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def add_alternative_features(df):\n",
    "   new_triples = []\n",
    "   rows = zip(df['feature_type'], df['concept'], df['feature'], df['feature_alternatives'], df['pf'])\n",
    "   for feature_type, concept, feature, feature_alternative, pf in rows:\n",
    "      row_list = set() \n",
    "      row_list.add((feature_type, concept, feature, pf))\n",
    "      for other_feature in feature_alternative.split(\";\"):\n",
    "         other = (feature_type, concept, other_feature, pf)\n",
    "         row_list.add(other)\n",
    "      new_triples.extend(list(row_list))\n",
    "   \n",
    "   df = pd.DataFrame(new_triples, columns=['feature_type', 'concept', 'feature', 'pf'])\n",
    "   return df \n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"Perform lemmatization and stopword removal in the clean text\n",
    "       Returns a list of lemmas\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemma_list = [str(tok.lemma_).lower() for tok in doc]\n",
    "    return \" \".join(lemma_list)\n",
    "\n",
    "def _get_article(word):\n",
    "    \n",
    "    if word[0] in ['a', 'e', 'i', 'o', 'u']:\n",
    "        return 'an'.capitalize()\n",
    "    return 'a'.capitalize()\n",
    "\n",
    "def mask_sentences(sentence, obj_label,   index_not_in = 10000):\n",
    "    '''\n",
    "    '''\n",
    "    # print(sentence, obj_label)\n",
    "    tokens = sentence.split() \n",
    "    mask_position = tokens.index(obj_label) \n",
    "\n",
    "    tokens[mask_position] = '[MASK]'\n",
    "    # print(tokens)\n",
    "    if mask_position !=0:\n",
    "        article = _get_article(tokens[0])\n",
    "        # tokens[0] = tokens[0].capitalize()\n",
    "        tokens.insert(0, article)\n",
    "    return  [\" \".join(tokens) + '.']\n",
    "\n",
    "\n",
    "def save_dict_to_json(examples, output_path):\n",
    "    ''' \n",
    "    save a list of dicts into otuput_path, orient='records' (each line is a dict) \n",
    "    examples: a list of dicts\n",
    "    output_path: \n",
    "    '''\n",
    "\n",
    "    with open(output_path, 'w') as fout:\n",
    "        for example in examples:\n",
    "            json.dump(example, fout)\n",
    "            fout.write(\"\\n\")\n",
    "        print(f\"save {output_path} with {len(examples)} lines\")\n",
    "\n",
    "def extract_feature(text, unqualified_feature='unqualified', triggers = ['is a ', 'made of ', 'is ', 'has a ', 'has ']):\n",
    "    '''\n",
    "    Returns:\n",
    "        feature: the single-token property\n",
    "        relation: the relation to test \n",
    "    Params:\n",
    "    text: the feature produced by humans\n",
    "    positive example: made of plastic -> plastic \n",
    "    negative example: \n",
    "        is an insignia for an organisation\n",
    "    '''\n",
    "    \n",
    "    for trigger in triggers:\n",
    "        if text.startswith(trigger):\n",
    "            start_index = text.index(trigger) + len(trigger) #the start index of a feature \n",
    "            feature = text[start_index:] #.split(\"-\")\n",
    "            # continue \n",
    "            feature =  feature.replace(\"_\", \" \").replace(\"-\", \" \").split()\n",
    "            if len(feature)==1:\n",
    "                return pd.Series((feature[0], trigger))\n",
    "    return pd.Series((unqualified_feature, 'UNK'))\n",
    "\n",
    "def add_plural(word):\n",
    "    word_plural = pluralize(word)\n",
    "    return [word, word_plural] if word_plural!=word else [word]\n",
    "\n",
    "mapping_relation_filename= {}\n",
    "\n",
    "def read_clsb(input_file, save_dir, output_file, add_plural_ground_truth=False):\n",
    "    df = pd.read_excel(input_file, engine='openpyxl')\n",
    "    \n",
    "    df['concept_token_len'] = df['concept'].apply(lambda x: len(x.split(\"_\"))) # discard bat_(animal), bat_(sport)\n",
    "    df = df.query(\"concept_token_len==1\")\n",
    "    df['feature_type'] = df['feature_type'].apply(lambda x: \"_\".join(x.split()))\n",
    "    df =  add_alternative_features(df)\n",
    "    df['sentence'] = df[['concept', 'feature']].apply(lambda x: \" \".join([x[0], x[1]]) , axis=1 ) #construct sentence before applying normalization \n",
    "\n",
    "    unqualified_feature = 'unqualified'\n",
    "    triggers = ['is a ', 'made of ', 'is ', 'has a ', 'has ', 'is an ']\n",
    "    df[['feature', 'relation']] = df['feature'].apply(lambda x: extract_feature(x, unqualified_feature=unqualified_feature, triggers = triggers  ))\n",
    "    # df['feature'] = df['feature'].progress_apply(lambda x: normalize_feature(x))\n",
    "    # df['feature_token_num'] = df['feature'].apply(lambda x: len(x.split()))\n",
    "    # df = df.query(\"feature_token_num ==1\").reset_index()\n",
    "    df = df.query(f\"feature!='{unqualified_feature}'\").reset_index(drop=True)\n",
    "    print(f\"#qualified instances: {len(df.index)}\")\n",
    "\n",
    "    index_not_in = 1000\n",
    "    # display(df.query(\"concept == ''\"))\n",
    "    df['masked_sentences'] = df[['sentence', 'feature']].apply(lambda x: mask_sentences(*x) , axis=1 )\n",
    "    # df = df.query(f\"index_not_in!='{index_not_in}'\").reset_index(drop=True)\n",
    "\n",
    "    df_new = pd.DataFrame(columns = ['sub_label', 'obj_label', 'masked_sentences', 'relation', 'weight'])\n",
    "    df_new['sub_label'] = df['concept']\n",
    "    df_new['obj_label'] = df['feature']\n",
    "    df_new.drop_duplicates().reset_index()\n",
    "    if  add_plural_ground_truth:\n",
    "        df_new['obj_label'] = df_new['obj_label'].apply(lambda x: add_plural(x)) \n",
    "    else:\n",
    "        df_new['obj_label'] = df_new['obj_label'].apply(lambda x: [x]) \n",
    "\n",
    "    df_new['masked_sentences'] = df['masked_sentences']\n",
    "    df_new['relation'] = df['relation'] #.apply(lambda x: x.replace(\" \", \"\").strip())\n",
    "    df_new['feature_type'] = df['feature_type']\n",
    "    df_new['weight'] = df['pf']\n",
    "\n",
    "    # df_new['head_lemma'] = df_new['head'].progress_apply(lambda x: lemmatize(x))\n",
    "    # df_new['tail_lemma'] = df_new['tail'].progress_apply(lambda x: lemmatize(x))\n",
    "\n",
    "    # df_new.to_csv(output_file,index=False)\n",
    "    save_dict_to_json(examples= df_new.to_dict(orient='records'), output_path =output_file )\n",
    "    print(f'save {output_file} {len(df_new.index)} lines')\n",
    "\n",
    "    for relation in triggers:\n",
    "        df_query = df_new.query(f\"relation == '{relation}'\")\n",
    "        output_file = f\"{save_dir}/clsb_\" + \"\".join(relation).upper().replace(\" \", \"\").strip() + '.jsonl'\n",
    "        save_dict_to_json(examples= df_query.to_dict(orient='records'), output_path =output_file )\n",
    "    # df_new.head()\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# if __name__=='__main__':\n",
    "# def main():\n",
    "path = 'CLSB2014.xlsx'\n",
    "save_dir = './cloze_form_prompts'\n",
    "output_file = f'{save_dir}/clsb2014.jsonl'\n",
    "df = read_clsb(path, save_dir, output_file, add_plural_ground_truth=True)\n",
    "\n",
    "# extract_relation_groups(df, relations=['is a', 'is an'], relation_group_name='IsA', output_file= f'{save_dir}/hypernym/IsA.jsonl')\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the hypernym from CLSB (IsA + IsAn) with multiple hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save ./cloze_form_prompts/IsA.jsonl with 522 lines\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_label</th>\n",
       "      <th>obj_label</th>\n",
       "      <th>masked_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>[vehicle]</td>\n",
       "      <td>[An aeroplane is a [MASK]., An aeroplane is an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alligator</td>\n",
       "      <td>[reptile, animal, carnivore, predator, amphibian]</td>\n",
       "      <td>[An alligator is a [MASK]., An alligator is an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ambulance</td>\n",
       "      <td>[vehicle, van]</td>\n",
       "      <td>[An ambulance is a [MASK]., An ambulance is an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anchor</td>\n",
       "      <td>[symbol]</td>\n",
       "      <td>[An anchor is a [MASK]., An anchor is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ant</td>\n",
       "      <td>[insect, animal, pest, arthropod]</td>\n",
       "      <td>[An ant is a [MASK]., An ant is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>wren</td>\n",
       "      <td>[bird]</td>\n",
       "      <td>[A wren is a [MASK]., A wren is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>yacht</td>\n",
       "      <td>[boat, luxury]</td>\n",
       "      <td>[A yacht is a [MASK]., A yacht is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>yoghurt</td>\n",
       "      <td>[food, snack, liquid]</td>\n",
       "      <td>[A yoghurt is a [MASK]., A yoghurt is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>yoyo</td>\n",
       "      <td>[toy, craze]</td>\n",
       "      <td>[A yoyo is a [MASK]., A yoyo is an [MASK].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>zebra</td>\n",
       "      <td>[animal, mammal, herbivore]</td>\n",
       "      <td>[A zebra is a [MASK]., A zebra is an [MASK].]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sub_label                                          obj_label  \\\n",
       "0    aeroplane                                          [vehicle]   \n",
       "1    alligator  [reptile, animal, carnivore, predator, amphibian]   \n",
       "2    ambulance                                     [vehicle, van]   \n",
       "3       anchor                                           [symbol]   \n",
       "4          ant                  [insect, animal, pest, arthropod]   \n",
       "..         ...                                                ...   \n",
       "517       wren                                             [bird]   \n",
       "518      yacht                                     [boat, luxury]   \n",
       "519    yoghurt                              [food, snack, liquid]   \n",
       "520       yoyo                                       [toy, craze]   \n",
       "521      zebra                        [animal, mammal, herbivore]   \n",
       "\n",
       "                                      masked_sentences  \n",
       "0    [An aeroplane is a [MASK]., An aeroplane is an...  \n",
       "1    [An alligator is a [MASK]., An alligator is an...  \n",
       "2    [An ambulance is a [MASK]., An ambulance is an...  \n",
       "3    [An anchor is a [MASK]., An anchor is an [MASK].]  \n",
       "4          [An ant is a [MASK]., An ant is an [MASK].]  \n",
       "..                                                 ...  \n",
       "517        [A wren is a [MASK]., A wren is an [MASK].]  \n",
       "518      [A yacht is a [MASK]., A yacht is an [MASK].]  \n",
       "519  [A yoghurt is a [MASK]., A yoghurt is an [MASK].]  \n",
       "520        [A yoyo is a [MASK]., A yoyo is an [MASK].]  \n",
       "521      [A zebra is a [MASK]., A zebra is an [MASK].]  \n",
       "\n",
       "[522 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_multiple_labels(df, relations, output_file):\n",
    "    df= df.query(f\"relation in {relations}\")\n",
    "    examples = []\n",
    "    for name, group in df.groupby(by='sub_label'):\n",
    "        example = defaultdict()\n",
    "        example['sub_label'] = name\n",
    "        example['obj_label'] = [obj[0] for obj in group['obj_label'].values] #.tolist()\n",
    "        example['masked_sentences'] = [f\"{_get_article(name)} {name} is a [MASK].\", f\"{_get_article(name)} {name} is an [MASK].\"]\n",
    "#         masked_sentences=[]\n",
    "#         for x in group['masked_sentences'].values:\n",
    "#             if x not in masked_sentences:\n",
    "#                 masked_sentences.append(x[0]) \n",
    "# #         example['masked_sentences'] = list(set(group['masked_sentences'].values)) #\n",
    "#         example['masked_sentences']  = list(set(masked_sentences))\n",
    "#         print(example['masked_sentence'])\n",
    "        examples.append(example)\n",
    "    examples = pd.DataFrame(examples)\n",
    "    save_dict_to_json(examples.to_dict(orient=\"records\") , output_path =output_file )\n",
    "    return examples\n",
    "\n",
    "# path = 'CLSB2014.xlsx'\n",
    "# save_dir = './cloze_form_prompts'\n",
    "# output_file = f'{save_dir}/clsb2014.jsonl'\n",
    "# df = read_clsb(path, save_dir, output_file, add_plural_ground_truth=False)\n",
    "output_file = f\"{save_dir}/\" + \"IsA\" + '.jsonl'\n",
    "merge_multiple_labels(df, relations=['is a ', 'is an '], output_file=output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the hypernym from CLSB (IsA + IsAn) with single labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fbee6f297c23>:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['feature_type'] = df['feature_type'].apply(lambda x: \"_\".join(x.split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#qualified instances: 13386\n",
      "save ./cloze_form_prompts/clsb2014.jsonl with 13386 lines\n",
      "save ./cloze_form_prompts/clsb2014.jsonl 13386 lines\n",
      "save ./cloze_form_prompts/clsb_ISA.jsonl with 1042 lines\n",
      "save ./cloze_form_prompts/clsb_MADEOF.jsonl with 1496 lines\n",
      "save ./cloze_form_prompts/clsb_IS.jsonl with 6859 lines\n",
      "save ./cloze_form_prompts/clsb_HASA.jsonl with 1422 lines\n",
      "save ./cloze_form_prompts/clsb_HAS.jsonl with 2299 lines\n",
      "save ./cloze_form_prompts/clsb_ISAN.jsonl with 268 lines\n",
      "save ./cloze_form_prompts/hypernym/IsA.jsonl with 1310 lines\n",
      "#Hypo 522\n",
      "#Hyper 329\n",
      "#Hypo-Hyper pairs: 1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9e579836da86>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['obj_label_single'] = df['obj_label'].apply(lambda x: x[0])\n",
      "<ipython-input-4-9e579836da86>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['relation'] = df['relation'].apply(lambda x: relation_group_name)\n"
     ]
    }
   ],
   "source": [
    "def extract_relation_groups(df, relations, relation_group_name, output_file):\n",
    "    df= df.query(f\"relation in {relations}\")\n",
    "    df['obj_label_single'] = df['obj_label'].apply(lambda x: x[0])\n",
    "    word_pairs = [name for name, group in df.groupby(['sub_label', 'obj_label_single'])]\n",
    "    hypernyms = set(df['obj_label_single'].to_list())\n",
    "\n",
    "    df['relation'] = df['relation'].apply(lambda x: relation_group_name)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['uuid'] = df.index\n",
    "    examples = df.to_dict(orient='records')\n",
    "    save_dict_to_json(examples , output_path =output_file )\n",
    "\n",
    "    print(f\"#Hypo {len(set(df['sub_label']))}\")\n",
    "    print(f\"#Hyper {len(hypernyms)}\")\n",
    "    print(f\"#Hypo-Hyper pairs:\",len(word_pairs) )\n",
    "    df = df[['sub_label', 'obj_label', 'relation', 'masked_sentences', 'uuid']]\n",
    "\n",
    "    # return examples\n",
    "\n",
    "path = 'CLSB2014.xlsx'\n",
    "save_dir = './cloze_form_prompts'\n",
    "output_file = f'{save_dir}/clsb2014.jsonl'\n",
    "df = read_clsb(path, save_dir, output_file, add_plural_ground_truth=True)\n",
    "extract_relation_groups(df, relations=['is a ', 'is an '], relation_group_name='IsA', output_file= f'{save_dir}/hypernym/IsA.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chunhua/.bashrc: line 103: bind: warning: line editing not enabled\n",
      "/home/chunhua/.bashrc: line 104: bind: warning: line editing not enabled\n",
      "IsA.jsonl                                     100%   70KB   7.5MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "# \"ISA\"\n",
    "# !scp ./cloze_form_prompts/clsb_ISA.jsonl spartan:~/cogsci/DAP/data/clsb/sgpl\n",
    "!scp ./cloze_form_prompts/IsA.jsonl spartan:~/cogsci/DAP/data/clsb/singular/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 221114 Extracting the IsA relation \n",
    "\n",
    "sub_label: original concepts </br>\n",
    "obj_label: original properties + their plural format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# from inflection import singularize, pluralize \n",
    "\n",
    "\n",
    "def get_sister_terms(word, hop=1):\n",
    "    '''\n",
    "    \"Coordinate (sister) terms: share the same hypernym\"\n",
    "    \"The sister relation is the usual one encountered when working with tree structures: sisters are word forms (either simple words or collocations) that are both immediate hyponyms of the same node\"\n",
    "    \n",
    "    Args:\n",
    "        word: the input word\n",
    "        hop: the hops to hypernyms, default is 1, which means take the top 1 hypernym of x\n",
    "    '''\n",
    "    sister_terms = set()\n",
    "    for synset in wn.synsets(word ,\"n\"):\n",
    "        for hypernym in synset.hypernyms()[:hop]:\n",
    "#             print(hypernym)\n",
    "            sister_synsets = hypernym.hyponyms()\n",
    "            for sister_synset in sister_synsets:\n",
    "                sister_names = [x.name() for x in sister_synset.lemmas()]\n",
    "                sister_names_selected = [name.lower() for name in sister_names if len(name.split(\"_\"))==1 and  len(name.split(\"-\"))==1  and name!=word]\n",
    "                sister_terms = sister_terms.union(set(sister_names_selected))\n",
    "    return list(sister_terms)\n",
    "\n",
    "# get_sister_terms(\"bag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2abfea23fdeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"IsA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "output_file = f\"{save_dir}/\" + \"IsA\" + '.jsonl'\n",
    "# merge_multiple_labels(df, relations=['is a', 'is an'], output_file=output_file)\n",
    "relations = ['is a ', 'is an ']\n",
    "df= df.query(f\"relation in {relations}\")\n",
    "print(len(df.index))\n",
    "add_plural_ground_truth = False #True #\n",
    "# df.head()\n",
    "examples = []\n",
    "for i, (name, group) in enumerate(df.groupby(by='sub_label')):\n",
    "    example = defaultdict()\n",
    "    example['uuid'] = i\n",
    "    example['sub_label'] = name\n",
    "    example['sub_sister'] = get_sister_terms(word=name)\n",
    "    if len(example['sub_sister'])==0: continue\n",
    "\n",
    "    obj_label = group['obj_label'].to_list()\n",
    "    if add_plural_ground_truth:\n",
    "        example['obj_label'] = obj_label + [pluralize(word) for word in obj_label]\n",
    "    else:\n",
    "        example['obj_label'] = obj_label\n",
    "\n",
    "    example['masked_sentences'] = list(set(group['masked_sentences'].to_list()))\n",
    "    example['feature_type'] = group['feature_type'].to_list()\n",
    "    example['relation'] = \"IsA\"\n",
    "\n",
    "    examples.append(example)\n",
    "save_dict_to_json(examples , output_path =output_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_label</th>\n",
       "      <th>sub_sister</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>[eggbeater, autogiro, ornithopter, drone, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alligator</td>\n",
       "      <td>[pigskin, morocco, suede, calf, buff, roan, ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ambulance</td>\n",
       "      <td>[racer, limousine, roadster, hack, bus, jeep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anchor</td>\n",
       "      <td>[keystone, linchpin, lynchpin, anchorperson, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ant</td>\n",
       "      <td>[chalcid, chalcidfly, wasp, emmet, pismire, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple</td>\n",
       "      <td>[plumcot, lanseh, litchee, ananas, jujube, gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apricot</td>\n",
       "      <td>[plumcot, lanseh, litchee, ananas, coral, juju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arm</td>\n",
       "      <td>[knee, cornice, cubitus, forearm, rim, burr, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>armchair</td>\n",
       "      <td>[wheelchair, highchair, chaise, rocker, daybed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arrow</td>\n",
       "      <td>[shot, boomerang, pointer, point, caret, punct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>artichoke</td>\n",
       "      <td>[squash, pumpkin, finocchio, rhubarb, onion, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>asparagus</td>\n",
       "      <td>[horseradish, acanthus, anchusa, gipsywort, rh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aspirin</td>\n",
       "      <td>[sublimaze, pyridium, tylenol, hydromorphone, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>aubergine</td>\n",
       "      <td>[horseradish, acanthus, anchusa, gipsywort, rh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avocado</td>\n",
       "      <td>[plumcot, lanseh, litchee, ananas, jujube, gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>axe</td>\n",
       "      <td>[adz, hob, sickle, knife, ax, plane, scythe, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bacon</td>\n",
       "      <td>[ham, jambon, gammon, spareribs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>badge</td>\n",
       "      <td>[centrepiece, contour, attractor, excellency, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bag</td>\n",
       "      <td>[kettleful, locating, politics, leading, mourn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ball</td>\n",
       "      <td>[calyculus, cytoskeleton, valve, hobby, capsul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sub_label                                         sub_sister\n",
       "0   aeroplane  [eggbeater, autogiro, ornithopter, drone, plan...\n",
       "1   alligator  [pigskin, morocco, suede, calf, buff, roan, ga...\n",
       "2   ambulance  [racer, limousine, roadster, hack, bus, jeep, ...\n",
       "3      anchor  [keystone, linchpin, lynchpin, anchorperson, m...\n",
       "4         ant  [chalcid, chalcidfly, wasp, emmet, pismire, mu...\n",
       "5       apple  [plumcot, lanseh, litchee, ananas, jujube, gen...\n",
       "6     apricot  [plumcot, lanseh, litchee, ananas, coral, juju...\n",
       "7         arm  [knee, cornice, cubitus, forearm, rim, burr, t...\n",
       "8    armchair    [wheelchair, highchair, chaise, rocker, daybed]\n",
       "9       arrow  [shot, boomerang, pointer, point, caret, punct...\n",
       "10  artichoke  [squash, pumpkin, finocchio, rhubarb, onion, g...\n",
       "11  asparagus  [horseradish, acanthus, anchusa, gipsywort, rh...\n",
       "12    aspirin  [sublimaze, pyridium, tylenol, hydromorphone, ...\n",
       "13  aubergine  [horseradish, acanthus, anchusa, gipsywort, rh...\n",
       "14    avocado  [plumcot, lanseh, litchee, ananas, jujube, gen...\n",
       "15        axe  [adz, hob, sickle, knife, ax, plane, scythe, s...\n",
       "16      bacon                   [ham, jambon, gammon, spareribs]\n",
       "17      badge  [centrepiece, contour, attractor, excellency, ...\n",
       "18        bag  [kettleful, locating, politics, leading, mourn...\n",
       "19       ball  [calyculus, cytoskeleton, valve, hobby, capsul..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa[['sub_label', 'sub_sister']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 16), (25, 16), (0, 16), (2, 16), (1, 15), (5, 14), (3, 14), (6, 14), (10, 11), (7, 11), (4, 10), (17, 10), (16, 9), (20, 9), (11, 9), (18, 9), (9, 9), (15, 8), (19, 8), (28, 8), (33, 7), (30, 7), (13, 7), (43, 7), (26, 6), (24, 6), (23, 6), (40, 6), (14, 6), (34, 6), (27, 5), (90, 5), (21, 5), (142, 4), (31, 4), (57, 4), (44, 4), (39, 4), (70, 4), (308, 3), (102, 3), (54, 3), (38, 3), (292, 3), (22, 3), (55, 3), (42, 3), (47, 3), (210, 3), (12, 3), (35, 3), (144, 3), (49, 3), (135, 3), (29, 2), (88, 2), (63, 2), (298, 2), (36, 2), (378, 2), (117, 2), (32, 2), (105, 2), (127, 2), (59, 2), (73, 2), (64, 2), (140, 2), (94, 2), (45, 2), (235, 2), (65, 2), (62, 2), (145, 2), (41, 2), (107, 2), (112, 2), (551, 2), (75, 2), (89, 2), (48, 2), (318, 2), (178, 2), (51, 2), (68, 2), (99, 2), (46, 2), (108, 1), (346, 1), (162, 1), (171, 1), (169, 1), (168, 1), (104, 1), (211, 1), (214, 1), (234, 1), (309, 1), (451, 1), (242, 1), (221, 1), (111, 1), (133, 1), (317, 1), (61, 1), (60, 1), (158, 1), (261, 1), (114, 1), (181, 1), (189, 1), (559, 1), (216, 1), (130, 1), (96, 1), (596, 1), (201, 1), (95, 1), (78, 1), (87, 1), (204, 1), (116, 1), (37, 1), (141, 1), (124, 1), (81, 1), (66, 1), (581, 1), (113, 1), (100, 1), (146, 1), (67, 1), (542, 1), (300, 1), (69, 1), (264, 1), (92, 1), (179, 1), (228, 1), (331, 1), (232, 1), (535, 1), (72, 1), (539, 1), (118, 1), (176, 1), (109, 1), (148, 1), (143, 1), (597, 1), (217, 1), (281, 1), (97, 1), (544, 1), (303, 1), (159, 1), (86, 1), (110, 1), (123, 1), (84, 1), (131, 1), (147, 1), (125, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "dfa = pd.DataFrame(examples)\n",
    "num = len(examples)\n",
    "dfa['sub_sister_num'] = dfa['sub_sister'].apply(lambda x: len(x))\n",
    "print(Counter(dfa['sub_sister_num']).most_common())\n",
    "\n",
    "# dfa['sub_sister_num'].value_counts(normalize=True).plot(kind='bar')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Obj labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg properties:  2.5095785440613025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>sub_label</th>\n",
       "      <th>sub_sister</th>\n",
       "      <th>obj_label</th>\n",
       "      <th>masked_sentences</th>\n",
       "      <th>feature_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>obj_label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aeroplane</td>\n",
       "      <td>{eggbeater, autogiro, ornithopter, drone, plan...</td>\n",
       "      <td>[vehicle]</td>\n",
       "      <td>[An aeroplane is a [MASK].]</td>\n",
       "      <td>[taxonomic]</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>alligator</td>\n",
       "      <td>{pigskin, morocco, suede, calf, buff, roan, ga...</td>\n",
       "      <td>[reptile, animal, carnivore, predator, amphibian]</td>\n",
       "      <td>[An alligator is an [MASK]., An alligator is a...</td>\n",
       "      <td>[taxonomic, taxonomic, functional, taxonomic, ...</td>\n",
       "      <td>IsA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>{racer, hot-rod, limousine, roadster, hack, bu...</td>\n",
       "      <td>[vehicle, van]</td>\n",
       "      <td>[An ambulance is a [MASK].]</td>\n",
       "      <td>[taxonomic, taxonomic]</td>\n",
       "      <td>IsA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>anchor</td>\n",
       "      <td>{keystone, linchpin, lynchpin, anchorperson, m...</td>\n",
       "      <td>[symbol]</td>\n",
       "      <td>[An anchor is a [MASK].]</td>\n",
       "      <td>[encyclopaedic]</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ant</td>\n",
       "      <td>{chalcid, chalcidfly, wasp, emmet, pismire, be...</td>\n",
       "      <td>[insect, animal, pest, arthropod]</td>\n",
       "      <td>[An ant is an [MASK]., An ant is a [MASK].]</td>\n",
       "      <td>[taxonomic, taxonomic, encyclopaedic, taxonomic]</td>\n",
       "      <td>IsA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uuid  sub_label                                         sub_sister  \\\n",
       "0     0  aeroplane  {eggbeater, autogiro, ornithopter, drone, plan...   \n",
       "1     1  alligator  {pigskin, morocco, suede, calf, buff, roan, ga...   \n",
       "2     2  ambulance  {racer, hot-rod, limousine, roadster, hack, bu...   \n",
       "3     3     anchor  {keystone, linchpin, lynchpin, anchorperson, m...   \n",
       "4     4        ant  {chalcid, chalcidfly, wasp, emmet, pismire, be...   \n",
       "\n",
       "                                           obj_label  \\\n",
       "0                                          [vehicle]   \n",
       "1  [reptile, animal, carnivore, predator, amphibian]   \n",
       "2                                     [vehicle, van]   \n",
       "3                                           [symbol]   \n",
       "4                  [insect, animal, pest, arthropod]   \n",
       "\n",
       "                                    masked_sentences  \\\n",
       "0                        [An aeroplane is a [MASK].]   \n",
       "1  [An alligator is an [MASK]., An alligator is a...   \n",
       "2                        [An ambulance is a [MASK].]   \n",
       "3                           [An anchor is a [MASK].]   \n",
       "4        [An ant is an [MASK]., An ant is a [MASK].]   \n",
       "\n",
       "                                        feature_type relation  obj_label_num  \n",
       "0                                        [taxonomic]      IsA              1  \n",
       "1  [taxonomic, taxonomic, functional, taxonomic, ...      IsA              5  \n",
       "2                             [taxonomic, taxonomic]      IsA              2  \n",
       "3                                    [encyclopaedic]      IsA              1  \n",
       "4   [taxonomic, taxonomic, encyclopaedic, taxonomic]      IsA              4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !ls\n",
    "# df = pd.read_excel(, )\n",
    "# df = pd.read_excel('CLSB2014.xlsx', engine='openpyxl')\n",
    "# df.head()\n",
    "from matplotlib import pyplot as plt\n",
    "dfa = pd.DataFrame(examples)\n",
    "num = len(examples)\n",
    "dfa['obj_label_num'] = dfa['obj_label'].apply(lambda x: len(x))\n",
    "dfa['obj_label_num'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "print(\"avg properties: \", sum(dfa['obj_label_num'].to_list())/num) \n",
    "display(dfa.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scp clsb_ISA.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/\n",
    "# !scp clsb_MADEOF.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/\n",
    "# !scp clsb_IS.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/\n",
    "# !scp clsb_HASA.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/\n",
    "# !scp clsb_HAS.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fec905d177e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_label_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mlowercase_first_letter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0muuid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muuid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uuid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-fec905d177e3>\u001b[0m in \u001b[0;36mlowercase_first_letter\u001b[0;34m(s, upper_rest)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlowercase_first_letter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupper_rest\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Everyone knows that a'\u001b[0m \u001b[0;31m#remove the 'a' if the vowel concepts (e.g., arm) are tested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def lowercase_first_letter(s, upper_rest = False):\n",
    "    return ''.join([s[:1].lower(), (s[1:].upper() if upper_rest else s[1:])]) \n",
    "\n",
    "prefix = 'Everyone knows that a' #remove the 'a' if the vowel concepts (e.g., arm) are tested\n",
    "groups = []\n",
    "save_dir = './concept_template_prompts'\n",
    "\n",
    "uuid = 0\n",
    "for rel_name, rel_group in  df.groupby([\"relation\", ]): \n",
    "    examples = []\n",
    "    for sub_label, group in rel_group.groupby([\"sub_label\"]):\n",
    "        example = defaultdict()\n",
    "        if sub_label[:1] not in ['a', 'e', 'i', 'o', 'u']: #follow the Wier et al., (2020), removing the concepts started with vowels\n",
    "            example['sub_label'] = sub_label\n",
    "            example['obj_label'] = group['obj_label'].to_list()\n",
    "            example['obj_label_weight'] = group['weight'].to_list() #values\n",
    "            example['masked_sentences'] = [ prefix +  lowercase_first_letter(group['masked_sentences'].values[0])]\n",
    "            uuid = uuid + 1\n",
    "            example['uuid'] = uuid\n",
    "            examples.append(example)\n",
    "    output_file = f\"{save_dir}/clsb_\" + \"\".join(rel_name).upper().replace(\" \", \"\").strip() + '.jsonl'\n",
    "    save_dict_to_json(examples, output_file)\n",
    "    # break                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clsb_HAS.jsonl                                100%   99KB  23.0MB/s   00:00    \n",
      "clsb_HASA.jsonl                               100%   86KB  37.2MB/s   00:00    \n",
      "clsb_IS.jsonl                                 100%  167KB  42.6MB/s   00:00    \n",
      "clsb_ISA.jsonl                                100%   80KB  29.8MB/s   00:00    \n",
      "clsb_MADEOF.jsonl                             100%   81KB  35.0MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ./concept_template_prompts/*.jsonl doe:/home/chunhua/CommonRel/DAP/data/clsb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58472/58472 [00:00<00:00, 333379.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58472/58472 [00:02<00:00, 23671.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58472/58472 [01:58<00:00, 492.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58472/58472 [02:05<00:00, 465.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save clsb2014.en.csv 58472 lines\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if __name__=='__main__':\n",
    "\n",
    "path = 'CLSB2014.xlsx'\n",
    "output_file = 'clsb2014.en.csv'\n",
    "read_clsb(path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproceeing sta data for anchor prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/made_of/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/MadeOf.jsonl with 495 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/is/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/Is.jsonl with 583 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/has_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/HasA.jsonl with 537 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/is_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/IsA.jsonl with 506 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/has/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/Has.jsonl with 564 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/made_of/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/MadeOf.jsonl with 495 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/is/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/Is.jsonl with 583 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/has_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/HasA.jsonl with 537 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/is_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/IsA.jsonl with 506 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/has/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/Has.jsonl with 564 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "for type in ['everyone_knows', 'blank']:\n",
    "    vocab_response = []\n",
    "\n",
    "    parent_dir = f'./sta_cogsci_prompts/concept2feature/{type}/'\n",
    "    target_dir = f'./sta_data_for_anchor_prompts/{type}/'\n",
    "\n",
    "    # def extract_sub_label():\n",
    "    relation_name_mapping={'is_a':'IsA', 'has_a': 'HasA', 'is': 'Is', 'made_of':'MadeOf', 'has': 'Has' }\n",
    "\n",
    "  \n",
    "\n",
    "    for folder in os.listdir(parent_dir):\n",
    "        if folder.startswith(\".DS_Store\"): continue \n",
    "        data_folder = os.path.join(parent_dir, folder)\n",
    "        files = os.listdir(data_folder)\n",
    "        examples = []\n",
    "        cur_relation = folder\n",
    "        for file in files: \n",
    "            if file.startswith(\".DS_Store\"): continue \n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            print(f\"reading: {file_path}\")\n",
    "\n",
    "            with open(file_path, ) as fin:\n",
    "                for line in fin.readlines():\n",
    "                    example = defaultdict()\n",
    "                    line = eval(line)\n",
    "\n",
    "                    masked_sentences = line[\"masked_sentences\"]\n",
    "\n",
    "                    example['uuid'] = line[\"id\"]\n",
    "                    example['masked_sentences'] = masked_sentences\n",
    "                    example[\"obj_label\"] = line[\"answers\"]\n",
    "                    example[\"relation\"] = line[\"seed\"]\n",
    "                    example[\"feature_type\"] = line[\"seed\"] = line['answer_types']\n",
    "                    if \"Everyone knows that\" in masked_sentences[0]:\n",
    "                        example[\"sub_label\"] = masked_sentences[0].split()[4] \n",
    "                    # else if 'blank' in \n",
    "                    else:\n",
    "                        example[\"sub_label\"] = masked_sentences[0].split()[1] \n",
    "                    examples.append(example)\n",
    "                    vocab_response.extend(example[\"obj_label\"])\n",
    "\n",
    "            output_path   = target_dir + relation_name_mapping.get(cur_relation) + \".jsonl\"\n",
    "            save_dict_to_json(examples, output_path)\n",
    "    vocab_response = set(vocab_response)\n",
    "    pd.DataFrame(vocab_response).to_csv(target_dir + \"vocab.txt\", index=False, header=False)\n",
    " \n",
    "# def save_vocab(vocab, output_file):\n",
    "#     with open(output_file, 'w') as fout:\n",
    "#         fout.write(vocab)\n",
    "# print(len(vocab_response))\n",
    "# print(vocab_response)\n",
    "\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/made_of/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/MadeOf.jsonl with 495 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/is/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/Is.jsonl with 583 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/has_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/HasA.jsonl with 537 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/is_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/IsA.jsonl with 506 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/everyone_knows/has/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/everyone_knows/Has.jsonl with 564 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/made_of/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/MadeOf.jsonl with 495 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/is/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/Is.jsonl with 583 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/has_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/HasA.jsonl with 537 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/is_a/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/IsA.jsonl with 506 lines\n",
      "reading: ./sta_cogsci_prompts/concept2feature/blank/has/gold.jsonl\n",
      "save ./sta_data_for_anchor_prompts/blank/Has.jsonl with 564 lines\n",
      "[(1, 938), (2, 922), (3, 828), (4, 548), (5, 388), (6, 244), (7, 200), (8, 192), (15, 128), (13, 112), (9, 110), (10, 108), (11, 100), (14, 96), (12, 88), (17, 80), (18, 74), (16, 70), (19, 44), (21, 30), (20, 20), (22, 18), (23, 10), (26, 10), (27, 4), (24, 4), (25, 2), (28, 2)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "obj_num_counter = []\n",
    "\n",
    "for type in ['everyone_knows', 'blank']:\n",
    "    vocab_response = []\n",
    "\n",
    "    parent_dir = f'./sta_cogsci_prompts/concept2feature/{type}/'\n",
    "    target_dir = f'./sta_data_for_anchor_prompts/{type}/'\n",
    "\n",
    "    # def extract_sub_label():\n",
    "    relation_name_mapping={'is_a':'IsA', 'has_a': 'HasA', 'is': 'Is', 'made_of':'MadeOf', 'has': 'Has' }\n",
    "\n",
    "  \n",
    "\n",
    "    for folder in os.listdir(parent_dir):\n",
    "        if folder.startswith(\".DS_Store\"): continue \n",
    "        data_folder = os.path.join(parent_dir, folder)\n",
    "        files = os.listdir(data_folder)\n",
    "        examples = []\n",
    "        cur_relation = folder\n",
    "        for file in files: \n",
    "            if file.startswith(\".DS_Store\"): continue \n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            print(f\"reading: {file_path}\")\n",
    "\n",
    "            with open(file_path, ) as fin:\n",
    "                for line in fin.readlines():\n",
    "                    example = defaultdict()\n",
    "                    line = eval(line)\n",
    "\n",
    "                    masked_sentences = line[\"masked_sentences\"]\n",
    "\n",
    "                    example['uuid'] = line[\"id\"]\n",
    "                    example['masked_sentences'] = masked_sentences\n",
    "                    example[\"obj_label\"] = line[\"answers\"]\n",
    "                    example['obj_label_num'] = len(example[\"obj_label\"]) \n",
    "\n",
    "                    obj_num_counter.append(example['obj_label_num']) \n",
    "\n",
    "                    example[\"relation\"] = line[\"seed\"]\n",
    "                    example[\"feature_type\"] = line[\"seed\"] = line['answer_types']\n",
    "                    if \"Everyone knows that\" in masked_sentences[0]:\n",
    "                        example[\"sub_label\"] = masked_sentences[0].split()[4] \n",
    "                    # else if 'blank' in \n",
    "                    else:\n",
    "                        example[\"sub_label\"] = masked_sentences[0].split()[1] \n",
    "                    examples.append(example)\n",
    "                    vocab_response.extend(example[\"obj_label\"])\n",
    "\n",
    "            output_path   = target_dir + relation_name_mapping.get(cur_relation) + \".jsonl\"\n",
    "            save_dict_to_json(examples, output_path)\n",
    "    vocab_response = set(vocab_response)\n",
    "    pd.DataFrame(vocab_response).to_csv(target_dir + \"vocab.txt\", index=False, header=False)\n",
    " \n",
    "# def save_vocab(vocab, output_file):\n",
    "#     with open(output_file, 'w') as fout:\n",
    "#         fout.write(vocab)\n",
    "# print(len(vocab_response))\n",
    "# print(vocab_response)\n",
    "\n",
    "\n",
    "# df.head()\n",
    "print(Counter(obj_num_counter).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>masked_sentences</th>\n",
       "      <th>obj_label</th>\n",
       "      <th>obj_label_num</th>\n",
       "      <th>relation</th>\n",
       "      <th>feature_type</th>\n",
       "      <th>sub_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[An aeroplane is a [MASK].]</td>\n",
       "      <td>[vehicle]</td>\n",
       "      <td>1</td>\n",
       "      <td>is_a</td>\n",
       "      <td>[taxonomic]</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[An alligator is a [MASK].]</td>\n",
       "      <td>[predator]</td>\n",
       "      <td>1</td>\n",
       "      <td>is_a</td>\n",
       "      <td>[taxonomic]</td>\n",
       "      <td>alligator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[An ambulance is a [MASK].]</td>\n",
       "      <td>[vehicle, van]</td>\n",
       "      <td>2</td>\n",
       "      <td>is_a</td>\n",
       "      <td>[taxonomic, taxonomic]</td>\n",
       "      <td>ambulance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[An anchor is a [MASK].]</td>\n",
       "      <td>[symbol]</td>\n",
       "      <td>1</td>\n",
       "      <td>is_a</td>\n",
       "      <td>[encyclopaedic]</td>\n",
       "      <td>anchor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[An apple is a [MASK].]</td>\n",
       "      <td>[fruit, food]</td>\n",
       "      <td>2</td>\n",
       "      <td>is_a</td>\n",
       "      <td>[taxonomic, taxonomic]</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uuid             masked_sentences       obj_label  obj_label_num relation  \\\n",
       "0     0  [An aeroplane is a [MASK].]       [vehicle]              1     is_a   \n",
       "1     1  [An alligator is a [MASK].]      [predator]              1     is_a   \n",
       "2     2  [An ambulance is a [MASK].]  [vehicle, van]              2     is_a   \n",
       "3     3     [An anchor is a [MASK].]        [symbol]              1     is_a   \n",
       "4     4      [An apple is a [MASK].]   [fruit, food]              2     is_a   \n",
       "\n",
       "             feature_type  sub_label  \n",
       "0             [taxonomic]  aeroplane  \n",
       "1             [taxonomic]  alligator  \n",
       "2  [taxonomic, taxonomic]  ambulance  \n",
       "3         [encyclopaedic]     anchor  \n",
       "4  [taxonomic, taxonomic]      apple  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './sta_data_for_anchor_prompts/blank/IsA.jsonl'\n",
    "# dft = pd.read_json(path, orient='records')\n",
    "# dft.head() \n",
    "dft = []\n",
    "with open(path, 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line = eval(line)\n",
    "        dft.append(line)\n",
    "dft = pd.DataFrame(dft)\n",
    "dft.head()\n",
    "\n",
    "# such [MASK] as [X] or [Z]\n",
    "# df.\n",
    "# with open(path, 'r') as fin:\n",
    "#     lines = fin.readlines()\n",
    "#     for line in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "1. the dataset from Weir includes many post-process \n",
    "> e.g., the properteis started with vowel are removed (ladybird, is an, insect). Most of them are \"A is a __\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1042, 'an': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dft['sub_article'] = dft['sub_label'].apply(lambda x: _get_article(x))\n",
    "obj_articles = []\n",
    "for x in dft.obj_label:\n",
    "    obj_articles.extend([_get_article(word) for word in x])\n",
    "\n",
    "Counter(obj_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_label</th>\n",
       "      <th>obj_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>tambourine</td>\n",
       "      <td>[drum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>slug</td>\n",
       "      <td>[snail, creature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>doorknob</td>\n",
       "      <td>[handle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>llama</td>\n",
       "      <td>[creature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>[treat, dessert, gift, sweet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>penguin</td>\n",
       "      <td>[bird, hunter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>corn</td>\n",
       "      <td>[vegetable, crop, plant, seed, staple, cereal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>clamp</td>\n",
       "      <td>[tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>[monkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>trowel</td>\n",
       "      <td>[tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>rattle</td>\n",
       "      <td>[toy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>[fruit, symbol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>pansy</td>\n",
       "      <td>[flower, plant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>screw</td>\n",
       "      <td>[tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>salmon</td>\n",
       "      <td>[fish, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>carrot</td>\n",
       "      <td>[vegetable, root]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>tugboat</td>\n",
       "      <td>[boat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>dinghy</td>\n",
       "      <td>[boat, device]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>train</td>\n",
       "      <td>[vehicle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>lizard</td>\n",
       "      <td>[creature, pet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>ant</td>\n",
       "      <td>[bug, worker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>elm</td>\n",
       "      <td>[tree, wood, plant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>apron</td>\n",
       "      <td>[garment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>potato</td>\n",
       "      <td>[vegetable, jacket, plant, staple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>dove</td>\n",
       "      <td>[bird, symbol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>wheelbarrow</td>\n",
       "      <td>[bucket, tool, container]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>slippers</td>\n",
       "      <td>[pair]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>plough</td>\n",
       "      <td>[tool, machine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bayonet</td>\n",
       "      <td>[weapon, gun, knife, blade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>hedgehog</td>\n",
       "      <td>[creature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>cart</td>\n",
       "      <td>[vehicle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>plum</td>\n",
       "      <td>[fruit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>jacket</td>\n",
       "      <td>[garment, coat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>strainer</td>\n",
       "      <td>[tool, filter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>glider</td>\n",
       "      <td>[plane]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>basket</td>\n",
       "      <td>[container]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>camera</td>\n",
       "      <td>[machine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>daisy</td>\n",
       "      <td>[flower, plant, weed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bottle</td>\n",
       "      <td>[container]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>scallop</td>\n",
       "      <td>[fish, creature, starter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bouquet</td>\n",
       "      <td>[gift, present]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>[plant, leaf, drug, crop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>envelope</td>\n",
       "      <td>[square, container]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>spinach</td>\n",
       "      <td>[vegetable, leaf, plant, salad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>dog</td>\n",
       "      <td>[pet, companion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>lamb</td>\n",
       "      <td>[sheep, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>gong</td>\n",
       "      <td>[disc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>skirt</td>\n",
       "      <td>[garment, length]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>bread</td>\n",
       "      <td>[staple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>turnip</td>\n",
       "      <td>[vegetable, root, bulb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_label                                       obj_label\n",
       "370   tambourine                                          [drum]\n",
       "486         slug                               [snail, creature]\n",
       "502     doorknob                                        [handle]\n",
       "463        llama                                      [creature]\n",
       "92     chocolate                   [treat, dessert, gift, sweet]\n",
       "274      penguin                                  [bird, hunter]\n",
       "106         corn  [vegetable, crop, plant, seed, staple, cereal]\n",
       "94         clamp                                          [tool]\n",
       "451      gorilla                                        [monkey]\n",
       "393       trowel                                          [tool]\n",
       "304       rattle                                           [toy]\n",
       "359   strawberry                                 [fruit, symbol]\n",
       "259        pansy                                 [flower, plant]\n",
       "329        screw                                          [tool]\n",
       "318       salmon                                    [fish, food]\n",
       "74        carrot                               [vegetable, root]\n",
       "395      tugboat                                          [boat]\n",
       "128       dinghy                                  [boat, device]\n",
       "390        train                                       [vehicle]\n",
       "217       lizard                                 [creature, pet]\n",
       "423          ant                                   [bug, worker]\n",
       "139          elm                             [tree, wood, plant]\n",
       "424        apron                                       [garment]\n",
       "290       potato              [vegetable, jacket, plant, staple]\n",
       "133         dove                                  [bird, symbol]\n",
       "409  wheelbarrow                       [bucket, tool, container]\n",
       "485     slippers                                          [pair]\n",
       "284       plough                                 [tool, machine]\n",
       "24       bayonet                     [weapon, gun, knife, blade]\n",
       "455     hedgehog                                      [creature]\n",
       "75          cart                                       [vehicle]\n",
       "285         plum                                         [fruit]\n",
       "457       jacket                                 [garment, coat]\n",
       "358     strainer                                  [tool, filter]\n",
       "448       glider                                         [plane]\n",
       "499       basket                                     [container]\n",
       "65        camera                                       [machine]\n",
       "121        daisy                           [flower, plant, weed]\n",
       "42        bottle                                     [container]\n",
       "325      scallop                       [fish, creature, starter]\n",
       "43       bouquet                                 [gift, present]\n",
       "385      tobacco                       [plant, leaf, drug, crop]\n",
       "142     envelope                             [square, container]\n",
       "352      spinach                 [vegetable, leaf, plant, salad]\n",
       "130          dog                                [pet, companion]\n",
       "205         lamb                                   [sheep, baby]\n",
       "164         gong                                          [disc]\n",
       "483        skirt                               [garment, length]\n",
       "429        bread                                        [staple]\n",
       "399       turnip                         [vegetable, root, bulb]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dft[['sub_label', 'obj_label']].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#property</th>\n",
       "      <th>freq</th>\n",
       "      <th>rel_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>938</td>\n",
       "      <td>0.174674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>922</td>\n",
       "      <td>0.171695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>828</td>\n",
       "      <td>0.154190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>548</td>\n",
       "      <td>0.102048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>388</td>\n",
       "      <td>0.072253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>244</td>\n",
       "      <td>0.045438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.037244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>192</td>\n",
       "      <td>0.035754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>0.023836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>112</td>\n",
       "      <td>0.020857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "      <td>0.020484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>108</td>\n",
       "      <td>0.020112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>0.018622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>0.017877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>88</td>\n",
       "      <td>0.016387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>80</td>\n",
       "      <td>0.014898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>74</td>\n",
       "      <td>0.013780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>70</td>\n",
       "      <td>0.013035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>0.008194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.003724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>0.003352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #property  freq  rel_freq\n",
       "0           1   938  0.174674\n",
       "1           2   922  0.171695\n",
       "2           3   828  0.154190\n",
       "3           4   548  0.102048\n",
       "4           5   388  0.072253\n",
       "5           6   244  0.045438\n",
       "6           7   200  0.037244\n",
       "7           8   192  0.035754\n",
       "8          15   128  0.023836\n",
       "9          13   112  0.020857\n",
       "10          9   110  0.020484\n",
       "11         10   108  0.020112\n",
       "12         11   100  0.018622\n",
       "13         14    96  0.017877\n",
       "14         12    88  0.016387\n",
       "15         17    80  0.014898\n",
       "16         18    74  0.013780\n",
       "17         16    70  0.013035\n",
       "18         19    44  0.008194\n",
       "19         21    30  0.005587\n",
       "20         20    20  0.003724\n",
       "21         22    18  0.003352\n",
       "22         23    10  0.001862\n",
       "23         26    10  0.001862\n",
       "24         27     4  0.000745\n",
       "25         24     4  0.000745\n",
       "26         25     2  0.000372\n",
       "27         28     2  0.000372"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # all = sum(obj_num_counter)\n",
    "# # for item in Counter(obj_num_counter)\n",
    "\n",
    "# df_tmp = pd.DataFrame(Counter(obj_num_counter).most_common(), columns=['#property', 'freq'])\n",
    "\n",
    "# df_tmp['rel_freq'] = df_tmp['freq']/sum(df_tmp['freq'])\n",
    "# # df_tmp[['#property']].plot(kind='bar') \n",
    "# df_tmp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def get_highest_mrr_among_labels(label, pred):\n",
    "    '''\n",
    "    return the highest rank among the multiple labels \n",
    "\n",
    "    pred: a list of words (candidates)\n",
    "    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)\n",
    "    '''\n",
    "    mrr = 0 \n",
    "    if pred is None: return mrr \n",
    "\n",
    "    rank_list = [ pred.index(item) + 1 for item in label if item in pred] \n",
    "    if len(rank_list)>0:\n",
    "        mrr = 1/min(rank_list)\n",
    "    return mrr \n",
    "\n",
    "label = ['object', 'objects']\n",
    "pred = ['objects', 'index', 'object', 'tool', 'ruler', 'circle', 'knot', 'algorithm', 'acronym', 'needle']\n",
    "\n",
    "mrr = get_highest_mrr_among_labels(label, pred)\n",
    "print(mrr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "339b0067a56740cc6f58ed77266b21ee1e7a595da146313008e3ea19567eaece"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
