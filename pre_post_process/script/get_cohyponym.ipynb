{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug False\n",
      "dataset SHWARTZ\n",
      "11061\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "5000\n",
      "5010\n",
      "5020\n",
      "5030\n",
      "5040\n",
      "5050\n",
      "5060\n",
      "5070\n",
      "5080\n",
      "5090\n",
      "5100\n",
      "5110\n",
      "5120\n",
      "5130\n",
      "5140\n",
      "5150\n",
      "5160\n",
      "5170\n",
      "5180\n",
      "5190\n",
      "5200\n",
      "5210\n",
      "5220\n",
      "5230\n",
      "5240\n",
      "5250\n",
      "5260\n",
      "5270\n",
      "5280\n",
      "5290\n",
      "5300\n",
      "5310\n",
      "5320\n",
      "5330\n",
      "5340\n",
      "5350\n",
      "5360\n",
      "5370\n",
      "5380\n",
      "5390\n",
      "5400\n",
      "5410\n",
      "5420\n",
      "5430\n",
      "5440\n",
      "5450\n",
      "5460\n",
      "5470\n",
      "5480\n",
      "5490\n",
      "5500\n",
      "5510\n",
      "5520\n",
      "5530\n",
      "5540\n",
      "5550\n",
      "5560\n",
      "5570\n",
      "5580\n",
      "5590\n",
      "5600\n",
      "5610\n",
      "5620\n",
      "5630\n",
      "5640\n",
      "5650\n",
      "5660\n",
      "5670\n",
      "5680\n",
      "5690\n",
      "5700\n",
      "5710\n",
      "5720\n",
      "5730\n",
      "5740\n",
      "5750\n",
      "5760\n",
      "5770\n",
      "5780\n",
      "5790\n",
      "5800\n",
      "5810\n",
      "5820\n",
      "5830\n",
      "5840\n",
      "5850\n",
      "5860\n",
      "5870\n",
      "5880\n",
      "5890\n",
      "5900\n",
      "5910\n",
      "5920\n",
      "5930\n",
      "5940\n",
      "5950\n",
      "5960\n",
      "5970\n",
      "5980\n",
      "5990\n",
      "6000\n",
      "6010\n",
      "6020\n",
      "6030\n",
      "6040\n",
      "6050\n",
      "6060\n",
      "6070\n",
      "6080\n",
      "6090\n",
      "6100\n",
      "6110\n",
      "6120\n",
      "6130\n",
      "6140\n",
      "6150\n",
      "6160\n",
      "6170\n",
      "6180\n",
      "6190\n",
      "6200\n",
      "6210\n",
      "6220\n",
      "6230\n",
      "6240\n",
      "6250\n",
      "6260\n",
      "6270\n",
      "6280\n",
      "6290\n",
      "6300\n",
      "6310\n",
      "6320\n",
      "6330\n",
      "6340\n",
      "6350\n",
      "6360\n",
      "6370\n",
      "6380\n",
      "6390\n",
      "6400\n",
      "6410\n",
      "6420\n",
      "6430\n",
      "6440\n",
      "6450\n",
      "6460\n",
      "6470\n",
      "6480\n",
      "6490\n",
      "6500\n",
      "6510\n",
      "6520\n",
      "6530\n",
      "6540\n",
      "6550\n",
      "6560\n",
      "6570\n",
      "6580\n",
      "6590\n",
      "6600\n",
      "6610\n",
      "6620\n",
      "6630\n",
      "6640\n",
      "6650\n",
      "6660\n",
      "6670\n",
      "6680\n",
      "6690\n",
      "6700\n",
      "6710\n",
      "6720\n",
      "6730\n",
      "6740\n",
      "6750\n",
      "6760\n",
      "6770\n",
      "6780\n",
      "6790\n",
      "6800\n",
      "6810\n",
      "6820\n",
      "6830\n",
      "6840\n",
      "6850\n",
      "6860\n",
      "6870\n",
      "6880\n",
      "6890\n",
      "6900\n",
      "6910\n",
      "6920\n",
      "6930\n",
      "6940\n",
      "6950\n",
      "6960\n",
      "6970\n",
      "6980\n",
      "6990\n",
      "7000\n",
      "7010\n",
      "7020\n",
      "7030\n",
      "7040\n",
      "7050\n",
      "7060\n",
      "7070\n",
      "7080\n",
      "7090\n",
      "7100\n",
      "7110\n",
      "7120\n",
      "7130\n",
      "7140\n",
      "7150\n",
      "7160\n",
      "7170\n",
      "7180\n",
      "7190\n",
      "7200\n",
      "7210\n",
      "7220\n",
      "7230\n",
      "7240\n",
      "7250\n",
      "7260\n",
      "7270\n",
      "7280\n",
      "7290\n",
      "7300\n",
      "7310\n",
      "7320\n",
      "7330\n",
      "7340\n",
      "7350\n",
      "7360\n",
      "7370\n",
      "7380\n",
      "7390\n",
      "7400\n",
      "7410\n",
      "7420\n",
      "7430\n",
      "7440\n",
      "7450\n",
      "7460\n",
      "7470\n",
      "7480\n",
      "7490\n",
      "7500\n",
      "7510\n",
      "7520\n",
      "7530\n",
      "7540\n",
      "7550\n",
      "7560\n",
      "7570\n",
      "7580\n",
      "7590\n",
      "7600\n",
      "7610\n",
      "7620\n",
      "7630\n",
      "7640\n",
      "7650\n",
      "7660\n",
      "7670\n",
      "7680\n",
      "7690\n",
      "7700\n",
      "7710\n",
      "7720\n",
      "7730\n",
      "7740\n",
      "7750\n",
      "7760\n",
      "7770\n",
      "7780\n",
      "7790\n",
      "7800\n",
      "7810\n",
      "7820\n",
      "7830\n",
      "7840\n",
      "7850\n",
      "7860\n",
      "7870\n",
      "7880\n",
      "7890\n",
      "7900\n",
      "7910\n",
      "7920\n",
      "7930\n",
      "7940\n",
      "7950\n",
      "7960\n",
      "7970\n",
      "7980\n",
      "7990\n",
      "8000\n",
      "8010\n",
      "8020\n",
      "8030\n",
      "8040\n",
      "8050\n",
      "8060\n",
      "8070\n",
      "8080\n",
      "8090\n",
      "8100\n",
      "8110\n",
      "8120\n",
      "8130\n",
      "8140\n",
      "8150\n",
      "8160\n",
      "8170\n",
      "8180\n",
      "8190\n",
      "8200\n",
      "8210\n",
      "8220\n",
      "8230\n",
      "8240\n",
      "8250\n",
      "8260\n",
      "8270\n",
      "8280\n",
      "8290\n",
      "8300\n",
      "8310\n",
      "8320\n",
      "8330\n",
      "8340\n",
      "8350\n",
      "8360\n",
      "8370\n",
      "8380\n",
      "8390\n",
      "8400\n",
      "8410\n",
      "8420\n",
      "8430\n",
      "8440\n",
      "8450\n",
      "8460\n",
      "8470\n",
      "8480\n",
      "8490\n",
      "8500\n",
      "8510\n",
      "8520\n",
      "8530\n",
      "8540\n",
      "8550\n",
      "8560\n",
      "8570\n",
      "8580\n",
      "8590\n",
      "8600\n",
      "8610\n",
      "8620\n",
      "8630\n",
      "8640\n",
      "8650\n",
      "8660\n",
      "8670\n",
      "8680\n",
      "8690\n",
      "8700\n",
      "8710\n",
      "8720\n",
      "8730\n",
      "8740\n",
      "8750\n",
      "8760\n",
      "8770\n",
      "8780\n",
      "8790\n",
      "8800\n",
      "8810\n",
      "8820\n",
      "8830\n",
      "8840\n",
      "8850\n",
      "8860\n",
      "8870\n",
      "8880\n",
      "8890\n",
      "8900\n",
      "8910\n",
      "8920\n",
      "8930\n",
      "8940\n",
      "8950\n",
      "8960\n",
      "8970\n",
      "8980\n",
      "8990\n",
      "9000\n",
      "9010\n",
      "9020\n",
      "9030\n",
      "9040\n",
      "9050\n",
      "9060\n",
      "9070\n",
      "9080\n",
      "9090\n",
      "9100\n",
      "9110\n",
      "9120\n",
      "9130\n",
      "9140\n",
      "9150\n",
      "9160\n",
      "9170\n",
      "9180\n",
      "9190\n",
      "9200\n",
      "9210\n",
      "9220\n",
      "9230\n",
      "9240\n",
      "9250\n",
      "9260\n",
      "9270\n",
      "9280\n",
      "9290\n",
      "9300\n",
      "9310\n",
      "9320\n",
      "9330\n",
      "9340\n",
      "9350\n",
      "9360\n",
      "9370\n",
      "9380\n",
      "9390\n",
      "9400\n",
      "9410\n",
      "9420\n",
      "9430\n",
      "9440\n",
      "9450\n",
      "9460\n",
      "9470\n",
      "9480\n",
      "9490\n",
      "9500\n",
      "9510\n",
      "9520\n",
      "9530\n",
      "9540\n",
      "9550\n",
      "9560\n",
      "9570\n",
      "9580\n",
      "9590\n",
      "9600\n",
      "9610\n",
      "9620\n",
      "9630\n",
      "9640\n",
      "9650\n",
      "9660\n",
      "9670\n",
      "9680\n",
      "9690\n",
      "9700\n",
      "9710\n",
      "9720\n",
      "9730\n",
      "9740\n",
      "9750\n",
      "9760\n",
      "9770\n",
      "9780\n",
      "9790\n",
      "9800\n",
      "9810\n",
      "9820\n",
      "9830\n",
      "9840\n",
      "9850\n",
      "9860\n",
      "9870\n",
      "9880\n",
      "9890\n",
      "9900\n",
      "9910\n",
      "9920\n",
      "9930\n",
      "9940\n",
      "9950\n",
      "9960\n",
      "9970\n",
      "9980\n",
      "9990\n",
      "10000\n",
      "10010\n",
      "10020\n",
      "10030\n",
      "10040\n",
      "10050\n",
      "10060\n",
      "10070\n",
      "10080\n",
      "10090\n",
      "10100\n",
      "10110\n",
      "10120\n",
      "10130\n",
      "10140\n",
      "10150\n",
      "10160\n",
      "10170\n",
      "10180\n",
      "10190\n",
      "10200\n",
      "10210\n",
      "10220\n",
      "10230\n",
      "10240\n",
      "10250\n",
      "10260\n",
      "10270\n",
      "10280\n",
      "10290\n",
      "10300\n",
      "10310\n",
      "10320\n",
      "10330\n",
      "10340\n",
      "10350\n",
      "10360\n",
      "10370\n",
      "10380\n",
      "10390\n",
      "10400\n",
      "10410\n",
      "10420\n",
      "10430\n",
      "10440\n",
      "10450\n",
      "10460\n",
      "10470\n",
      "10480\n",
      "10490\n",
      "10500\n",
      "10510\n",
      "10520\n",
      "10530\n",
      "10540\n",
      "10550\n",
      "10560\n",
      "10570\n",
      "10580\n",
      "10590\n",
      "10600\n",
      "10610\n",
      "10620\n",
      "10630\n",
      "10640\n",
      "10650\n",
      "10660\n",
      "10670\n",
      "10680\n",
      "10690\n",
      "10700\n",
      "10710\n",
      "10720\n",
      "10730\n",
      "10740\n",
      "10750\n",
      "10760\n",
      "10770\n",
      "10780\n",
      "10790\n",
      "10800\n",
      "10810\n",
      "10820\n",
      "10830\n",
      "10840\n",
      "10850\n",
      "10860\n",
      "10870\n",
      "10880\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=500\n",
    "pd.options.display.max_colwidth=1000\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "import re \n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from tabulate import tabulate, simple_separated_format\n",
    "from inflection import singularize, pluralize\n",
    "from df_to_latex import DataFrame2Latex\n",
    "from collections import Counter, defaultdict\n",
    "from inflection import singularize, pluralize\n",
    "\n",
    "from utils_path import dataset_to_respath\n",
    "# # Using WN, WN.taxonomy to retrieve path distance \n",
    "# - Pointers: \n",
    "#     - https://wn.readthedocs.io/en/latest/setup.html\n",
    "#     - https://wn.readthedocs.io/en/latest/api/wn.html\n",
    "# \n",
    "# - Installation: \n",
    "# ```\n",
    "# !pip install wn\n",
    "# !pip install wn[web]\n",
    "# wn.download('ewn:2020')\n",
    "# ```\n",
    "\n",
    "# get the co-hyponyms from WordNet (Shick and Schutze, 2020)\n",
    "# - get the hypernyms, maxinum d(x,y) is 2\n",
    "# - get the top 2 most frequent senses of each hypernym \n",
    "# - get hyponyms of each hypernyms, maxinum distance d(y,z) is 4 \n",
    "# - constrain the depeth of hypernyms to be 6\n",
    "\n",
    "# In[201]:\n",
    "\n",
    "\n",
    "import re \n",
    "import wn, wn.taxonomy\n",
    "ewn = wn.Wordnet('ewn:2020')\n",
    "\n",
    "def test_min_depth():\n",
    "    for word in ['concept', 'thought', 'living thing', 'whole', 'psychological feature', 'unit', 'artifact', 'abstraction', 'object','physical entity', 'entity']:\n",
    "        synset = wn.synsets(word, pos='n')[0]\n",
    "        min_depth = wn.taxonomy.min_depth(synset, simulate_root=False)\n",
    "        print(word, min_depth)\n",
    "\n",
    "\n",
    "def get_inherited_hypernyms(word, k_synset, max_path_hyper, min_taxo_depth=6, print_flag=False):\n",
    "    '''\n",
    "    k_synset: the most frequent k_synset of word\n",
    "    max_path_hyper: up to k level of hypernyms, e.g., 2 level higher than word \n",
    "    min_taxo_depth=6: concept, exluded hypernyms: unit, object, artifact, entity\n",
    "    '''\n",
    "    \n",
    "    hyper_synsets = []\n",
    "    for i, synset in enumerate(wn.synsets(word, pos='n')[:k_synset]): #top K senses of word \n",
    "        #print(f\"{word} synset {i+1}\")\n",
    "        for j, path in enumerate(wn.taxonomy.hypernym_paths(synset)): #retrieve the hyper path for each synset \n",
    "            #print(f\"path {j}\")\n",
    "            for i, ss in enumerate(path[:max_path_hyper]): # get the hypernyms within max_path_hyper\n",
    "                ss_min_txo_depth = wn.taxonomy.min_depth(ss, simulate_root=False)\n",
    "                \n",
    "                if ss_min_txo_depth< min_taxo_depth: continue  #remove general concepts like \"entity\", 'physical entity'\n",
    "                hyper_synsets.append(ss)\n",
    "                if print_flag: \n",
    "                    print(' ' * i, ss, ss.lemmas()[0], ss_min_txo_depth)\n",
    "                    \n",
    "    return hyper_synsets\n",
    "\n",
    "def get_direct_hyonyms(synsets):\n",
    "    '''\n",
    "    Return the direct hyponyms of a given list of synsets\n",
    "    ''' \n",
    "    sister_synsets = []\n",
    "    for synset in synsets: \n",
    "        sister_synsets.extend(synset.hyponyms() )\n",
    "    return sister_synsets\n",
    "\n",
    "\n",
    "def get_inherited_hyponyms(initial_synsets, max_path_hypo):\n",
    "    synsets = initial_synsets\n",
    "    synsets_hyponyms = []\n",
    "    \n",
    "    while max_path_hypo>0:\n",
    "        synsets = get_direct_hyonyms(synsets)\n",
    "        synsets_hyponyms.extend(synsets)\n",
    "        max_path_hypo -=1\n",
    "        #print(dist)\n",
    "        #print(synsets)\n",
    "        #print(\"-\"*80)\n",
    "    #print(Counter(synsets_all).most_common())\n",
    "    return synsets_hyponyms\n",
    "\n",
    "\n",
    "def filter_cohyponyms(word, synsets_cohyponyms, top_k=50):\n",
    "    cohyponyms = []\n",
    "    for synset in synsets_cohyponyms:\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma == word: continue \n",
    "            if len(lemma.split(\" \")) >1 or len(lemma.split(\"-\")) >1: continue \n",
    "            cohyponyms.append(lemma.lower())\n",
    "    cohyponyms = Counter(cohyponyms)\n",
    "     #if top_k !=None:        \n",
    "    return cohyponyms.most_common(top_k)\n",
    "    #else:\n",
    "    #    return dict(cohyponyms.most_common())\n",
    "\n",
    "    \n",
    "def get_cohyponyms(word, top_k_cohyonyms=50, top_k_word_synset=2, max_path_hyper=2, max_path_hypo =4, print_flag=False):\n",
    "    \n",
    "    hyper_synsets = get_inherited_hypernyms(word, k_synset=top_k_word_synset, max_path_hyper = max_path_hyper)\n",
    "    if print_flag:\n",
    "        for synset in hyper_synsets:\n",
    "            print(synset, synset.lemmas())\n",
    "\n",
    "    synsets_cohyponyms = get_inherited_hyponyms(hyper_synsets, max_path_hypo= max_path_hypo)\n",
    "\n",
    "    concept_cohyponyms = filter_cohyponyms(word, synsets_cohyponyms, top_k=top_k_cohyonyms)\n",
    "    return list(dict(concept_cohyponyms).keys())\n",
    "   \n",
    "\n",
    "def test_get_cohyponyms(word, test_cohyponyms):\n",
    "    '''\n",
    "    word = 'corn'\n",
    "    test_cohyponyms = ['bean', 'potato', 'barley', 'wheat', 'pea'] \n",
    "    word = 'train'\n",
    "    test_cohyponyms = ['bus', 'plane', 'car', 'tram', 'truck']\n",
    "    test_get_cohyponyms(word,test_cohyponyms )\n",
    "    '''\n",
    "    top_k_cohyonyms = None #200 \n",
    "    top_k_word_synset = 2\n",
    "    max_path_hyper = 2\n",
    "    max_path_hypo = 4\n",
    "\n",
    "    concept_cohyponyms  = get_cohyponyms(word, top_k_cohyonyms=top_k_cohyonyms, \n",
    "                                         top_k_word_synset=top_k_word_synset, \n",
    "                                         max_path_hyper=max_path_hyper, max_path_hypo = max_path_hypo)\n",
    "    \n",
    "    for query in test_cohyponyms:\n",
    "        if query in concept_cohyponyms:\n",
    "            print(query, 'yes')\n",
    "        else:\n",
    "            print(query, 'no')\n",
    "    print(len(concept_cohyponyms), concept_cohyponyms)\n",
    "\n",
    "\n",
    "\n",
    "# # Evaluation \n",
    "\n",
    "# In[225]:\n",
    "\n",
    "\n",
    "def merge_predictions_in_concept_level(words, uniform_funcion=None, top_k=None ):\n",
    "    '''\n",
    "    uniform_function: either signualarize or pluralize \n",
    "    '''\n",
    "    words_uniformed = [uniform_funcion(word) for word in words] if uniform_funcion !=None else words\n",
    "    concepts = list(OrderedDict.fromkeys(words_uniformed))\n",
    "    return concepts[:top_k] if top_k is not None else concepts\n",
    "\n",
    "def concept_evaluation(label, pred):\n",
    "    '''\n",
    "    \n",
    "    label: a list with the singualr and plural labels (e.g., ['tool', 'tools'])\n",
    "    pred: the top K prediction list \n",
    "\n",
    "    return:\n",
    "        1 if label share with pred else 0  \n",
    "    '''\n",
    "    if not isinstance(label, list):\n",
    "        label = eval(label)\n",
    "        \n",
    "    if not isinstance(pred, list):\n",
    "        pred = eval(pred)\n",
    "\n",
    "    shared = set(label).intersection(set(pred))\n",
    "    return 1 if len(shared)>0 else 0 \n",
    "    # return len(shared)/len(pred)\n",
    "    \n",
    "\n",
    "def get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "    '''\n",
    "    evalaute model predictions in concept level, ignoring the morphology affects (singular, plural)\n",
    "    '''\n",
    "\n",
    "    p_at_x = [] #defaultdict() \n",
    "    for pred_col in pred_cols: \n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "        prec_cur = defaultdict()\n",
    "        prec_cur['mask_type'] = suffix\n",
    "        for k in k_list: \n",
    "            df[f'p{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: concept_evaluation(x[0], eval(x[1])[:k] if isinstance(x[1], str) else x[1][:k]), axis=1 )\n",
    "            prec_cur[f'p@{k}'] = round(df[f'p{k}_{suffix}'].mean() , 3)*100\n",
    "\n",
    "        p_at_x.append(prec_cur)  \n",
    "        \n",
    "\n",
    "    # aggregate the average precision across k \n",
    "    df_res = pd.DataFrame(p_at_x) #, columns=['mask_type', 'mAP'])\n",
    "    df_res['relation'] = [relation]*len(df_res)\n",
    "    return df_res\n",
    "\n",
    "def get_highest_mrr_among_labels(label, pred):\n",
    "    '''\n",
    "    return the highest rank among the multiple labels. This is applicable to single labels as well, if we the single label is put in a list\n",
    "\n",
    "    pred: a list of words (candidates)\n",
    "    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)\n",
    "    '''\n",
    "    mrr = 0 \n",
    "    if pred is None: return mrr \n",
    "\n",
    "    rank_list = [ pred.index(item) + 1 for item in label if item in pred] \n",
    "    if len(rank_list)>0:\n",
    "        mrr = 1/min(rank_list)\n",
    "\n",
    "    return mrr \n",
    "\n",
    "\n",
    "def get_mrr(df, relation, pred_cols, label_col, pred_col_suffix):\n",
    "    '''\n",
    "    mrr is calculated based on the top_k rank, all elements in obj_col are used\n",
    "    '''\n",
    "\n",
    "    mrr = [] \n",
    "    for i, pred_col in enumerate(pred_cols):\n",
    "        cur_mrr = defaultdict()\n",
    "        suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "\n",
    "        df[f'mrr_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: get_highest_mrr_among_labels(x[0], x[1]), axis=1 ) \n",
    "        \n",
    "        cur_mrr['mask_type'] = suffix\n",
    "        cur_mrr[f\"mrr\"] = round(df[f'mrr_{suffix}'].mean(), 3)*100\n",
    "        mrr.append(cur_mrr)\n",
    "\n",
    "    mrr_df =  pd.DataFrame(data = mrr) #, columns=['mask_type', 'mrr'])\n",
    "    # mrr_df['mask_type']= mrr_df['mask_type'].apply(lambda x: x.replace(\"\"))\n",
    "    mrr_df['relation'] = relation\n",
    "    return mrr_df \n",
    "\n",
    "\n",
    "# In[202]:\n",
    "\n",
    "\n",
    "def get_dataset_to_respath(dataset_to_respath, print_flag=False):\n",
    "    # remote path \n",
    "#     dataset_to_respath = {'hypernymsuite-BLESS': 'log/bert-large-uncased/hypernymsuite/BLESS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'lm_diagnostic_extended-singular': 'log/bert-large-uncased/lm_diagnostic_extended/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.LM_DIAGNOSTIC_EXTENDED.csv', 'clsb-singular': 'log/bert-large-uncased/clsb/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.CLSB.csv', 'hypernymsuite-LEDS': 'log/bert-large-uncased/hypernymsuite/LEDS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'hypernymsuite-EVAL': 'log/bert-large-uncased/hypernymsuite/EVAL/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'hypernymsuite-SHWARTZ': 'log/bert-large-uncased/hypernymsuite/SHWARTZ/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv'}\n",
    "\n",
    "    source_dir = 'spartan:~/cogsci/DAP/'\n",
    "    target_dir = '../../'\n",
    "    dataset_to_localpath = defaultdict()\n",
    "    dataset_rename = {\n",
    "        'hypernymsuite-BLESS': 'BLESS', 'lm_diagnostic_extended-singular': 'DIAG', 'clsb-singular':'CLSB', 'hypernymsuite-LEDS': 'LEDS', 'hypernymsuite-EVAL': 'EVAL', 'hypernymsuite-SHWARTZ': \n",
    "        \"SHWARTZ\"\n",
    "    }\n",
    "    for dataset, path in dataset_to_respath.items():\n",
    "        path = path.replace(\".tsv\", \".csv\")\n",
    "        source_path = source_dir + path \n",
    "        dataset_l1 = dataset.split(\"-\")[0]\n",
    "        dataset_l2 = dataset.split(\"-\")[1] \n",
    "        target_path = target_dir + path\n",
    "        scp_string = f\"!scp {source_path} {target_path}\"\n",
    "        if print_flag:\n",
    "            print(scp_string)\n",
    "            print()\n",
    "#         print(target_path)\n",
    "        dataset_to_localpath[dataset_rename[dataset]] = target_path \n",
    "#     print(dataset_to_localpath)\n",
    "    return dataset_to_localpath\n",
    "dataset_to_localpath = get_dataset_to_respath(dataset_to_respath)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet \n",
    "wn_lemmas = set(wordnet.all_lemma_names())\n",
    "def check_word_in_wordnet(word, wn_lemmas):\n",
    "    '''\n",
    "    1 if word in wordnet else 0\n",
    "    '''\n",
    "    return 1 if word in wn_lemmas else 0 \n",
    "\n",
    "\n",
    "def get_all_vocab(dataset_to_localpath):\n",
    "    dataset_to_df = defaultdict()\n",
    "    vocab_sub = set()\n",
    "    for dataset, path in dataset_to_localpath.items(): \n",
    "        if debug:\n",
    "           if dataset!='DIAG': continue \n",
    "        print(\"dataset\", dataset)\n",
    "        df = pd.read_csv(path)\n",
    "        vocab_sub.update(df['sub_label_sg'].to_list())\n",
    "        print(len(vocab_sub))\n",
    "    return list(vocab_sub)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Config for getting co-hyponyms from WordNet\n",
    "top_k_cohyonyms = None #200 \n",
    "top_k_word_synset = 2\n",
    "max_path_hyper = 2\n",
    "max_path_hypo = 4\n",
    "\n",
    "#config for evaluation \n",
    "pred_col_suffix=''\n",
    "label_col = 'sub_sister_new'\n",
    "pred_cols = ['subj_anchors_all_sg']\n",
    "relation='co-hyponyms'\n",
    "debug= False #True #eval(sys.argv[1])\n",
    "print(\"debug\", debug)\n",
    "\n",
    "# dataset = sys.argv[2]\n",
    "# path = sys.argv[3]\n",
    "\n",
    "for dataset, path in dataset_to_localpath.items(): \n",
    "    if dataset!='SHWARTZ':continue\n",
    "    vocab_sub = set()\n",
    "    if debug:\n",
    "       if dataset!='DIAG': continue \n",
    "    print(\"dataset\", dataset)\n",
    "    df = pd.read_csv(path)\n",
    "    vocab_sub.update(df['sub_label_sg'].to_list())\n",
    "    print(len(vocab_sub))\n",
    "\n",
    "    word_to_cohyponyms = defaultdict(list)\n",
    "    for i, word in enumerate(vocab_sub):\n",
    "        if i%10==0: print(i)\n",
    "        if not check_word_in_wordnet(word, wn_lemmas):\n",
    "            word_to_cohyponyms[word] = []\n",
    "        else:\n",
    "            cohyponyms = get_cohyponyms(word, top_k_cohyonyms=top_k_cohyonyms, \n",
    "                                     top_k_word_synset=top_k_word_synset, \n",
    "                                     max_path_hyper=max_path_hyper, \n",
    "                                     max_path_hypo = max_path_hypo)\n",
    "            word_to_cohyponyms[word] = cohyponyms\n",
    "        if debug:\n",
    "            print(word, cohyponyms)\n",
    "            \n",
    "    if i%100==0:\n",
    "        df = pd.DataFrame(word_to_cohyponyms.items(), columns=['word', 'cohyponyms'])\n",
    "        output_path = f'../log/{dataset}_word_to_cohyponyms.txt'\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"save {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_to_cohyponyms.items(), columns=['word', 'cohyponyms'])\n",
    "output_path = f'../log/{dataset}_word_to_cohyponyms.txt'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"save {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -u get_cohyponyms_dataset.py  BLESS ../../log/bert-large-uncased/hypernymsuite/BLESS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.HYPERNYMSUITE.csv\n",
      "python -u get_cohyponyms_dataset.py  DIAG ../../log/bert-large-uncased/lm_diagnostic_extended/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.LM_DIAGNOSTIC_EXTENDED.csv\n",
      "python -u get_cohyponyms_dataset.py  CLSB ../../log/bert-large-uncased/clsb/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.CLSB.csv\n",
      "python -u get_cohyponyms_dataset.py  LEDS ../../log/bert-large-uncased/hypernymsuite/LEDS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.HYPERNYMSUITE.csv\n",
      "python -u get_cohyponyms_dataset.py  EVAL ../../log/bert-large-uncased/hypernymsuite/EVAL/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.HYPERNYMSUITE.csv\n",
      "python -u get_cohyponyms_dataset.py  SHWARTZ ../../log/bert-large-uncased/hypernymsuite/SHWARTZ/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False.HYPERNYMSUITE.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset, path in dataset_to_localpath.items(): \n",
    "    print(\"python -u get_cohyponyms_dataset.py \", dataset, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug False\n",
      "dataset BLESS\n",
      "626\n",
      "dataset DIAG\n",
      "1561\n",
      "dataset CLSB\n",
      "1962\n",
      "dataset LEDS\n",
      "3782\n",
      "dataset EVAL\n",
      "4250\n",
      "dataset SHWARTZ\n",
      "8351\n",
      "#vocab 8351 ['spa', 'poles', 'lovers', 'moderate', 'talk']\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "save ../log/word_to_singular.txt\n"
     ]
    }
   ],
   "source": [
    "from utils_path import dataset_to_respath \n",
    "\n",
    "def get_dataset_to_respath(dataset_to_respath, print_flag=False):\n",
    "    # remote path \n",
    "#     dataset_to_respath = {'hypernymsuite-BLESS': 'log/bert-large-uncased/hypernymsuite/BLESS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'lm_diagnostic_extended-singular': 'log/bert-large-uncased/lm_diagnostic_extended/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.LM_DIAGNOSTIC_EXTENDED.csv', 'clsb-singular': 'log/bert-large-uncased/clsb/singular/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.CLSB.csv', 'hypernymsuite-LEDS': 'log/bert-large-uncased/hypernymsuite/LEDS/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'hypernymsuite-EVAL': 'log/bert-large-uncased/hypernymsuite/EVAL/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv', 'hypernymsuite-SHWARTZ': 'log/bert-large-uncased/hypernymsuite/SHWARTZ/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_10_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_True_cpt_False.HYPERNYMSUITE.csv'}\n",
    "\n",
    "    source_dir = 'spartan:~/cogsci/DAP/'\n",
    "    target_dir = '../../'\n",
    "    dataset_to_localpath = defaultdict()\n",
    "    dataset_rename = {\n",
    "        'hypernymsuite-BLESS': 'BLESS', 'lm_diagnostic_extended-singular': 'DIAG', 'clsb-singular':'CLSB', 'hypernymsuite-LEDS': 'LEDS', 'hypernymsuite-EVAL': 'EVAL', 'hypernymsuite-SHWARTZ': \n",
    "        \"SHWARTZ\"\n",
    "    }\n",
    "    for dataset, path in dataset_to_respath.items():\n",
    "        path = path.replace(\".tsv\", \".csv\")\n",
    "        source_path = source_dir + path \n",
    "        dataset_l1 = dataset.split(\"-\")[0]\n",
    "        dataset_l2 = dataset.split(\"-\")[1] \n",
    "        target_path = target_dir + path\n",
    "        scp_string = f\"!scp {source_path} {target_path}\"\n",
    "        if print_flag:\n",
    "            print(scp_string)\n",
    "            print()\n",
    "#         print(target_path)\n",
    "        dataset_to_localpath[dataset_rename[dataset]] = target_path \n",
    "#     print(dataset_to_localpath)\n",
    "    return dataset_to_localpath\n",
    "dataset_to_localpath = get_dataset_to_respath(dataset_to_respath)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet \n",
    "wn_lemmas = set(wordnet.all_lemma_names())\n",
    "def check_word_in_wordnet(word, wn_lemmas):\n",
    "    '''\n",
    "    1 if word in wordnet else 0\n",
    "    '''\n",
    "    return 1 if word in wn_lemmas else 0 \n",
    "\n",
    "\n",
    "def get_all_vocab(dataset_to_localpath):\n",
    "    dataset_to_df = defaultdict()\n",
    "    vocab = set()\n",
    "    for dataset, path in dataset_to_localpath.items(): \n",
    "        if debug:\n",
    "           if dataset!='DIAG': continue \n",
    "        print(\"dataset\", dataset)\n",
    "        df = pd.read_csv(path)\n",
    "        df['subj_anchors'] = df['subj_anchors'].apply(lambda x: eval(x))\n",
    "        for anchors in df['subj_anchors'].to_list():\n",
    "            vocab.update(anchors)\n",
    "        print(len(vocab))\n",
    "    return list(vocab)\n",
    "\n",
    "#config for evaluation \n",
    "pred_col_suffix=''\n",
    "label_col = 'sub_sister_new'\n",
    "pred_cols = ['subj_anchors_all_sg']\n",
    "relation='co-hyponyms'\n",
    "debug= False #True #eval(sys.argv[1])\n",
    "print(\"debug\", debug)\n",
    "from inflection import singularize, pluralize \n",
    "\n",
    "vocab = get_all_vocab(dataset_to_localpath)\n",
    "print(\"#vocab\", len(vocab), vocab[:5])\n",
    "word_to_singular = defaultdict(list)\n",
    "for i, word in enumerate(vocab):\n",
    "    if i%1000==0: print(i)\n",
    "    word_to_singular[word] = singularize(word)\n",
    "    if debug:\n",
    "        print(word, word_to_singular)\n",
    "       \n",
    "\n",
    "df = pd.DataFrame(word_to_singular.items(), columns=['word', 'singular'])\n",
    "output_path = '../log/word_to_singular.txt'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"save {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 (default, Aug 13 2020, 07:46:32) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
