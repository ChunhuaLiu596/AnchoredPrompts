{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../../data/swow/S_RW.R123.csv to get similar words ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_to_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e790407b35ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/swow/swow.en.similar_words.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mdic_sub_to_anchors_singular\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdic_sub_to_anchors_plural\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_anchors_from_swow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_sub_to_anchors_singular\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdic_sub_to_anchors_plural\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e790407b35ac>\u001b[0m in \u001b[0;36msave_anchors_from_swow\u001b[0;34m(json_path, path_rw, debug)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_anchors_from_swow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/swow/swow.en.similar_words.json'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpath_rw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../data/swow/S_RW.R123.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mcue_to_similar_words_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cue_to_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_rw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#cue_to_similar_words_score= {'cartoon': cue_to_similar_words_score['cartoon']}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e790407b35ac>\u001b[0m in \u001b[0;36mload_cue_to_similar_words\u001b[0;34m(path, path_rw)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading {path_rw} to get similar words ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mcue_to_similar_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_rw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0msave_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcue_to_similar_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcue_to_similar_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e790407b35ac>\u001b[0m in \u001b[0;36mget_similar_words\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mword_to_pos_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sorting similar words ..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcue_to_similar_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_pos_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_to_pos' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "from collections import Counter, defaultdict \n",
    "import os, sys \n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from inflection import singularize,  pluralize \n",
    "import spacy\n",
    "# import pyinflect\n",
    "# import lemminflect\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def word_to_pos(words):\n",
    "    word_to_pos_dict = defaultdict()\n",
    "    for word in words:\n",
    "        if word == 'null': continue\n",
    "        pos = nlp(str(word))[0].pos_ \n",
    "        word_to_pos_dict[word] = pos\n",
    "    return word_to_pos_dict\n",
    "\n",
    "\n",
    "def sort_similar_words(df, word_to_pos_dict=None, top_k = 10):\n",
    "    '''\n",
    "    sort simialr words and return the top_k\n",
    "    '''\n",
    "    cue_to_similar_words = defaultdict()\n",
    "    for i, (k,v) in enumerate(df.to_dict().items()):\n",
    "        if k=='null': continue\n",
    "        #k_pos = word_to_pos_dict.get(k)\n",
    "        #if k_pos !='NOUN': continue \n",
    "\n",
    "        v_sorted = Counter(v).most_common()\n",
    "        v_sorted_noun = []\n",
    "        for v in v_sorted:\n",
    "            if len( str(v[0]).split())>1: continue \n",
    "            #v_pos = word_to_pos_dict.get(v[0])\n",
    "            #if v_pos!='NOUN': continue \n",
    "            v_sorted_noun.append(v)\n",
    "        cue_to_similar_words[k] = dict(v_sorted_noun[1:top_k+1])\n",
    "    print('dog: ', cue_to_similar_words['dog'])\n",
    "    return cue_to_similar_words\n",
    "\n",
    "# path = '../output/2018/S_RW.R1.csv'\n",
    "\n",
    "\n",
    "def get_similar_words(path):\n",
    "    \n",
    "    df1 = pd.read_csv(path)\n",
    "\n",
    "    df1.index = df1[\"Unnamed: 0\"]\n",
    "    df1 = df1.drop(columns=[\"Unnamed: 0\"], axis=1)\n",
    "    #word_to_pos_dict = word_to_pos(list(df1.index))\n",
    "    print(\"sorting similar words ..\")\n",
    "    cue_to_similar_words = sort_similar_words(df1) #1.head(20)) #, word_to_pos_dict )\n",
    "    return cue_to_similar_words \n",
    "    #cue_to_similar_words['bicycle']\n",
    "# cue_to_similar_words.keys()\n",
    "\n",
    "\n",
    "def load_cue_to_similar_words(path, path_rw='../../data/S_RW.R123.csv'):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"loading {path}...\")\n",
    "        cue_to_similar_words = json.load(open(path))\n",
    "    else:\n",
    "        print(f\"loading {path_rw} to get similar words ...\")\n",
    "        cue_to_similar_words = get_similar_words(path_rw)\n",
    "        save_similar_words(cue_to_similar_words, path)\n",
    "    return cue_to_similar_words \n",
    "\n",
    "\n",
    "def save_similar_words(cue_to_similar_words, output_file, indent=4):\n",
    "    \n",
    "    with open(output_file, 'w') as fout:\n",
    "        json.dump(cue_to_similar_words, fout, indent=indent)\n",
    "    print(f\"save {output_file}\")\n",
    "    \n",
    "def save_anchors_from_swow(json_path = '../../data/swow/swow.en.similar_words.json',  path_rw='../../data/swow/S_RW.R123.csv', debug=False):\n",
    "    cue_to_similar_words_score = load_cue_to_similar_words(json_path, path_rw)\n",
    "    \n",
    "    vocab_cues = set(cue_to_similar_words_score.keys())\n",
    "    vocab_res = set()\n",
    "    for k,v in cue_to_similar_words_score.items():\n",
    "        vocab_res.update(v.keys() )\n",
    "    vocab = vocab_cues.union(vocab_res)\n",
    "    vocab_to_singular = {k: singularize(k) for k in vocab}\n",
    "    vocab_to_plural = {k: pluralize(k) for k in vocab}\n",
    "\n",
    "    cue_to_similar_words_score_sgpl = defaultdict()\n",
    "    cue_to_similar_words_score_sg = defaultdict()\n",
    "    for k,v in cue_to_similar_words_score.items():\n",
    "        k = vocab_to_singular.get(k)\n",
    "        v = { vocab_to_singular.get(k1): v1 for k1, v1 in v.items()}\n",
    "        cue_to_similar_words_score_sg[k] = v \n",
    "        cue_to_similar_words_score_sgpl[k] =v \n",
    "\n",
    "    cue_to_similar_words_score_pl = defaultdict()\n",
    "    for k,v in cue_to_similar_words_score.items():\n",
    "        k = vocab_to_plural.get(k)\n",
    "        v = { vocab_to_plural.get(k1): v1 for k1, v1 in v.items()}\n",
    "        cue_to_similar_words_score_pl[k] = v \n",
    "        cue_to_similar_words_score_sgpl[k] =v \n",
    "\n",
    "    return dic_sub_to_anchors_singular,  dic_sub_to_anchors_plural\n",
    "\n",
    "\n",
    "def save_foward_associaitons_from_swow():\n",
    "    path = '../../data/swow/strength.SWOW-EN.R123.csv'\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df.head()\n",
    "\n",
    "    df['cue'] = df['cue'].apply(lambda x: str(x).strip().lower())\n",
    "    df['response'] = df['response'].apply(lambda x: str(x).strip().lower())\n",
    "    df['cue_token_num'] = df['cue'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['response_token_num'] = df['response'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df = df.query(\"response_token_num ==1 and cue_token_num==1\")    \n",
    "\n",
    "    cue_association_dict = defaultdict()\n",
    "    for name, group in df.groupby(['cue']):\n",
    "        cue = name\n",
    "        response_strength =  dict(zip(group['response'], group['R123.Strength']))\n",
    "        cue_association_dict[cue] = response_strength\n",
    "\n",
    "    vocab_cues = set(cue_association_dict.keys())\n",
    "    vocab_res = set()\n",
    "    for k,v in cue_association_dict.items():\n",
    "        vocab_res.update(v.keys() )\n",
    "\n",
    "    vocab = vocab_cues.union(vocab_res)\n",
    "    vocab_to_singular = {k: singularize(k) for k in vocab}\n",
    "    vocab_to_plural = {k: pluralize(k) for k in vocab}\n",
    "\n",
    "    cue_association_dict_sgpl = defaultdict()\n",
    "    for k,v in cue_association_dict.items():\n",
    "        cue_association_dict_sgpl[vocab_to_singular[k]] = v\n",
    "        cue_association_dict_sgpl[vocab_to_plural[k]] = v \n",
    "\n",
    "\n",
    "    output_file = '../../data/swow/swow.en.strength.R123.json'\n",
    "    with open(output_file, 'w') as fout:\n",
    "        json.dump(cue_association_dict_sgpl, fout, indent=4)\n",
    "    print(f\"save {output_file}\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     association_type =  sys.argv[1]\n",
    "if association_type == 'similar':\n",
    "    print(\"generating the related/similar words in swow\")\n",
    "    json_path = '../../data/swow/swow.en.similar_words.json'\n",
    "    dic_sub_to_anchors_singular,  dic_sub_to_anchors_plural = save_anchors_from_swow(json_path)\n",
    "\n",
    "    # cue_to_similar_words_score = load_cue_to_similar_words(json_path, path_rw='../../data/S_RW.R123.csv')\n",
    "    print(\"test: \")\n",
    "    #print( dic_sub_to_anchors_singular['bicycle'] )\n",
    "    #print( dic_sub_to_anchors_plural['bicycles'] )\n",
    "    print( dic_sub_to_anchors_singular['cartoon'] )\n",
    "    print( dic_sub_to_anchors_plural['cartoons'] )\n",
    "elif association_type == 'strength':\n",
    "    print('generating forward associations')\n",
    "    save_foward_associaitons_from_swow()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save ../../data/swow/swow.en.strength.R123.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../../data/swow/swow.en.similar_words.json...\n"
     ]
    }
   ],
   "source": [
    "json_path = '../../data/swow/swow.en.similar_words.json'\n",
    "path_rw='../../data/swow/S_RW.R123.csv'\n",
    "\n",
    "cue_to_similar_words_score = load_cue_to_similar_words(json_path, path_rw)\n",
    "    \n",
    "#cue_to_similar_words_score= {'cartoon': cue_to_similar_words_score['cartoon']}\n",
    "\n",
    "#cue_debug = ['cartoon', 'animal', 'dog']\n",
    "# cue_to_similar_words = defaultdict()\n",
    "# for k,v in cue_to_similar_words_score.items():\n",
    "#     #if k in cue_debug:\n",
    "#     cue_to_similar_words[k]= list(v.keys()) \n",
    "\n",
    "# add a vocab, voab_to_singular, vocab_to_plural to save time\n",
    "#  \n",
    "\n",
    "# return dic_sub_to_anchors_singular,  dic_sub_to_anchors_plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([list(v.keys())  for v in cue_to_similar_words.values() for x in ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save ../../data/swow/swow.en.similar_words.sg.json\n",
      "save ../../data/swow/swow.en.similar_words.pl.json\n",
      "save ../../data/swow/swow.en.similar_words.sgpl.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "save_similar_words(cue_to_similar_words_score_sg, json_path.replace(\".json\", \".sg.json\"), indent=4)\n",
    "save_similar_words(cue_to_similar_words_score_pl, json_path.replace(\".json\", \".pl.json\"), indent=4)\n",
    "save_similar_words(cue_to_similar_words_score_sgpl, json_path.replace(\".json\", \".sgpl.json\"), indent=4)\n",
    "\n",
    "# print(cue_to_similar_words_score_sgpl['rabbits'])\n",
    "# print(cue_to_similar_words_score_sgpl['rabbit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dic_sub_to_anchors_singular = {vocab_to_singular(k): {vocab_to_singular(x) for k1,v1 in v.items()} for k,v in deepcopy(cue_to_similar_words_score).items()}\n",
    "dic_sub_to_anchors_plural = {vocab_to_plural(k): [vocab_to_plural(x) for x in v] for k,v in deepcopy(cue_to_similar_words).items()}\n",
    "\n",
    "dic_sub_to_anchors_singular = {singularize(k): [singularize(x) for x in v] for k,v in deepcopy(cue_to_similar_words).items()}\n",
    "dic_sub_to_anchors_plural = {pluralize(k): [pluralize(x) for x in v] for k,v in deepcopy(cue_to_similar_words).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rw='../../data/swow/S_RW.R123.csv'\n",
    "df1 = pd.read_csv(path_rw)\n",
    "df1.index = df1[\"Unnamed: 0\"]\n",
    "df1 = df1.drop(columns=[\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "# return cue_to_similar_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting similar words ..\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'umpire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1a7017c4138f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mword_to_pos_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sorting similar words ..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcue_to_similar_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_pos_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-9ba8a500a1b3>\u001b[0m in \u001b[0;36msort_similar_words\u001b[0;34m(df, word_to_pos_dict, top_k)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'null'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mk_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_pos_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#if k_pos !='NOUN': continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'umpire'"
     ]
    }
   ],
   "source": [
    "def word_to_pos(words):\n",
    "    word_to_pos_dict = defaultdict()\n",
    "    for word in words:\n",
    "        if word=='null': continue \n",
    "        pos = nlp(str(word))[0].pos_ \n",
    "        word_to_pos_dict[word] = pos\n",
    "    return word_to_pos_dict\n",
    "\n",
    "word_to_pos_dict = word_to_pos(list(df1.index))\n",
    "print(\"sorting similar words ..\")\n",
    "cue_to_similar_words = sort_similar_words(df1.head(20), word_to_pos_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.377987526653529\n",
      "12215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_swow_score(cue, response, score_dict):\n",
    "    if cue in score_dict and response in score_dict[cue]:\n",
    "        return score_dict[cue][response]\n",
    "    return 0\n",
    "\n",
    "path = '../../data/swow/swow.en.strength.R123.json'\n",
    "path = '../../data/swow/swow.en.similar_words.json'\n",
    "\n",
    "score_dict = json.load(open(path))\n",
    "score = get_swow_score('dog', 'cat', score_dict)\n",
    "\n",
    "\n",
    "print(score)\n",
    "# score_dict['abdomen']\n",
    "print (len( list(score_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if 'dogs' in score_dict:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "# len(word_to_pos_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_label</th>\n",
       "      <th>subj_anchors</th>\n",
       "      <th>subj_anchors_swow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alligators</td>\n",
       "      <td>['snakes', 'turtles', 'frogs', 'lizards', 'sha...</td>\n",
       "      <td>['crocodile', 'reptile', 'reptile', 'cayman', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alligators</td>\n",
       "      <td>['snakes', 'turtles', 'frogs', 'lizards', 'sha...</td>\n",
       "      <td>['crocodile', 'reptile', 'reptile', 'cayman', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ambulances</td>\n",
       "      <td>['police', 'buses', 'helicopters', 'cars', 'em...</td>\n",
       "      <td>['emergency', 'medic', 'hospital', 'firetruck'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ambulances</td>\n",
       "      <td>['police', 'buses', 'helicopters', 'cars', 'em...</td>\n",
       "      <td>['emergency', 'medic', 'hospital', 'firetruck'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anchors</td>\n",
       "      <td>['reporters', 'ropes', 'pilots', 'boats', 'pro...</td>\n",
       "      <td>['nautical', 'sailboat', 'sail', 'boat', 'ship...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sub_label                                       subj_anchors  \\\n",
       "0  alligators  ['snakes', 'turtles', 'frogs', 'lizards', 'sha...   \n",
       "1  alligators  ['snakes', 'turtles', 'frogs', 'lizards', 'sha...   \n",
       "2  ambulances  ['police', 'buses', 'helicopters', 'cars', 'em...   \n",
       "3  ambulances  ['police', 'buses', 'helicopters', 'cars', 'em...   \n",
       "4     anchors  ['reporters', 'ropes', 'pilots', 'boats', 'pro...   \n",
       "\n",
       "                                   subj_anchors_swow  \n",
       "0  ['crocodile', 'reptile', 'reptile', 'cayman', ...  \n",
       "1  ['crocodile', 'reptile', 'reptile', 'cayman', ...  \n",
       "2  ['emergency', 'medic', 'hospital', 'firetruck'...  \n",
       "3  ['emergency', 'medic', 'hospital', 'firetruck'...  \n",
       "4  ['nautical', 'sailboat', 'sail', 'boat', 'ship...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../../log/bert-large-uncased/clsb/swow_rw/exp_data_results_anchor_type_Coordinate_remove_Y_PUNC_FULL_concate_or_single_max_anchor_num_5_anchor_scorer_probAvg_filter_obj_True_filter_objects_with_input_True_wnp_False_cpt_False_anchor_source_LM_swow_score_source_SWOWSimilar.CLSB.csv'\n",
    "dfp = pd.read_csv(path)\n",
    "dfp[['sub_label', 'subj_anchors', 'subj_anchors_swow']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e765ed476e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdict2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/swow/swow.en.similar_words.pl.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdict_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict1\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mdict2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdict2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rabbits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "# cue_to_similar_words_score = load_cue_to_similar_words(json_path, path_rw='../../data/S_RW.R123.csv')\n",
    "\n",
    "# print( dic_sub_to_anchors_singular['bicycle'] )\n",
    "# print( dic_sub_to_anchors_plural['bicycles'] )\n",
    "# coat\n",
    "\n",
    "dict1 = json.load(open('../../data/swow/swow.en.similar_words.sg.json'))\n",
    "dict2 = json.load(open('../../data/swow/swow.en.similar_words.pl.json'))\n",
    "\n",
    "# dict_all = dict1+dict2 \n",
    "print( len(dict_all.keys ) )\n",
    "dict2['rabbits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_num = 5\n",
    "k='absent'\n",
    "cue_to_similar_words= {k: [str(x[0]) for x in cue_to_similar_words_score[k]] }\n",
    "print(cue_to_similar_words)\n",
    "dic_sub_to_anchors_singular = {singularize(k): [singularize(x) for x in v[:anchor_num]] for k,v in cue_to_similar_words.items()}\n",
    "dic_sub_to_anchors_plural = {pluralize(k): [pluralize(x) for x in v[:anchor_num]] for k,v in cue_to_similar_words.items()}\n",
    "print(dic_sub_to_anchors_singular)\n",
    "print(dic_sub_to_anchors_plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sub_to_anchors_singular,  dic_sub_to_anchors_plural = read_anchors_from_swow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic_sub_to_anchors_singular['bicyble'])\n",
    "print(\"\\n\")\n",
    "print(dic_sub_to_anchors_plural['bicycles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_to_similar_words_score = load_cue_to_similar_words(json_path, path_rw='../../data/swow/S_RW.R123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in cue_to_similar_words_score.items():\n",
    "    for x in v:\n",
    "        if isinstance(str(x[0]), float):\n",
    "            print(k,x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absent\n",
    "cue_to_similar_words_score['absent']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Vocab: 800071\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>MEDI</td>\n",
       "      <td>UNSEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_freq_level</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0    1     2       3\n",
       "index           HIGH  LOW  MEDI  UNSEEN\n",
       "sub_freq_level   1.0  0.0   0.0     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import pyinflect\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "word = 'yes'\n",
    "\n",
    "def pluralize_test(word):\n",
    "    '''\n",
    "    sg is None if the word is not in the vocab \n",
    "    '''\n",
    "    \n",
    "    pl= nlp(word)[0]._.inflect('NNS')\n",
    "    return pl if pl is not None else word\n",
    "\n",
    "pl = pluralize_test(word)\n",
    "# print(pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluralize(word):\n",
    "    pl = nlp(word)\n",
    "    if pl[0].pos_ not in [\"NOUN\"] or x.tag_ == 'NNS':\n",
    "        return word\n",
    "    else:\n",
    "        return pl[0]._.inflect('NNS')\n",
    "\n",
    "def singularize(word):\n",
    "    '''\n",
    "    sg is None if the word is not in the vocab \n",
    "    '''\n",
    "    sg =  nlp(word) #[0]._.inflect('NN')\n",
    "\n",
    "    if sg[0].pos_ not in [\"NOUN\"] or x.tag_ == 'NN':\n",
    "        return word\n",
    "    else:\n",
    "        return sg[0]._.inflect('NN')\n",
    "    \n",
    "    return sg if sg is not None else word\n",
    "\n",
    "word = 'flower'\n",
    "print( pluralize_test2(word) )\n",
    "print( singularize_test2(word) )\n",
    "pl = nlp(word)\n",
    "for x in pl:\n",
    "    print(x, x.pos_, x.lemma_, x.tag_, x._.inflect('NNS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying datapoins from five datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4549\n",
      "11292\n",
      "reading cohyponyms: ../log/word_to_cohyponyms.txt\n",
      "#instances shared in SWOW and BERT 2094\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter \n",
    "import pandas as pd\n",
    "import json \n",
    "import os, sys \n",
    "\n",
    "def read_cohyponyms(path = '../log/word_to_cohyponyms.txt'):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"reading cohyponyms: {path}\")\n",
    "        df = pd.read_csv(path)\n",
    "        df['cohyponyms'] = df['cohyponyms'].apply(lambda x: eval(x))\n",
    "        word_to_cohyponym = dict(zip(df['word'].to_list(), df['cohyponyms'].to_list()))\n",
    "        return word_to_cohyponym\n",
    "    print(f\"{path} not found\")\n",
    "    \n",
    "def read_bert_vocab(bert_vocab_path = 'data/bert-large-uncased-vocab.txt'):\n",
    "    \n",
    "    vocab = set()\n",
    "    with open(bert_vocab_path, 'r') as fin: \n",
    "        lines = fin.readlines()\n",
    "        for line in lines: \n",
    "            line = line.strip()\n",
    "            vocab.add(line)\n",
    "    return vocab        \n",
    "\n",
    "\n",
    "def save_dict_to_json(examples, output_path):\n",
    "    ''' \n",
    "    save a list of dicts into otuput_path, orient='records' (each line is a dict) \n",
    "    examples: a list of dicts\n",
    "    output_path: \n",
    "    '''\n",
    "    with open(output_path, 'w') as fout:\n",
    "        for example in examples:\n",
    "            json.dump(example, fout)\n",
    "            fout.write(\"\\n\")\n",
    "        print(f\"save {output_path} with {len(examples)} lines\")\n",
    "        \n",
    "        \n",
    "dataset_to_orig_paths = {\n",
    "    'CLSB': '../../data/clsb/singular/IsA.jsonl',\n",
    "    'BLESS': '../../data/hypernymsuite/BLESS/IsA.jsonl',\n",
    "    'EVAL': '../../data/hypernymsuite/EVAL/IsA.jsonl',\n",
    "    'LEDS': '../../data/hypernymsuite/LEDS/IsA.jsonl',\n",
    "    'SHWARTZ': '../../data/hypernymsuite/SHWARTZ/IsA.jsonl',\n",
    "    'DIAG': '../../data/lm_diagnostic_extended/singular/IsA.jsonl'\n",
    "}\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as fin:\n",
    "        data = fin.readlines()\n",
    "        data = [eval(x) for x in data]\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df['obj_label'] = df['obj_label'].apply(lambda x: x if isinstance(x, str) else x[0])\n",
    "        return df \n",
    "\n",
    "dataset_to_df = defaultdict()\n",
    "for dataset, path in dataset_to_orig_paths.items():\n",
    "    df = load_data(path)\n",
    "    #display (df.head())\n",
    "    dataset_to_df[dataset] = df \n",
    "    \n",
    "atom_datasets = ['BLESS', 'DIAG', 'CLSB', 'LEDS','EVAL'] #, 'SHWARTZ'] exluding SHWARTZ as it's too noisy \n",
    "dataset_to_df['ALL'] = pd.concat([dataset_to_df[dataset] for dataset in atom_datasets], axis=0)\n",
    "dataset_to_df['ALL'] = dataset_to_df['ALL'][['sub_label', 'obj_label', 'relation']].drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "dataset_to_df['ALL'].head()\n",
    "print(len(dataset_to_df['ALL'].index))\n",
    "bert_vocab= read_bert_vocab(bert_vocab_path = '../../data/bert-large-uncased-vocab.txt')\n",
    "\n",
    "\n",
    "# path = '../../data/swow/swow.en.fbs.json'\n",
    "path = '../../data/swow/swow.en.strength.R123.json'\n",
    "# path = \"../../data/swow/vocab_to_plural.json\"\n",
    "\n",
    "swow_cue_vocab = list(json.load(open(path, 'r')).keys())\n",
    "print(len(swow_cue_vocab))\n",
    "\n",
    "word_to_cohyponyms = read_cohyponyms(path = '../log/word_to_cohyponyms.txt')\n",
    "\n",
    "########## filter the subjects and objects \n",
    "dataset_to_df['ALL']['obj_in_BERT'] = dataset_to_df['ALL']['obj_label'].apply(lambda x: 1 if x in bert_vocab else 0)\n",
    "df = dataset_to_df['ALL'].query(f\"sub_label in {swow_cue_vocab} and obj_in_BERT==1\").reset_index(drop=True)\n",
    "print(\"#instances shared in SWOW and BERT\", len(df.index))\n",
    "\n",
    "df['masked_sentences'] = df['sub_label'].apply(lambda x: [f\"A {x} is a [MASK]\"])\n",
    "df['uuid'] = df.index + 1\n",
    "\n",
    "df['subj_sister'] = df['sub_label'].apply(lambda x: word_to_cohyponyms.get(x))\n",
    "print(len(df.loc[df.subj_sister.str.len()==0].index))\n",
    "# save_dict_to_json(df.to_dict(orient='records'), output_path = '../../data/hypernymsuite/ALL/swow_rw/IsA.jsonl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# word_to_plural = {word: pluralize(word) for word in set(df['sub_label']).union(df['obj_label'])}\n",
    "\n",
    "# swow_type_to_paths = {\"Stregnth\": '../../data/swow/swow.en.strength.R123.pl.json', \n",
    "#                       \"FW+BW\": '../../data/swow/swow.en.fbs.json',\n",
    "#                       \"Similar\": '../../data/swow/swow.en.similar_words.sgpl.json'}\n",
    "\n",
    "# df['sub_label_pl'] = df['sub_label'].apply(lambda x: word_to_plural[x])\n",
    "\n",
    "# label_col = 'subj_anchors_label'\n",
    "# df['obj_label'] = df['obj_label'].apply(lambda x: [x] )\n",
    "\n",
    "# ######################################### \n",
    "# for swow_type, path in swow_type_to_paths.items():\n",
    "#     print(swow_type)\n",
    "#     word_to_swow = json.load( open(path, 'r'))\n",
    "#     word_to_swow = {k: list(v.keys()) for k,v in word_to_swow.items()}\n",
    "#     df['subj_anchors_pred'] = df['sub_label_pl'].progress_apply(lambda x: word_to_swow.get(x, []))\n",
    "#     df_new = df.loc[df['subj_anchors_pred'].str.len() >0]\n",
    "#     print(\"#instances with non empty swow anchors\", len(df_new))\n",
    "\n",
    "#     df = df.loc[df['subj_anchors_label'].str.len() >0]\n",
    "#     print(\"#instances with non empty wordnet anchors\", len(df))\n",
    "\n",
    "#     pred_cols = ['subj_anchors_pred']# ['subj_anchors_all_sg'] #['subj_anchors_swow']\n",
    "#     #df = df.loc[df['subj_anchors_swow'].str.len() >0]\n",
    "#     df_prec_anchor = get_precision_at_k_concept(df, relation, pred_cols, label_col, k_list=[1, 5, 10],pred_col_suffix=pred_col_suffix ) ##note that this would be super slow when top_k is large (>1000) \n",
    "#     df_mrr =  get_mrr(df, relation, pred_cols, label_col, pred_col_suffix)\n",
    "#     df_prec_anchor['mrr'] = df_prec_anchor['mask_type'].apply(lambda x:  df_mrr.loc[df_mrr['mask_type']==x, f'mrr'].values[0])\n",
    "#     df_prec_anchor_display = df_prec_anchor[[\"mask_type\", 'mrr', \"p@1\", \"p@5\", \"p@10\"]] \n",
    "#     print(tabulate(df_prec_anchor_display, tablefmt='latex', headers=df_prec_anchor_display.columns).replace(\"\\\\\", \"\").replace(\"&\", \"\\t\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3136\n",
       "0    1413\n",
       "Name: sub_in_BERT, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.loc[df.subj_sister.str.len()==0]\n",
    "\n",
    "dataset_to_df['ALL']['sub_in_BERT'] = dataset_to_df['ALL']['sub_label'].apply(lambda x: 1 if x in bert_vocab else 0)\n",
    "dataset_to_df['ALL'][\"sub_in_BERT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_label</th>\n",
       "      <th>obj_label</th>\n",
       "      <th>relation</th>\n",
       "      <th>obj_in_BERT</th>\n",
       "      <th>sub_in_BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>musket</td>\n",
       "      <td>firearm</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>giraffe</td>\n",
       "      <td>creature</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sieve</td>\n",
       "      <td>object</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>hatchet</td>\n",
       "      <td>object</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>cockroach</td>\n",
       "      <td>arthropod</td>\n",
       "      <td>IsA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>gent</td>\n",
       "      <td>gentleman</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>humility</td>\n",
       "      <td>emotion</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>pant</td>\n",
       "      <td>trouser</td>\n",
       "      <td>IsA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>skateboard</td>\n",
       "      <td>sport</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>wale</td>\n",
       "      <td>country</td>\n",
       "      <td>IsA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1413 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sub_label  obj_label relation  obj_in_BERT  sub_in_BERT\n",
       "3        musket    firearm      IsA            1            0\n",
       "21      giraffe   creature      IsA            1            0\n",
       "26        sieve     object      IsA            1            0\n",
       "60      hatchet     object      IsA            1            0\n",
       "61    cockroach  arthropod      IsA            0            0\n",
       "..          ...        ...      ...          ...          ...\n",
       "397        gent  gentleman      IsA            1            0\n",
       "459    humility    emotion      IsA            1            0\n",
       "627        pant    trouser      IsA            0            0\n",
       "777  skateboard      sport      IsA            1            0\n",
       "918        wale    country      IsA            1            0\n",
       "\n",
       "[1413 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_df['ALL'].query(\"sub_in_BERT==0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats between SWOW and six hypernym dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "swow_path = '../../data/swow/swow.en.similar_words.json'\n",
    "cue_to_similar_words = json.load(open(swow_path))\n",
    "\n",
    "\n",
    "# from inflection import singularize, pluralize \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "# import pyinflect\n",
    "import lemminflect\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def pluralize(word):\n",
    "    pl = nlp(word)\n",
    "    if pl[0].pos_ not in [\"NOUN\"] or pl[0].tag_ == 'NNS':\n",
    "        return word\n",
    "    else:\n",
    "        return pl[0]._.inflect('NNS')\n",
    "\n",
    "def singularize(word):\n",
    "    '''\n",
    "    sg is None if the word is not in the vocab \n",
    "    '''\n",
    "    sg =  nlp(word) #[0]._.inflect('NN')\n",
    "\n",
    "    if sg[0].pos_ not in [\"NOUN\"] or sg[0].tag_ == 'NN':\n",
    "        return word\n",
    "    else:\n",
    "        return sg[0]._.inflect('NN')\n",
    "    \n",
    "    \n",
    "word = 'cartoon'#'animation'\n",
    "print(cue_to_similar_words[word])\n",
    "for v in cue_to_similar_words[word]:\n",
    "    #print(v)\n",
    "    print (v[0], singularize(v[0]))\n",
    "    print (v[0], pluralize(v[0]))\n",
    "    \n",
    "print( len(cue_to_similar_words.keys()) )\n",
    "import os, sys \n",
    "from utils_path import dataset_to_respath\n",
    "\n",
    "cues = set(cue_to_similar_words.keys())\n",
    "df_stats = []\n",
    "dfs = []\n",
    "for dataset, path in dataset_to_respath.items():\n",
    "    path = \"../../\"+path\n",
    "    df = pd.read_csv(path)\n",
    "    sub_label_sg = set(df['sub_label_sg'].to_list())\n",
    "    shared = sub_label_sg.intersection(cues)\n",
    "    shared_rate = round( len(shared)/ len(sub_label_sg), 3)\n",
    "    stats = {\"dataset\": dataset, '#sub_label': len(sub_label_sg), \"#SWOW_shared_sub_label\": len(shared), \"#shared_rate\": shared_rate}\n",
    "    #print(stats)\n",
    "    df['dataset'] = dataset\n",
    "    shared_list = list(shared)\n",
    "    dfs.append(df.query(f'sub_label_sg in {shared_list}').reset_index(drop=True))\n",
    "    df_stats.append(stats)\n",
    "df_stats = pd.DataFrame(df_stats)\n",
    "\n",
    "display(df_stats.sort_values(by=['#shared_rate'], ascending=False))\n",
    "dfs = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json \n",
    "import pandas as pd\n",
    "from inflection import singularize, pluralize \n",
    "\n",
    "def get_strength_dict(path, source_path):\n",
    "    if os.path.exists(path):\n",
    "        swow_score_dict_pl = json.load(open(path, 'r'))\n",
    "    else: \n",
    "        swow_score_dict = json.load( open (source_path))\n",
    "\n",
    "        vocab_cues = set(swow_score_dict.keys())\n",
    "        vocab_res = set()\n",
    "        for k,v in swow_score_dict.items():\n",
    "            vocab_res.update(v.keys() )\n",
    "            \n",
    "        vocab = vocab_cues.union(vocab_res)\n",
    "        vocab = set({str(k) for k in vocab})\n",
    "        vocab_to_plural = {k: pluralize(k) for k in vocab }\n",
    "\n",
    "        swow_score_dict_pl = defaultdict()\n",
    "        for k,v in swow_score_dict.items():\n",
    "            v_pl = {vocab_to_plural.get(k1):v1 for k1, v1 in v.items() }\n",
    "            swow_score_dict_pl[vocab_to_plural.get(k)] = v_pl\n",
    "        output_path = input_path.replace('.json', '.pl.json')\n",
    "        json.dump(swow_score_dict_pl, open(output_path, 'w'))\n",
    "    return swow_score_dict_pl \n",
    "\n",
    "def query_strength_score(cue, response, score_dict):\n",
    "    if cue in score_dict and response in score_dict[cue]:\n",
    "        return score_dict[cue][response]\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def get_sim_matrix(path_rw='../../data/swow/S_RW.R123.csv'):\n",
    "    df = pd.read_csv(path_rw)\n",
    "    sim_matrix = df.to_numpy()\n",
    "    vocab = df.columns[1:]\n",
    "    vocab_pl = [pluralize(word) for word in vocab] #this is normalized to fit into the anchor mining module\n",
    "    word2id = {word:i for i, word in enumerate(vocab)}\n",
    "    return word2id, sim_matrix\n",
    "\n",
    "\n",
    "def query_sim(word1, word2, swow_score_tuple ):\n",
    "    '''\n",
    "    query the similary between two words in a similarity matrix \n",
    "    '''\n",
    "    word2id, sim_matrix = swow_score_tuple\n",
    "    sim_score =0 \n",
    "    if word1 in word2id and word2 in word2id:\n",
    "        id1 = word2id.get(word1)\n",
    "        id2 = word2id.get(word2)\n",
    "\n",
    "        sim_score = sim_matrix[id1][id2+1] #the first col is the word \n",
    "    return sim_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae9f672da5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquery_swow_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_strength_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mswow_score_source\u001b[0m  \u001b[0;34m==\u001b[0m\u001b[0;34m'AddSWOWSimilar'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#similar words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mswow_score_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sim_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_rw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../data/swow/S_RW.R123.csv'\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#word2id, sim_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mquery_swow_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-13dedc8708d6>\u001b[0m in \u001b[0;36mget_sim_matrix\u001b[0;34m(path_rw)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sim_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_rw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../data/swow/S_RW.R123.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_rw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m     \"\"\"\n\u001b[1;32m   1431\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# swow_score_source = 'AddSWOWStrength'\n",
    "swow_score_source = 'AddSWOWSimilar'\n",
    "if swow_score_source =='AddSWOWStrength':\n",
    "    swow_score_tuple = get_strength_dict(path='../../data/swow/swow.en.fbs.json', source_path = 'data/swow/swow.en.strength.R123.json')\n",
    "    query_swow_score = query_strength_score\n",
    "elif swow_score_source  =='AddSWOWSimilar': #similar words\n",
    "    swow_score_tuple = get_sim_matrix(path_rw='../../data/swow/S_RW.R123.csv')   #word2id, sim_matrix \n",
    "    query_swow_score = query_sim\n",
    "    \n",
    "cue = 'lime'\n",
    "responses = swow_score_tuple[cue]\n",
    "print(responses)\n",
    "responses = ['lemon', 'zest', 'citrus', 'margarita']\n",
    "for response in responses:\n",
    "    score = query_swow_score(cue =cue,  response= response, score_dict=swow_score_tuple)\n",
    "    print(cue, response, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemon': 0.583459797300129,\n",
       " 'citrus': 0.583859542567453,\n",
       " 'lemony': 0.487566375532904,\n",
       " 'tequila': 0.474077408141611,\n",
       " 'juice': 0.470873289673751,\n",
       " 'guacamole': 0.468157988084381,\n",
       " 'lemonade': 0.467682347096958,\n",
       " 'tangy': 0.451454685461879,\n",
       " 'sour': 0.434657464571513}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load( open('../../data/swow/swow.en.similar_words.sg.json', 'r'))\n",
    "data['lime']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
