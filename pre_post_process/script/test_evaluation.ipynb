{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f1b508-471a-400a-9597-0ed29bf3f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333333 1.0\n",
      "1.0 0.5\n"
     ]
    }
   ],
   "source": [
    "#from evaluation import mean_average_precision, average_precision_at_k, precision_at_k, recall_at_k\n",
    "\n",
    "# def get_mean_average_precision_at_k(df, relation, pred_cols, label_col, k_list, pred_col_suffix='obj_mask_'):\n",
    "#     # get the avearage precision per query\n",
    "#     map_at_x = []\n",
    "#     for pred_col in pred_cols:\n",
    "#         suffix = pred_col.replace(pred_col_suffix, \"\")\n",
    "#         map_cur = defaultdict()\n",
    "#         map_cur['mask_type'] = suffix\n",
    "\n",
    "#         for k in k_list:\n",
    "#             df[f'ap{k}_{suffix}'] = df[[label_col, pred_col]].apply(lambda x: average_precision_at_k(y_true=np.array([x[0]]) if isinstance(x[0], str) else np.array(x[0]) , y_pred= np.array(x[1]), k=k), axis=1)\n",
    "#             map_cur[f'mAP@{k}'] = round(df[f'ap{k}_{suffix}'].mean(), 3)*100\n",
    "#         map_at_x.append(map_cur)\n",
    "\n",
    "#     # aggregate the average precision across k\n",
    "#     df_res = pd.DataFrame(map_at_x) #, columns=['mask_type', 'mAP'])\n",
    "#     df_res['relation'] = [relation]*len(df_res)\n",
    "#     return df_res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################### mAP #####################\n",
    "#reference\n",
    "# 1. https://www.kaggle.com/code/debarshichanda/understanding-mean-average-precision/notebook (main)\n",
    "# 2. https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52#f9ce\n",
    "\n",
    "def rel_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\" Computes Relevance at k for one sample\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Relevance at k\n",
    "    \"\"\"\n",
    "#     print(f\"y_pred:{y_pred}\")\n",
    "#     print(f\"y_true:{y_true}\")\n",
    "\n",
    "    if y_pred[k-1] in y_true:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\" Computes Precision at k for one sample\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Precision at k\n",
    "    \"\"\"\n",
    "#     intersection = np.intersect1d(y_true, y_pred[:k])\n",
    "#     if len(y_pred)==0 or len(y_true)==0:\n",
    "        # return 0\n",
    "    intersection = set(y_true).intersection(set(y_pred[:k]))\n",
    "    return len(intersection) / k\n",
    "\n",
    "\n",
    "def mean_average_precision(y_true, y_pred, k=10):\n",
    "    \"\"\" Computes MAP at k\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: list\n",
    "            2D list of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.list\n",
    "            2D list of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           MAP at k\n",
    "\n",
    "        return np.mean([average_precision_at_k(np.array(gt), np.array(pred), k) \\\n",
    "                        for gt, pred in zip(y_true, y_pred)])\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean([average_precision_at_k(gt, pred, k) \\\n",
    "                        for gt, pred in zip(y_true, y_pred)])\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.mean([average_precision_at_k(gt, pred, k) \\\n",
    "                    for gt, pred in zip(y_true, y_pred)])\n",
    "\n",
    "\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\" Computes Precision at k for one sample\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Precision at k\n",
    "    \"\"\"\n",
    "\n",
    "    intersection = set(y_true).intersection(set(y_pred[:k]))\n",
    "    return len(intersection) / len(y_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_mean_average_precision():\n",
    "    gt = np.array(['a', 'b', 'c', 'd', 'e' ])\n",
    "    preds1 = np.array(['a', 'x', 'y', 'd', 'e'])\n",
    "    preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "    preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "    preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "    preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "    preds6 = np.array(['d', 'c', 'b', 'a', 'e'])\n",
    "\n",
    "    # y_true = np.array([gt, gt, gt, gt, gt, gt])\n",
    "    # y_pred = np.array([preds1, preds2, preds3, preds4, preds5, preds6])\n",
    "\n",
    "    y_true = np.array([gt])\n",
    "    y_pred = np.array([preds1])\n",
    "    mean_ap = mean_average_precision(y_true, y_pred, k=5)\n",
    "\n",
    "    print(mean_ap)\n",
    "    print(\"{k}\\t{p_at_k}\\t{rel}\\t{ap}\")\n",
    "    for k in [1, 2,3, 4, 5]:\n",
    "            ap = average_precision_at_k(gt, preds1, k=k)\n",
    "            rel = rel_at_k(gt, preds1, k=k)\n",
    "            p_at_k = precision_at_k(gt, preds1, k=k)\n",
    "\n",
    "            # y_true = gt\n",
    "            # y_pred = preds1\n",
    "            # ap = 0.0\n",
    "            # for i in range(1, k+1):\n",
    "            #         ap += precision_at_k(y_true, y_pred, i) * rel_at_k(y_true, y_pred, i)\n",
    "            # print('before:', ap)\n",
    "            # # ap = ap / min(k, len(y_true))\n",
    "            # # ap = ap / min(k, len(y_true))\n",
    "            # ap = ap / sum(rel_at_k(y_true, y_pred, i))\n",
    "            print(f\"{k}\\t{p_at_k}\\t{rel}\\t{ap}\")\n",
    "\n",
    "            # intersection = np.intersect1d(y_true, y_pred[:k])\n",
    "    #     return len(intersection) / k\n",
    "\n",
    "\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\" Computes Average Precision at k for one sample\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Average Precision at k\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    indicators = []\n",
    "#     for i in range(1, k+1):\n",
    "        # https://stackoverflow.com/questions/46374405/precision-at-k-when-fewer-than-k-documents-are-retrieved\n",
    "        # Precision measured at various doc level cutoffs in the ranking.\n",
    "        #If the cutoff is larger than the number of docs retrieved, then\n",
    "        #it is assumed nonrelevant docs fill in the rest.  Eg, if a method\n",
    "        #retrieves 15 docs of which 4 are relevant, then P20 is 0.2 (4/20).\n",
    "        #Precision is a very nice user oriented measure, and a good comparison\n",
    "        #number for a single topic, but it does not average well. For example,\n",
    "        #P20 has very different expected characteristics if there 300\n",
    "        #total relevant docs for a topic as opposed to 10.\n",
    "    p_at_k = []\n",
    "    ap = 0.0\n",
    "    for i in range(1, min( len(y_pred)+1, k+1)): # revised range to fix the cornor case: when K < len(y_pred)\n",
    "        indicator = rel_at_k(y_true, y_pred, i)\n",
    "        p_at_k = precision_at_k(y_true, y_pred, i)\n",
    "        ap +=  p_at_k* indicator \n",
    "        indicators.append(indicator)\n",
    "    return ap/sum(indicators)  if sum(indicators)>0 else 0 #min(k, len(y_pred))\n",
    "\n",
    "\n",
    "def test_map():\n",
    "        y_true = ['tree', 'plant']\n",
    "        y_pred_sap = ['tree', 'wood', 'plant', 'shrub', 'forest', 'woodland', 'vegetable', 'herb', 'thing', 'specie']\n",
    "        y_pred_dap = ['tree', 'wood', 'forest', 'woodland', 'shrub', 'plant', 'oak', 'bush', 'specie', 'herb']\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred_sap = np.array(y_pred_sap)\n",
    "        y_pred_dap = np.array(y_pred_dap)\n",
    "        # 0.833333\t1.000000\t['oaks', 'pines', 'cedars', 'willows', 'beeches']\n",
    "        ap1 = average_precision_at_k(y_true, y_pred_sap, k=5)\n",
    "        ap2 = average_precision_at_k(y_true, y_pred_dap, k=5)\n",
    "        print(ap1, ap2)\n",
    "        rc1 = recall_at_k(y_true, y_pred_sap, k=5)\n",
    "        rc2 = recall_at_k(y_true, y_pred_dap, k=5)\n",
    "        print(rc1, rc2)\n",
    "test_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa8ede-7d38-4305-a8f7-789dd9833fa0",
   "metadata": {},
   "source": [
    "# Test MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93e2922f-d29e-4261-b3bc-6ed6e83a2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def get_highest_mrr_among_labels(label, pred):\n",
    "    '''\n",
    "    return the highest rank among the multiple labels. This is applicable to single labels as well, if we the single label is put in a list\n",
    "\n",
    "    pred: a list of words (candidates)\n",
    "    label: the true labels, which is a list (different forms of a word, e.g., singular or plurs, like animal and animals)\n",
    "    '''\n",
    "    mrr = 0 \n",
    "    if pred is None: return mrr \n",
    "\n",
    "    rank_list = [ pred.index(item) + 1 for item in label if item in pred] \n",
    "    if len(rank_list)>0:\n",
    "        mrr = 1/min(rank_list)\n",
    "\n",
    "    return mrr \n",
    "\n",
    "label = ['plant', 'tree']\n",
    "pred = [ 'example', 'plant', 'ebony', 'tree', 'oak', 'shrub', 'elm']\n",
    "mrr = get_highest_mrr_among_labels(label, pred)\n",
    "print(mrr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
